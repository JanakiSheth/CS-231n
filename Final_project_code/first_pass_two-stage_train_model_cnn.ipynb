{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Classification\n",
    "    Two-stage model: yes/no on class 1 first and then 9-class classification for the rest. \n",
    "    \n",
    "## Packages Installation\n",
    "- tensorflow v1.5 (https://stackoverflow.com/questions/40416056/how-to-download-previous-version-of-tensorflow)\n",
    "- If you have gpu: cuda v9.0; cudnn v7.0\n",
    "- pytorch v0.4.0\n",
    "- jupyter notebook >= 4.4.0\n",
    "- numpy (newest)\n",
    "- other packages as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "#\n",
    "# 09/03/18 setup\n",
    "#\n",
    "##############################\n",
    "from __future__ import division\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import torch\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "# set random seed\n",
    "# We need to set the cudnn to derministic mode so that we can get consistent result\n",
    "# Yet, this will slow down the training :( It's a trade-off\n",
    "# see https://discuss.pytorch.org/t/how-to-confugure-pytorch-to-get-deterministic-results/11797\n",
    "# see https://discuss.pytorch.org/t/network-forward-backward-calculation-precision-error/17716/2\n",
    "random_state = 100\n",
    "np.random.seed(random_state)\n",
    "torch.manual_seed(random_state)\n",
    "random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janaki\\Miniconda3\\envs\\janaki_gpu\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "C:\\Users\\janaki\\Miniconda3\\envs\\janaki_gpu\\lib\\site-packages\\ipykernel_launcher.py:14: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "data_root_dir = 'C:/Users/janaki/Dropbox/bci/bill_grant_Fall2018/'\n",
    "f_name = '442.csv'\n",
    "\n",
    "# load the feature and data\n",
    "df = pd.read_csv('%s/%s' %(data_root_dir, f_name), header=-1)\n",
    "\n",
    "# define label and feature; convert it to ndarray\n",
    "# adjust the label from 0 to 9\n",
    "# y does not have to be one-hot embedding, i.e., if 10 class, then dim is 1000x10, can be 1000,\n",
    "df_label = df.iloc[:,0]\n",
    "df_f = df.iloc[:,1:]\n",
    "X_raw = df_f.as_matrix()\n",
    "y = df_label.as_matrix()-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27675, 148)\n",
      "(27675,)\n"
     ]
    }
   ],
   "source": [
    "# X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
    "print(X_raw.shape)\n",
    "print(y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5119, 148)\n",
      "(5119,)\n",
      "(10238, 148)\n",
      "(10238,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prepare the datasets for 2-stage classificaton: yes/no class 1 and then 9-class classification for the rest. \n",
    "\n",
    "y_major: yes/no major class (1st stage)\n",
    "y_minor: 9-class classification (2nd stage)\n",
    "\"\"\"\n",
    "\n",
    "def get_dataset(X_raw, y):\n",
    "    # get the class 1-9\n",
    "    minor_class_index = np.where(y>0)\n",
    "    X_minor = np.squeeze(X_raw[minor_class_index,:])\n",
    "    y_minor = y[minor_class_index]\n",
    "\n",
    "\n",
    "    # get the majority class 0\n",
    "    major_class_index = np.where(y==0)\n",
    "    X_major = np.squeeze(X_raw[major_class_index,:])\n",
    "    y_major = y[major_class_index]\n",
    "\n",
    "\n",
    "    # prepare the major class for 1st stage training\n",
    "    s = X_minor.shape\n",
    "    r_index = random.sample(range(X_major.shape[0]),s[0])\n",
    "    #r_index = range(X_major.shape[0])\n",
    "    X_major = X_major[r_index,:]\n",
    "    y_major = y_major[r_index]\n",
    "\n",
    "\n",
    "    # create the major class set\n",
    "    X_tmp = np.copy(X_minor)\n",
    "    y_tmp = np.ones(X_minor.shape[0])\n",
    "    X_major = np.concatenate((X_major,X_tmp))\n",
    "    y_major = np.concatenate((y_major,y_tmp))\n",
    "\n",
    "\n",
    "    # adjust the minor class set label\n",
    "    y_minor = y_minor - 1\n",
    "    return X_minor, y_minor, X_major, y_major\n",
    "\n",
    "\n",
    "# check\n",
    "X_minor, y_minor, X_major, y_major = get_dataset(X_raw,y)\n",
    "print(X_minor.shape)\n",
    "print(y_minor.shape)\n",
    "print(X_major.shape)\n",
    "print(y_major.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      66\n",
      "1      92\n",
      "2     571\n",
      "3     820\n",
      "4     297\n",
      "5     210\n",
      "6    1539\n",
      "7     733\n",
      "8     791\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "    df_label = pd.DataFrame(y_minor)\n",
    "    df_label = df_label.iloc[:,0]\n",
    "    \n",
    "    counts = df_label.value_counts().sort_index()\n",
    "    print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 1. 1. 1.]\n",
      "[23.318181818181817, 16.72826086956522, 2.6952714535901925, 1.8768292682926828, 5.181818181818182, 7.328571428571428, 1.0, 2.0995907230559343, 1.945638432364096]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janaki\\Miniconda3\\envs\\janaki_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# calculate class ratio for loss calculation because it is imbalanced\n",
    "\n",
    "def get_weight_balance(y):\n",
    "    \"\"\"\n",
    "    Calculate the class ratio.\n",
    "    \"\"\"\n",
    "    df_label = pd.DataFrame(y)\n",
    "    df_label = df_label.iloc[:,0]\n",
    "    \n",
    "    counts = df_label.value_counts().sort_index()\n",
    "    counts_np = counts.as_matrix()\n",
    "    max_val = np.max(np.abs(counts_np),axis=0)\n",
    "\n",
    "    class_weight = np.max(np.abs(counts_np),axis=0)/counts_np\n",
    "    class_weight = class_weight.tolist()\n",
    "    \n",
    "    return class_weight\n",
    "\n",
    "weight_minor = get_weight_balance(y_minor)\n",
    "weight_major = get_weight_balance(y_major)\n",
    "\n",
    "# # check\n",
    "print(y_major)\n",
    "print(weight_minor)\n",
    "# print(len(weight_major))\n",
    "# print(len(weight_minor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define the CNN model.\n",
    "\"\"\"\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModule(nn.Module):\n",
    "    def __init__(self, num_units=10, nonlin=F.relu, num_output=11):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.conv1d_1 = nn.Conv1d(1, 40, kernel_size=147)\n",
    "        self.pool = nn.MaxPool1d(2, padding = 0)\n",
    "        \n",
    "        self.nonlin = nonlin\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.dense0 = nn.Linear(40, num_units)\n",
    "        self.dense0_bn = nn.BatchNorm1d(10)\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.dense1_bn = nn.BatchNorm1d(10)\n",
    "        self.output = nn.Linear(10, num_output)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.conv1d_1(X))\n",
    "        X= self.pool(X)\n",
    "        X = X.view(-1, self._get_element_count(X))     \n",
    "        X = self.nonlin(self.dense0_bn(self.dense0(X)))\n",
    "        X = self.nonlin(self.dense1_bn(self.dense1(X)))\n",
    "        X = self.output(X)\n",
    "        #X = F.softmax(self.output(X), dim=-1)\n",
    "        return X\n",
    "    \n",
    "    def _get_element_count(self,x):\n",
    "        \"\"\"\n",
    "        Get the total number of elements in a layer output (not consider the batch level)\n",
    "        \"\"\"\n",
    "        x_dim = list(x.size())[1:]\n",
    "        return reduce(lambda x, y: x*y, x_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xy_preprocess(X_raw,y):\n",
    "    # define the data\n",
    "    X = X_raw.astype(np.float32) #1000x148\n",
    "    y = y.astype(np.int64) #1000x1\n",
    "\n",
    "    # add a dummy dimension to X to make it having channel\n",
    "    X = np.expand_dims(X, axis=1) #1000x1x148\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_proba(y_proba):\n",
    "    # convert it into probabilities\n",
    "    y_proba = torch.from_numpy(y_proba)\n",
    "    y_proba = F.softmax(y_proba, dim=-1)\n",
    "    y_proba = y_proba.numpy()\n",
    "    return y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janaki\\Miniconda3\\envs\\janaki_gpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:652: Warning: The least populated class in y has only 66 members, which is too few. The minimum number of members in any class cannot be less than n_splits=123.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Fold:1==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janaki\\Miniconda3\\envs\\janaki_gpu\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2550  0.5994\n",
      "      2        0.1679  0.4741\n",
      "      3        0.1550  0.4691\n",
      "      4        0.1468  0.4831\n",
      "      5        0.1397  0.4731\n",
      "      6        0.1361  0.4846\n",
      "      7        0.1290  0.4756\n",
      "      8        0.1270  0.4911\n",
      "      9        0.1189  0.4766\n",
      "     10        0.1143  0.4746\n",
      "     11        0.1111  0.4686\n",
      "     12        0.1105  0.4891\n",
      "     13        0.1073  0.4851\n",
      "     14        0.1045  0.4776\n",
      "     15        0.1026  0.4821\n",
      "     16        0.1025  0.4891\n",
      "     17        0.0994  0.4542\n",
      "     18        0.0945  0.4901\n",
      "     19        0.0928  0.4621\n",
      "     20        0.0927  0.4796\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0255  0.2376\n",
      "      2        1.5949  0.2500\n",
      "      3        1.2831  0.2376\n",
      "      4        1.0548  0.2535\n",
      "      5        0.9754  0.2974\n",
      "      6        0.8132  0.2371\n",
      "      7        0.6797  0.2535\n",
      "      8        0.6825  0.2301\n",
      "      9        0.6025  0.2326\n",
      "     10        0.5418  0.2495\n",
      "     11        0.4643  0.2346\n",
      "     12        0.4534  0.2475\n",
      "     13        0.4283  0.2421\n",
      "     14        0.3596  0.2480\n",
      "     15        0.3212  0.2460\n",
      "     16        0.3644  0.2490\n",
      "     17        0.3848  0.2495\n",
      "     18        0.5387  0.2505\n",
      "     19        0.3361  0.2246\n",
      "     20        0.2838  0.2460\n",
      "     21        0.3041  0.2371\n",
      "     22        0.3957  0.2475\n",
      "     23        0.3490  0.2500\n",
      "     24        0.2776  0.2465\n",
      "     25        0.2014  0.2421\n",
      "     26        0.1890  0.2376\n",
      "     27        0.1839  0.2530\n",
      "     28        0.2226  0.2381\n",
      "     29        0.2186  0.2500\n",
      "     30        0.2329  0.2455\n",
      "==========Fold:2==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2380  0.4581\n",
      "      2        0.1659  0.4711\n",
      "      3        0.1539  0.4956\n",
      "      4        0.1431  0.4666\n",
      "      5        0.1356  0.4826\n",
      "      6        0.1281  0.4786\n",
      "      7        0.1216  0.4716\n",
      "      8        0.1158  0.4761\n",
      "      9        0.1140  0.4497\n",
      "     10        0.1056  0.4756\n",
      "     11        0.1010  0.4776\n",
      "     12        0.0989  0.4736\n",
      "     13        0.0931  0.4741\n",
      "     14        0.0958  0.4422\n",
      "     15        0.0885  0.4806\n",
      "     16        0.0861  0.4811\n",
      "     17        0.0880  0.4806\n",
      "     18        0.0781  0.4572\n",
      "     19        0.0814  0.4776\n",
      "     20        0.0805  0.4631\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0510  0.2425\n",
      "      2        1.5957  0.2495\n",
      "      3        1.2636  0.2455\n",
      "      4        1.0402  0.2386\n",
      "      5        0.9654  0.2261\n",
      "      6        0.7910  0.2286\n",
      "      7        0.6877  0.2530\n",
      "      8        0.7062  0.2406\n",
      "      9        0.6736  0.2475\n",
      "     10        0.6270  0.2450\n",
      "     11        0.5375  0.2505\n",
      "     12        0.5077  0.2346\n",
      "     13        0.4629  0.2580\n",
      "     14        0.4044  0.2440\n",
      "     15        0.3587  0.2381\n",
      "     16        0.4193  0.2445\n",
      "     17        0.4900  0.2455\n",
      "     18        0.4062  0.2276\n",
      "     19        0.3241  0.2455\n",
      "     20        0.3279  0.2237\n",
      "     21        0.3523  0.2346\n",
      "     22        0.3198  0.2455\n",
      "     23        0.2763  0.2371\n",
      "     24        0.2670  0.2396\n",
      "     25        0.2228  0.2555\n",
      "     26        0.3099  0.2503\n",
      "     27        0.2987  0.2435\n",
      "     28        0.2281  0.2401\n",
      "     29        0.1922  0.2490\n",
      "     30        0.2796  0.2391\n",
      "==========Fold:3==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2581  0.4736\n",
      "      2        0.1678  0.4761\n",
      "      3        0.1538  0.4936\n",
      "      4        0.1429  0.4671\n",
      "      5        0.1351  0.4776\n",
      "      6        0.1292  0.4686\n",
      "      7        0.1206  0.4778\n",
      "      8        0.1137  0.4821\n",
      "      9        0.1102  0.4696\n",
      "     10        0.1033  0.4751\n",
      "     11        0.1037  0.4806\n",
      "     12        0.0990  0.4701\n",
      "     13        0.0929  0.4711\n",
      "     14        0.0930  0.4761\n",
      "     15        0.0893  0.4606\n",
      "     16        0.0860  0.4946\n",
      "     17        0.0883  0.4876\n",
      "     18        0.0813  0.4646\n",
      "     19        0.0856  0.4841\n",
      "     20        0.0772  0.4881\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1077  0.2401\n",
      "      2        1.7370  0.2485\n",
      "      3        1.4436  0.2351\n",
      "      4        1.1949  0.2376\n",
      "      5        1.1707  0.2381\n",
      "      6        0.9104  0.2306\n",
      "      7        0.8078  0.2565\n",
      "      8        0.8467  0.2416\n",
      "      9        0.7323  0.2465\n",
      "     10        0.6152  0.2440\n",
      "     11        0.5671  0.2281\n",
      "     12        0.6025  0.2490\n",
      "     13        0.5429  0.2430\n",
      "     14        0.5520  0.2550\n",
      "     15        0.4656  0.2406\n",
      "     16        0.4275  0.2176\n",
      "     17        0.3891  0.2341\n",
      "     18        0.4217  0.2440\n",
      "     19        0.4567  0.2535\n",
      "     20        0.4608  0.2515\n",
      "     21        0.4120  0.2411\n",
      "     22        0.3622  0.2455\n",
      "     23        0.3429  0.2445\n",
      "     24        0.3518  0.2480\n",
      "     25        0.3082  0.2196\n",
      "     26        0.3492  0.2321\n",
      "     27        0.3314  0.2480\n",
      "     28        0.3616  0.2241\n",
      "     29        0.2916  0.2480\n",
      "     30        0.2854  0.2560\n",
      "==========Fold:4==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2402  0.4646\n",
      "      2        0.1685  0.4991\n",
      "      3        0.1583  0.4766\n",
      "      4        0.1486  0.4721\n",
      "      5        0.1398  0.4826\n",
      "      6        0.1363  0.4931\n",
      "      7        0.1315  0.4726\n",
      "      8        0.1272  0.4731\n",
      "      9        0.1205  0.4771\n",
      "     10        0.1144  0.4786\n",
      "     11        0.1104  0.4911\n",
      "     12        0.1031  0.4926\n",
      "     13        0.1026  0.4487\n",
      "     14        0.0991  0.4761\n",
      "     15        0.1027  0.4721\n",
      "     16        0.0957  0.4811\n",
      "     17        0.0931  0.4596\n",
      "     18        0.0852  0.4801\n",
      "     19        0.0991  0.4731\n",
      "     20        0.0889  0.4751\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0731  0.2371\n",
      "      2        1.7031  0.2391\n",
      "      3        1.3352  0.2465\n",
      "      4        1.0890  0.2435\n",
      "      5        0.9757  0.2386\n",
      "      6        0.7932  0.2371\n",
      "      7        0.7253  0.2391\n",
      "      8        0.6815  0.2411\n",
      "      9        0.6613  0.2435\n",
      "     10        0.5611  0.2420\n",
      "     11        0.4936  0.2545\n",
      "     12        0.5328  0.2460\n",
      "     13        0.6030  0.2266\n",
      "     14        0.4701  0.2440\n",
      "     15        0.3891  0.2401\n",
      "     16        0.3628  0.2425\n",
      "     17        0.3825  0.2460\n",
      "     18        0.3591  0.2450\n",
      "     19        0.3251  0.2530\n",
      "     20        0.3163  0.2705\n",
      "     21        0.3855  0.2470\n",
      "     22        0.3292  0.2351\n",
      "     23        0.2930  0.2445\n",
      "     24        0.2761  0.2460\n",
      "     25        0.3022  0.2515\n",
      "     26        0.3019  0.2361\n",
      "     27        0.2900  0.2416\n",
      "     28        0.2708  0.2440\n",
      "     29        0.2184  0.2386\n",
      "     30        0.2278  0.2346\n",
      "==========Fold:5==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.3256  0.4716\n",
      "      2        0.1727  0.4756\n",
      "      3        0.1591  0.4781\n",
      "      4        0.1498  0.4552\n",
      "      5        0.1413  0.4646\n",
      "      6        0.1342  0.4651\n",
      "      7        0.1277  0.4766\n",
      "      8        0.1216  0.4916\n",
      "      9        0.1156  0.4741\n",
      "     10        0.1124  0.4806\n",
      "     11        0.1076  0.4796\n",
      "     12        0.1035  0.4826\n",
      "     13        0.1023  0.4876\n",
      "     14        0.0946  0.4741\n",
      "     15        0.0977  0.4666\n",
      "     16        0.0972  0.4831\n",
      "     17        0.0883  0.4562\n",
      "     18        0.0864  0.4711\n",
      "     19        0.0826  0.4611\n",
      "     20        0.0844  0.4766\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0688  0.2525\n",
      "      2        1.7138  0.2450\n",
      "      3        1.3599  0.2381\n",
      "      4        1.1078  0.2425\n",
      "      5        0.9933  0.2465\n",
      "      6        0.7950  0.2465\n",
      "      7        0.7317  0.2326\n",
      "      8        0.7868  0.2590\n",
      "      9        0.6741  0.2510\n",
      "     10        0.5666  0.2435\n",
      "     11        0.5183  0.2391\n",
      "     12        0.5296  0.2416\n",
      "     13        0.5131  0.2435\n",
      "     14        0.5058  0.2530\n",
      "     15        0.4433  0.2376\n",
      "     16        0.3832  0.2485\n",
      "     17        0.3750  0.2246\n",
      "     18        0.4190  0.2425\n",
      "     19        0.4110  0.2465\n",
      "     20        0.3134  0.2505\n",
      "     21        0.2680  0.2371\n",
      "     22        0.3039  0.2495\n",
      "     23        0.3164  0.2480\n",
      "     24        0.3087  0.2435\n",
      "     25        0.2683  0.2510\n",
      "     26        0.3108  0.2505\n",
      "     27        0.2708  0.2495\n",
      "     28        0.2714  0.2470\n",
      "     29        0.3591  0.2406\n",
      "     30        0.2803  0.2386\n",
      "==========Fold:6==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2145  0.4641\n",
      "      2        0.1623  0.4741\n",
      "      3        0.1502  0.4731\n",
      "      4        0.1401  0.4666\n",
      "      5        0.1345  0.4946\n",
      "      6        0.1251  0.4881\n",
      "      7        0.1197  0.4736\n",
      "      8        0.1131  0.4626\n",
      "      9        0.1075  0.4851\n",
      "     10        0.1049  0.4931\n",
      "     11        0.1006  0.4821\n",
      "     12        0.0940  0.4811\n",
      "     13        0.0962  0.4836\n",
      "     14        0.0899  0.4372\n",
      "     15        0.0861  0.4661\n",
      "     16        0.0839  0.4821\n",
      "     17        0.0823  0.4766\n",
      "     18        0.0806  0.4691\n",
      "     19        0.0825  0.4611\n",
      "     20        0.0747  0.4606\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0648  0.2620\n",
      "      2        1.6649  0.2371\n",
      "      3        1.3115  0.2331\n",
      "      4        1.0806  0.2386\n",
      "      5        0.9110  0.2560\n",
      "      6        0.8126  0.2426\n",
      "      7        0.7863  0.2485\n",
      "      8        0.7269  0.2540\n",
      "      9        0.6335  0.2460\n",
      "     10        0.6010  0.2401\n",
      "     11        0.5178  0.2460\n",
      "     12        0.4864  0.2336\n",
      "     13        0.4921  0.2530\n",
      "     14        0.4740  0.2440\n",
      "     15        0.4831  0.2465\n",
      "     16        0.4269  0.2566\n",
      "     17        0.3662  0.2435\n",
      "     18        0.3729  0.2435\n",
      "     19        0.3613  0.2401\n",
      "     20        0.2923  0.2361\n",
      "     21        0.3067  0.2505\n",
      "     22        0.3241  0.2490\n",
      "     23        0.3564  0.2301\n",
      "     24        0.2930  0.2346\n",
      "     25        0.2790  0.2351\n",
      "     26        0.2430  0.2455\n",
      "     27        0.3533  0.2475\n",
      "     28        0.3731  0.2411\n",
      "     29        0.2976  0.2450\n",
      "     30        0.3478  0.2401\n",
      "==========Fold:7==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2235  0.4766\n",
      "      2        0.1636  0.4806\n",
      "      3        0.1518  0.4811\n",
      "      4        0.1428  0.4736\n",
      "      5        0.1351  0.4656\n",
      "      6        0.1271  0.4876\n",
      "      7        0.1183  0.4776\n",
      "      8        0.1146  0.4547\n",
      "      9        0.1143  0.4811\n",
      "     10        0.1070  0.4886\n",
      "     11        0.1002  0.5011\n",
      "     12        0.0958  0.4846\n",
      "     13        0.0941  0.4746\n",
      "     14        0.0886  0.4676\n",
      "     15        0.0868  0.4616\n",
      "     16        0.0767  0.4916\n",
      "     17        0.0832  0.4931\n",
      "     18        0.0809  0.4826\n",
      "     19        0.0735  0.4831\n",
      "     20        0.0791  0.4841\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0964  0.2445\n",
      "      2        1.7395  0.2745\n",
      "      3        1.3627  0.2675\n",
      "      4        1.0953  0.2585\n",
      "      5        0.9472  0.2455\n",
      "      6        0.8136  0.2356\n",
      "      7        0.7776  0.2545\n",
      "      8        0.6506  0.2475\n",
      "      9        0.6079  0.2470\n",
      "     10        0.5739  0.2525\n",
      "     11        0.5234  0.2421\n",
      "     12        0.4659  0.2500\n",
      "     13        0.4711  0.2515\n",
      "     14        0.4401  0.2416\n",
      "     15        0.4642  0.2465\n",
      "     16        0.4319  0.2545\n",
      "     17        0.3968  0.2465\n",
      "     18        0.3874  0.2485\n",
      "     19        0.3380  0.2331\n",
      "     20        0.3033  0.2450\n",
      "     21        0.2933  0.2520\n",
      "     22        0.3279  0.2440\n",
      "     23        0.3106  0.2790\n",
      "     24        0.3958  0.2455\n",
      "     25        0.3370  0.2450\n",
      "     26        0.2939  0.2371\n",
      "     27        0.2638  0.2470\n",
      "     28        0.2260  0.2450\n",
      "     29        0.2827  0.2610\n",
      "     30        0.2373  0.2281\n",
      "==========Fold:8==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2564  0.4781\n",
      "      2        0.1666  0.4926\n",
      "      3        0.1524  0.5260\n",
      "      4        0.1434  0.4851\n",
      "      5        0.1343  0.4986\n",
      "      6        0.1269  0.4846\n",
      "      7        0.1217  0.4492\n",
      "      8        0.1154  0.4781\n",
      "      9        0.1129  0.4671\n",
      "     10        0.1100  0.4746\n",
      "     11        0.1024  0.4836\n",
      "     12        0.0995  0.4761\n",
      "     13        0.0916  0.4956\n",
      "     14        0.0949  0.4691\n",
      "     15        0.0916  0.4786\n",
      "     16        0.0852  0.4601\n",
      "     17        0.0834  0.4881\n",
      "     18        0.0837  0.4716\n",
      "     19        0.0828  0.4661\n",
      "     20        0.0763  0.4816\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0482  0.2430\n",
      "      2        1.6683  0.2510\n",
      "      3        1.3363  0.2445\n",
      "      4        1.0879  0.2545\n",
      "      5        0.9521  0.2485\n",
      "      6        0.8526  0.2525\n",
      "      7        0.7826  0.2271\n",
      "      8        0.7028  0.2535\n",
      "      9        0.6465  0.2480\n",
      "     10        0.5408  0.2411\n",
      "     11        0.5801  0.2361\n",
      "     12        0.5674  0.2401\n",
      "     13        0.5189  0.2391\n",
      "     14        0.4348  0.2336\n",
      "     15        0.4597  0.2445\n",
      "     16        0.4226  0.2565\n",
      "     17        0.5027  0.2520\n",
      "     18        0.3675  0.2505\n",
      "     19        0.3430  0.2515\n",
      "     20        0.2917  0.2391\n",
      "     21        0.2979  0.2351\n",
      "     22        0.2902  0.2391\n",
      "     23        0.3504  0.2450\n",
      "     24        0.3508  0.2460\n",
      "     25        0.2837  0.2495\n",
      "     26        0.2999  0.2376\n",
      "     27        0.2868  0.2570\n",
      "     28        0.2528  0.2430\n",
      "     29        0.2209  0.2425\n",
      "     30        0.1820  0.2445\n",
      "==========Fold:9==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2037  0.4641\n",
      "      2        0.1704  0.4966\n",
      "      3        0.1548  0.4776\n",
      "      4        0.1470  0.4831\n",
      "      5        0.1365  0.4781\n",
      "      6        0.1253  0.4781\n",
      "      7        0.1233  0.4896\n",
      "      8        0.1157  0.4776\n",
      "      9        0.1111  0.4876\n",
      "     10        0.1023  0.4741\n",
      "     11        0.0995  0.4766\n",
      "     12        0.1018  0.4736\n",
      "     13        0.0881  0.4846\n",
      "     14        0.0910  0.4696\n",
      "     15        0.0898  0.4701\n",
      "     16        0.0833  0.4746\n",
      "     17        0.0811  0.4821\n",
      "     18        0.0789  0.4547\n",
      "     19        0.0795  0.4472\n",
      "     20        0.0774  0.4796\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0795  0.2460\n",
      "      2        1.7471  0.2470\n",
      "      3        1.4242  0.2430\n",
      "      4        1.1718  0.2421\n",
      "      5        1.0088  0.2430\n",
      "      6        0.8884  0.2356\n",
      "      7        0.7918  0.2555\n",
      "      8        0.7363  0.2401\n",
      "      9        0.6937  0.2475\n",
      "     10        0.6083  0.2445\n",
      "     11        0.5857  0.2550\n",
      "     12        0.6431  0.2396\n",
      "     13        0.5277  0.2435\n",
      "     14        0.5870  0.2316\n",
      "     15        0.4669  0.2291\n",
      "     16        0.3827  0.2470\n",
      "     17        0.3877  0.2525\n",
      "     18        0.3715  0.2525\n",
      "     19        0.3891  0.2391\n",
      "     20        0.3768  0.2455\n",
      "     21        0.3255  0.2416\n",
      "     22        0.2947  0.2281\n",
      "     23        0.3702  0.2356\n",
      "     24        0.3514  0.2595\n",
      "     25        0.2852  0.2555\n",
      "     26        0.2626  0.2540\n",
      "     27        0.2713  0.2406\n",
      "     28        0.2680  0.2356\n",
      "     29        0.3528  0.2271\n",
      "     30        0.3031  0.2505\n",
      "==========Fold:10==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2277  0.4721\n",
      "      2        0.1675  0.4701\n",
      "      3        0.1538  0.5170\n",
      "      4        0.1419  0.4851\n",
      "      5        0.1358  0.4866\n",
      "      6        0.1299  0.4731\n",
      "      7        0.1190  0.4691\n",
      "      8        0.1147  0.4766\n",
      "      9        0.1113  0.4851\n",
      "     10        0.1010  0.4636\n",
      "     11        0.1066  0.4651\n",
      "     12        0.0951  0.4631\n",
      "     13        0.0944  0.4756\n",
      "     14        0.0835  0.4941\n",
      "     15        0.0899  0.4641\n",
      "     16        0.0935  0.4711\n",
      "     17        0.0816  0.4821\n",
      "     18        0.0833  0.4711\n",
      "     19        0.0812  0.4771\n",
      "     20        0.0788  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0802  0.2366\n",
      "      2        1.6595  0.2490\n",
      "      3        1.2989  0.2455\n",
      "      4        1.0427  0.2510\n",
      "      5        0.9530  0.2421\n",
      "      6        0.8009  0.2371\n",
      "      7        0.7290  0.2450\n",
      "      8        0.6675  0.2495\n",
      "      9        0.5715  0.2396\n",
      "     10        0.5498  0.2401\n",
      "     11        0.5067  0.2455\n",
      "     12        0.4228  0.2525\n",
      "     13        0.4183  0.2430\n",
      "     14        0.4098  0.2495\n",
      "     15        0.3955  0.2510\n",
      "     16        0.3472  0.2435\n",
      "     17        0.3094  0.2525\n",
      "     18        0.2827  0.2530\n",
      "     19        0.2718  0.2475\n",
      "     20        0.3144  0.2500\n",
      "     21        0.3152  0.2396\n",
      "     22        0.3015  0.2480\n",
      "     23        0.2926  0.2435\n",
      "     24        0.2371  0.2475\n",
      "     25        0.2588  0.2580\n",
      "     26        0.2572  0.2346\n",
      "     27        0.2334  0.2530\n",
      "     28        0.2897  0.2401\n",
      "     29        0.3336  0.2425\n",
      "     30        0.3250  0.2445\n",
      "==========Fold:11==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2334  0.4552\n",
      "      2        0.1666  0.4866\n",
      "      3        0.1547  0.4816\n",
      "      4        0.1447  0.4786\n",
      "      5        0.1368  0.4781\n",
      "      6        0.1298  0.4856\n",
      "      7        0.1240  0.4656\n",
      "      8        0.1180  0.4606\n",
      "      9        0.1142  0.4866\n",
      "     10        0.1072  0.4676\n",
      "     11        0.1061  0.4716\n",
      "     12        0.1002  0.4876\n",
      "     13        0.0939  0.4776\n",
      "     14        0.0906  0.4816\n",
      "     15        0.0908  0.4751\n",
      "     16        0.0891  0.4567\n",
      "     17        0.0854  0.4901\n",
      "     18        0.0811  0.4851\n",
      "     19        0.0788  0.4711\n",
      "     20        0.0804  0.4377\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0745  0.2440\n",
      "      2        1.6982  0.2445\n",
      "      3        1.3241  0.2490\n",
      "      4        1.0992  0.2311\n",
      "      5        0.9194  0.2555\n",
      "      6        0.8089  0.2396\n",
      "      7        0.7689  0.2381\n",
      "      8        0.7329  0.2430\n",
      "      9        0.6031  0.2530\n",
      "     10        0.6027  0.2775\n",
      "     11        0.5259  0.2545\n",
      "     12        0.4515  0.2480\n",
      "     13        0.4449  0.2416\n",
      "     14        0.4030  0.2490\n",
      "     15        0.3586  0.2406\n",
      "     16        0.4442  0.2396\n",
      "     17        0.4231  0.2460\n",
      "     18        0.3662  0.2470\n",
      "     19        0.2972  0.2470\n",
      "     20        0.2742  0.2326\n",
      "     21        0.3553  0.2341\n",
      "     22        0.5292  0.2445\n",
      "     23        0.3906  0.2470\n",
      "     24        0.2628  0.2485\n",
      "     25        0.2354  0.2510\n",
      "     26        0.2442  0.2421\n",
      "     27        0.2187  0.2515\n",
      "     28        0.2124  0.2460\n",
      "     29        0.2245  0.2296\n",
      "     30        0.2179  0.2460\n",
      "==========Fold:12==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2238  0.4731\n",
      "      2        0.1649  0.4921\n",
      "      3        0.1543  0.4686\n",
      "      4        0.1434  0.4736\n",
      "      5        0.1357  0.4966\n",
      "      6        0.1248  0.4816\n",
      "      7        0.1225  0.5021\n",
      "      8        0.1150  0.4846\n",
      "      9        0.1092  0.4686\n",
      "     10        0.1057  0.4736\n",
      "     11        0.1012  0.4771\n",
      "     12        0.0984  0.4916\n",
      "     13        0.1024  0.4601\n",
      "     14        0.0996  0.4701\n",
      "     15        0.0946  0.4816\n",
      "     16        0.0860  0.4756\n",
      "     17        0.0879  0.4786\n",
      "     18        0.0842  0.4751\n",
      "     19        0.0860  0.4866\n",
      "     20        0.0790  0.4801\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1047  0.2510\n",
      "      2        1.7408  0.2585\n",
      "      3        1.3954  0.2485\n",
      "      4        1.1168  0.2231\n",
      "      5        0.9174  0.2505\n",
      "      6        0.8238  0.2540\n",
      "      7        0.7041  0.2450\n",
      "      8        0.6884  0.2401\n",
      "      9        0.6134  0.2460\n",
      "     10        0.5587  0.2450\n",
      "     11        0.5263  0.2460\n",
      "     12        0.4668  0.2450\n",
      "     13        0.4164  0.2430\n",
      "     14        0.4243  0.2565\n",
      "     15        0.4232  0.2366\n",
      "     16        0.3633  0.2396\n",
      "     17        0.4022  0.2406\n",
      "     18        0.4317  0.2286\n",
      "     19        0.3614  0.2485\n",
      "     20        0.2986  0.2450\n",
      "     21        0.2660  0.2351\n",
      "     22        0.2868  0.2455\n",
      "     23        0.3021  0.2306\n",
      "     24        0.3109  0.2450\n",
      "     25        0.2610  0.2435\n",
      "     26        0.2617  0.2500\n",
      "     27        0.3083  0.2555\n",
      "     28        0.2405  0.2470\n",
      "     29        0.2226  0.2406\n",
      "     30        0.2078  0.2515\n",
      "==========Fold:13==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2242  0.4671\n",
      "      2        0.1698  0.4811\n",
      "      3        0.1567  0.4936\n",
      "      4        0.1476  0.4691\n",
      "      5        0.1380  0.4801\n",
      "      6        0.1333  0.4756\n",
      "      7        0.1264  0.4846\n",
      "      8        0.1199  0.4781\n",
      "      9        0.1159  0.4761\n",
      "     10        0.1135  0.4746\n",
      "     11        0.1012  0.4901\n",
      "     12        0.1063  0.5006\n",
      "     13        0.0997  0.4661\n",
      "     14        0.1026  0.4701\n",
      "     15        0.0947  0.4626\n",
      "     16        0.1022  0.4941\n",
      "     17        0.0928  0.4856\n",
      "     18        0.0888  0.4701\n",
      "     19        0.0983  0.4841\n",
      "     20        0.0840  0.4776\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0721  0.2406\n",
      "      2        1.6467  0.2555\n",
      "      3        1.3244  0.2351\n",
      "      4        1.0863  0.2326\n",
      "      5        0.9373  0.2480\n",
      "      6        0.8133  0.2386\n",
      "      7        0.7480  0.2470\n",
      "      8        0.7496  0.2416\n",
      "      9        0.6130  0.2510\n",
      "     10        0.5446  0.2391\n",
      "     11        0.4900  0.2520\n",
      "     12        0.4715  0.2510\n",
      "     13        0.4593  0.2530\n",
      "     14        0.4130  0.2465\n",
      "     15        0.4144  0.2515\n",
      "     16        0.4425  0.2470\n",
      "     17        0.3899  0.2445\n",
      "     18        0.3510  0.2500\n",
      "     19        0.3227  0.2440\n",
      "     20        0.3359  0.2411\n",
      "     21        0.5283  0.2411\n",
      "     22        0.3517  0.2475\n",
      "     23        0.2809  0.2331\n",
      "     24        0.2536  0.2505\n",
      "     25        0.2366  0.2416\n",
      "     26        0.2868  0.2381\n",
      "     27        0.2501  0.2396\n",
      "     28        0.2356  0.2416\n",
      "     29        0.2479  0.2475\n",
      "     30        0.2194  0.2575\n",
      "==========Fold:14==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2373  0.4996\n",
      "      2        0.1682  0.4806\n",
      "      3        0.1561  0.4726\n",
      "      4        0.1467  0.4856\n",
      "      5        0.1379  0.4746\n",
      "      6        0.1274  0.4756\n",
      "      7        0.1239  0.4781\n",
      "      8        0.1217  0.4886\n",
      "      9        0.1103  0.4736\n",
      "     10        0.1093  0.4871\n",
      "     11        0.1079  0.4591\n",
      "     12        0.1038  0.4866\n",
      "     13        0.1007  0.4891\n",
      "     14        0.0968  0.5290\n",
      "     15        0.0993  0.4841\n",
      "     16        0.0908  0.4871\n",
      "     17        0.0903  0.4696\n",
      "     18        0.0904  0.5160\n",
      "     19        0.0925  0.4666\n",
      "     20        0.0909  0.4871\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0544  0.2515\n",
      "      2        1.6975  0.2430\n",
      "      3        1.3587  0.2585\n",
      "      4        1.1847  0.2545\n",
      "      5        0.9923  0.2475\n",
      "      6        0.8551  0.2575\n",
      "      7        0.8109  0.2411\n",
      "      8        0.7676  0.2475\n",
      "      9        0.6636  0.2455\n",
      "     10        0.6541  0.2406\n",
      "     11        0.5724  0.2460\n",
      "     12        0.5636  0.2450\n",
      "     13        0.4870  0.2560\n",
      "     14        0.5087  0.2445\n",
      "     15        0.4648  0.2381\n",
      "     16        0.4417  0.2540\n",
      "     17        0.3972  0.2565\n",
      "     18        0.3686  0.2590\n",
      "     19        0.4066  0.2391\n",
      "     20        0.3731  0.2560\n",
      "     21        0.3607  0.2505\n",
      "     22        0.3727  0.2495\n",
      "     23        0.3681  0.2475\n",
      "     24        0.3101  0.2421\n",
      "     25        0.3100  0.2430\n",
      "     26        0.2961  0.2391\n",
      "     27        0.2648  0.2505\n",
      "     28        0.2367  0.2610\n",
      "     29        0.2257  0.2440\n",
      "     30        0.2216  0.2391\n",
      "==========Fold:15==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2280  0.4751\n",
      "      2        0.1652  0.4696\n",
      "      3        0.1541  0.4731\n",
      "      4        0.1434  0.5096\n",
      "      5        0.1342  0.4856\n",
      "      6        0.1298  0.4746\n",
      "      7        0.1250  0.4721\n",
      "      8        0.1165  0.4557\n",
      "      9        0.1148  0.4976\n",
      "     10        0.1065  0.4896\n",
      "     11        0.1022  0.4931\n",
      "     12        0.0980  0.4911\n",
      "     13        0.0972  0.4646\n",
      "     14        0.0927  0.4686\n",
      "     15        0.0898  0.4861\n",
      "     16        0.0884  0.4961\n",
      "     17        0.0853  0.5051\n",
      "     18        0.0826  0.4946\n",
      "     19        0.0828  0.4696\n",
      "     20        0.0724  0.4986\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0799  0.2465\n",
      "      2        1.6431  0.2475\n",
      "      3        1.2978  0.2475\n",
      "      4        1.1068  0.2336\n",
      "      5        0.9814  0.2425\n",
      "      6        0.8674  0.2401\n",
      "      7        0.7619  0.2510\n",
      "      8        0.7231  0.2440\n",
      "      9        0.6398  0.2515\n",
      "     10        0.6064  0.2565\n",
      "     11        0.5564  0.2450\n",
      "     12        0.5085  0.2316\n",
      "     13        0.4711  0.2535\n",
      "     14        0.5125  0.2435\n",
      "     15        0.5091  0.2330\n",
      "     16        0.4395  0.2465\n",
      "     17        0.3826  0.2445\n",
      "     18        0.3613  0.2570\n",
      "     19        0.3445  0.2326\n",
      "     20        0.3812  0.2510\n",
      "     21        0.3839  0.2286\n",
      "     22        0.3285  0.2401\n",
      "     23        0.3011  0.2575\n",
      "     24        0.2647  0.2570\n",
      "     25        0.2671  0.2480\n",
      "     26        0.2654  0.2366\n",
      "     27        0.2594  0.2396\n",
      "     28        0.2805  0.2450\n",
      "     29        0.3004  0.2391\n",
      "     30        0.3078  0.2286\n",
      "==========Fold:16==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2629  0.4836\n",
      "      2        0.1662  0.4906\n",
      "      3        0.1523  0.4681\n",
      "      4        0.1403  0.4841\n",
      "      5        0.1333  0.4786\n",
      "      6        0.1295  0.4816\n",
      "      7        0.1220  0.4776\n",
      "      8        0.1176  0.4816\n",
      "      9        0.1113  0.4751\n",
      "     10        0.1040  0.4751\n",
      "     11        0.1018  0.4911\n",
      "     12        0.0997  0.4771\n",
      "     13        0.0980  0.4866\n",
      "     14        0.0905  0.4966\n",
      "     15        0.0872  0.4881\n",
      "     16        0.0887  0.4811\n",
      "     17        0.0826  0.4876\n",
      "     18        0.0828  0.4821\n",
      "     19        0.0874  0.4956\n",
      "     20        0.0763  0.4666\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0668  0.2535\n",
      "      2        1.5820  0.2455\n",
      "      3        1.1940  0.2505\n",
      "      4        1.0262  0.2515\n",
      "      5        0.8717  0.2490\n",
      "      6        0.7996  0.2655\n",
      "      7        0.6874  0.2495\n",
      "      8        0.6418  0.2450\n",
      "      9        0.6201  0.2401\n",
      "     10        0.5636  0.2560\n",
      "     11        0.5746  0.2500\n",
      "     12        0.5312  0.2490\n",
      "     13        0.4871  0.2535\n",
      "     14        0.4296  0.2505\n",
      "     15        0.4288  0.2425\n",
      "     16        0.4074  0.2570\n",
      "     17        0.3391  0.2475\n",
      "     18        0.3506  0.2575\n",
      "     19        0.3626  0.2490\n",
      "     20        0.3686  0.2490\n",
      "     21        0.3333  0.2430\n",
      "     22        0.4120  0.2545\n",
      "     23        0.3664  0.2440\n",
      "     24        0.2987  0.2391\n",
      "     25        0.2876  0.2470\n",
      "     26        0.2603  0.2525\n",
      "     27        0.3425  0.2515\n",
      "     28        0.2932  0.2505\n",
      "     29        0.2269  0.2361\n",
      "     30        0.2340  0.2630\n",
      "==========Fold:17==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2295  0.4766\n",
      "      2        0.1690  0.5175\n",
      "      3        0.1533  0.4881\n",
      "      4        0.1429  0.4711\n",
      "      5        0.1354  0.4661\n",
      "      6        0.1289  0.4841\n",
      "      7        0.1226  0.4711\n",
      "      8        0.1198  0.5011\n",
      "      9        0.1109  0.4971\n",
      "     10        0.1086  0.4696\n",
      "     11        0.1038  0.4846\n",
      "     12        0.0992  0.4826\n",
      "     13        0.1012  0.4756\n",
      "     14        0.0918  0.4656\n",
      "     15        0.0896  0.4801\n",
      "     16        0.0880  0.4691\n",
      "     17        0.0866  0.4851\n",
      "     18        0.0846  0.4801\n",
      "     19        0.0828  0.4731\n",
      "     20        0.0797  0.4542\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0925  0.2475\n",
      "      2        1.7053  0.2450\n",
      "      3        1.2707  0.2435\n",
      "      4        1.0595  0.2565\n",
      "      5        0.8624  0.2401\n",
      "      6        0.7791  0.2430\n",
      "      7        0.6592  0.2545\n",
      "      8        0.6579  0.2465\n",
      "      9        0.5786  0.2560\n",
      "     10        0.6104  0.2276\n",
      "     11        0.5403  0.2490\n",
      "     12        0.4418  0.2316\n",
      "     13        0.4448  0.2465\n",
      "     14        0.4525  0.2450\n",
      "     15        0.4239  0.2560\n",
      "     16        0.4454  0.2520\n",
      "     17        0.4004  0.2510\n",
      "     18        0.3434  0.2381\n",
      "     19        0.3326  0.2480\n",
      "     20        0.4304  0.2560\n",
      "     21        0.3499  0.2485\n",
      "     22        0.2796  0.2585\n",
      "     23        0.2759  0.2535\n",
      "     24        0.2716  0.2545\n",
      "     25        0.3738  0.2376\n",
      "     26        0.3259  0.2520\n",
      "     27        0.3202  0.2396\n",
      "     28        0.2600  0.2505\n",
      "     29        0.2164  0.2555\n",
      "     30        0.2324  0.2450\n",
      "==========Fold:18==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2592  0.4671\n",
      "      2        0.1685  0.4576\n",
      "      3        0.1548  0.4791\n",
      "      4        0.1482  0.4741\n",
      "      5        0.1411  0.4796\n",
      "      6        0.1328  0.4706\n",
      "      7        0.1252  0.4821\n",
      "      8        0.1251  0.4806\n",
      "      9        0.1170  0.4851\n",
      "     10        0.1113  0.4726\n",
      "     11        0.1108  0.4696\n",
      "     12        0.1065  0.4921\n",
      "     13        0.1028  0.4696\n",
      "     14        0.0996  0.4786\n",
      "     15        0.0973  0.4801\n",
      "     16        0.0926  0.4786\n",
      "     17        0.0942  0.5026\n",
      "     18        0.0947  0.4856\n",
      "     19        0.0883  0.4626\n",
      "     20        0.0904  0.4861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0779  0.2505\n",
      "      2        1.6157  0.2830\n",
      "      3        1.2637  0.2485\n",
      "      4        1.0294  0.2425\n",
      "      5        0.8262  0.2356\n",
      "      6        0.7709  0.2525\n",
      "      7        0.7267  0.2545\n",
      "      8        0.6219  0.2510\n",
      "      9        0.5565  0.2540\n",
      "     10        0.5427  0.2440\n",
      "     11        0.4643  0.2505\n",
      "     12        0.4541  0.2515\n",
      "     13        0.3991  0.2585\n",
      "     14        0.3732  0.2416\n",
      "     15        0.3471  0.2460\n",
      "     16        0.2976  0.2595\n",
      "     17        0.4755  0.2505\n",
      "     18        0.4438  0.2550\n",
      "     19        0.3540  0.2555\n",
      "     20        0.2931  0.2510\n",
      "     21        0.2437  0.2420\n",
      "     22        0.2507  0.2570\n",
      "     23        0.2258  0.2416\n",
      "     24        0.2104  0.2505\n",
      "     25        0.2303  0.2480\n",
      "     26        0.2074  0.2475\n",
      "     27        0.2440  0.2560\n",
      "     28        0.3881  0.2341\n",
      "     29        0.2788  0.2460\n",
      "     30        0.1874  0.2525\n",
      "==========Fold:19==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2517  0.4661\n",
      "      2        0.1688  0.4911\n",
      "      3        0.1549  0.4946\n",
      "      4        0.1442  0.4661\n",
      "      5        0.1354  0.4811\n",
      "      6        0.1287  0.4891\n",
      "      7        0.1207  0.4856\n",
      "      8        0.1168  0.4721\n",
      "      9        0.1093  0.4986\n",
      "     10        0.1058  0.4811\n",
      "     11        0.0985  0.4931\n",
      "     12        0.0968  0.4731\n",
      "     13        0.0936  0.4746\n",
      "     14        0.0913  0.4746\n",
      "     15        0.0860  0.4761\n",
      "     16        0.0835  0.4971\n",
      "     17        0.0855  0.4886\n",
      "     18        0.0786  0.4906\n",
      "     19        0.0794  0.4851\n",
      "     20        0.0743  0.4886\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0855  0.2406\n",
      "      2        1.7243  0.2535\n",
      "      3        1.3652  0.2485\n",
      "      4        1.1427  0.2505\n",
      "      5        0.9810  0.2545\n",
      "      6        0.9095  0.2460\n",
      "      7        0.7730  0.2435\n",
      "      8        0.7285  0.2495\n",
      "      9        0.6577  0.2490\n",
      "     10        0.6346  0.2540\n",
      "     11        0.6342  0.2465\n",
      "     12        0.4961  0.2460\n",
      "     13        0.4763  0.2545\n",
      "     14        0.4952  0.2391\n",
      "     15        0.4409  0.2396\n",
      "     16        0.3888  0.2480\n",
      "     17        0.3458  0.2490\n",
      "     18        0.3580  0.2490\n",
      "     19        0.3774  0.2450\n",
      "     20        0.4183  0.2381\n",
      "     21        0.4143  0.2361\n",
      "     22        0.3334  0.2540\n",
      "     23        0.2748  0.2555\n",
      "     24        0.2671  0.2585\n",
      "     25        0.2720  0.2535\n",
      "     26        0.2475  0.2440\n",
      "     27        0.2651  0.2550\n",
      "     28        0.3818  0.2485\n",
      "     29        0.2912  0.2366\n",
      "     30        0.2324  0.2535\n",
      "==========Fold:20==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2403  0.4751\n",
      "      2        0.1650  0.4786\n",
      "      3        0.1538  0.4711\n",
      "      4        0.1458  0.4731\n",
      "      5        0.1378  0.4646\n",
      "      6        0.1301  0.4721\n",
      "      7        0.1208  0.4856\n",
      "      8        0.1206  0.4741\n",
      "      9        0.1134  0.5160\n",
      "     10        0.1064  0.5165\n",
      "     11        0.1039  0.4716\n",
      "     12        0.0970  0.4557\n",
      "     13        0.1032  0.4701\n",
      "     14        0.0897  0.4761\n",
      "     15        0.0931  0.4776\n",
      "     16        0.0938  0.4796\n",
      "     17        0.0836  0.4831\n",
      "     18        0.0833  0.4871\n",
      "     19        0.0818  0.4886\n",
      "     20        0.0765  0.4856\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0897  0.2411\n",
      "      2        1.7099  0.2475\n",
      "      3        1.3683  0.2495\n",
      "      4        1.1000  0.2440\n",
      "      5        0.9729  0.2475\n",
      "      6        0.8481  0.2595\n",
      "      7        0.7605  0.2366\n",
      "      8        0.7069  0.2560\n",
      "      9        0.6202  0.2515\n",
      "     10        0.5405  0.2430\n",
      "     11        0.5225  0.2570\n",
      "     12        0.4976  0.2515\n",
      "     13        0.4987  0.2495\n",
      "     14        0.4628  0.2560\n",
      "     15        0.4131  0.2450\n",
      "     16        0.3946  0.2585\n",
      "     17        0.3826  0.2420\n",
      "     18        0.3681  0.2515\n",
      "     19        0.3584  0.2535\n",
      "     20        0.3348  0.2530\n",
      "     21        0.3338  0.2610\n",
      "     22        0.3853  0.2485\n",
      "     23        0.2864  0.2411\n",
      "     24        0.2539  0.2490\n",
      "     25        0.2616  0.2540\n",
      "     26        0.2528  0.2326\n",
      "     27        0.2711  0.2535\n",
      "     28        0.2910  0.2381\n",
      "     29        0.2514  0.2465\n",
      "     30        0.2216  0.2495\n",
      "==========Fold:21==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2314  0.4736\n",
      "      2        0.1703  0.4641\n",
      "      3        0.1567  0.4886\n",
      "      4        0.1466  0.4861\n",
      "      5        0.1370  0.4646\n",
      "      6        0.1308  0.4581\n",
      "      7        0.1250  0.4956\n",
      "      8        0.1175  0.5101\n",
      "      9        0.1138  0.5180\n",
      "     10        0.1083  0.4891\n",
      "     11        0.1026  0.4701\n",
      "     12        0.1013  0.5255\n",
      "     13        0.0960  0.4766\n",
      "     14        0.0954  0.4696\n",
      "     15        0.0941  0.4916\n",
      "     16        0.0863  0.4641\n",
      "     17        0.0880  0.4861\n",
      "     18        0.0772  0.4961\n",
      "     19        0.0828  0.4991\n",
      "     20        0.0781  0.4891\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0877  0.2396\n",
      "      2        1.7088  0.2550\n",
      "      3        1.3426  0.2530\n",
      "      4        1.1459  0.2450\n",
      "      5        0.9415  0.2440\n",
      "      6        0.7899  0.2505\n",
      "      7        0.7200  0.2445\n",
      "      8        0.6353  0.2505\n",
      "      9        0.5980  0.2460\n",
      "     10        0.5567  0.2565\n",
      "     11        0.5837  0.2470\n",
      "     12        0.4714  0.2520\n",
      "     13        0.4699  0.2376\n",
      "     14        0.4198  0.2416\n",
      "     15        0.3742  0.2575\n",
      "     16        0.3940  0.2411\n",
      "     17        0.3938  0.2770\n",
      "     18        0.3633  0.2570\n",
      "     19        0.3309  0.2465\n",
      "     20        0.3667  0.2460\n",
      "     21        0.4422  0.2455\n",
      "     22        0.3287  0.2490\n",
      "     23        0.2632  0.2445\n",
      "     24        0.2636  0.2580\n",
      "     25        0.3366  0.2635\n",
      "     26        0.2919  0.2470\n",
      "     27        0.2920  0.2460\n",
      "     28        0.2644  0.2465\n",
      "     29        0.2539  0.2445\n",
      "     30        0.2613  0.2475\n",
      "==========Fold:22==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2228  0.4606\n",
      "      2        0.1681  0.4881\n",
      "      3        0.1568  0.4601\n",
      "      4        0.1478  0.4946\n",
      "      5        0.1410  0.4726\n",
      "      6        0.1305  0.4746\n",
      "      7        0.1243  0.4871\n",
      "      8        0.1226  0.4666\n",
      "      9        0.1182  0.4681\n",
      "     10        0.1119  0.4726\n",
      "     11        0.1141  0.4836\n",
      "     12        0.1064  0.4836\n",
      "     13        0.1000  0.4591\n",
      "     14        0.1018  0.4886\n",
      "     15        0.0958  0.4741\n",
      "     16        0.0991  0.4771\n",
      "     17        0.0915  0.4567\n",
      "     18        0.0922  0.4731\n",
      "     19        0.0890  0.4776\n",
      "     20        0.0890  0.4831\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0631  0.2510\n",
      "      2        1.6189  0.2545\n",
      "      3        1.2804  0.2515\n",
      "      4        1.0712  0.2351\n",
      "      5        0.8782  0.2515\n",
      "      6        0.8006  0.2391\n",
      "      7        0.7424  0.2490\n",
      "      8        0.6293  0.2545\n",
      "      9        0.6589  0.2445\n",
      "     10        0.5917  0.2396\n",
      "     11        0.4956  0.2440\n",
      "     12        0.5141  0.2600\n",
      "     13        0.4467  0.2515\n",
      "     14        0.4130  0.2500\n",
      "     15        0.4153  0.2550\n",
      "     16        0.3710  0.2336\n",
      "     17        0.3573  0.2425\n",
      "     18        0.4579  0.2371\n",
      "     19        0.3593  0.2535\n",
      "     20        0.3223  0.2316\n",
      "     21        0.2786  0.2460\n",
      "     22        0.2840  0.2470\n",
      "     23        0.2842  0.2525\n",
      "     24        0.2655  0.2450\n",
      "     25        0.2480  0.2460\n",
      "     26        0.3030  0.2470\n",
      "     27        0.2878  0.2371\n",
      "     28        0.2742  0.2480\n",
      "     29        0.2647  0.2465\n",
      "     30        0.2212  0.2545\n",
      "==========Fold:23==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2285  0.4791\n",
      "      2        0.1696  0.4876\n",
      "      3        0.1572  0.4926\n",
      "      4        0.1498  0.4736\n",
      "      5        0.1385  0.4726\n",
      "      6        0.1326  0.5086\n",
      "      7        0.1251  0.4891\n",
      "      8        0.1219  0.4756\n",
      "      9        0.1127  0.4711\n",
      "     10        0.1120  0.4601\n",
      "     11        0.1069  0.4726\n",
      "     12        0.1086  0.4881\n",
      "     13        0.1100  0.4771\n",
      "     14        0.0983  0.4636\n",
      "     15        0.0965  0.4876\n",
      "     16        0.1011  0.4831\n",
      "     17        0.0959  0.4946\n",
      "     18        0.0962  0.4751\n",
      "     19        0.0911  0.4621\n",
      "     20        0.0931  0.4941\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0860  0.2411\n",
      "      2        1.7077  0.2660\n",
      "      3        1.3330  0.2480\n",
      "      4        1.0831  0.2560\n",
      "      5        0.8795  0.2555\n",
      "      6        0.7910  0.2440\n",
      "      7        0.8217  0.2525\n",
      "      8        0.6274  0.2515\n",
      "      9        0.5932  0.2590\n",
      "     10        0.5145  0.2460\n",
      "     11        0.4734  0.2530\n",
      "     12        0.4445  0.2510\n",
      "     13        0.4495  0.2421\n",
      "     14        0.5201  0.2550\n",
      "     15        0.4848  0.2560\n",
      "     16        0.3624  0.2416\n",
      "     17        0.3337  0.2455\n",
      "     18        0.3440  0.2560\n",
      "     19        0.3158  0.2411\n",
      "     20        0.2871  0.2530\n",
      "     21        0.3268  0.2525\n",
      "     22        0.2956  0.2550\n",
      "     23        0.2970  0.2525\n",
      "     24        0.3882  0.2460\n",
      "     25        0.3267  0.2455\n",
      "     26        0.2922  0.2470\n",
      "     27        0.2599  0.2445\n",
      "     28        0.2240  0.2515\n",
      "     29        0.1982  0.2510\n",
      "     30        0.1947  0.2470\n",
      "==========Fold:24==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2397  0.4821\n",
      "      2        0.1656  0.4906\n",
      "      3        0.1540  0.4746\n",
      "      4        0.1421  0.4851\n",
      "      5        0.1337  0.4906\n",
      "      6        0.1260  0.4961\n",
      "      7        0.1190  0.4886\n",
      "      8        0.1165  0.4766\n",
      "      9        0.1087  0.4671\n",
      "     10        0.1045  0.4846\n",
      "     11        0.0989  0.4856\n",
      "     12        0.0930  0.4811\n",
      "     13        0.0950  0.4761\n",
      "     14        0.0893  0.4626\n",
      "     15        0.0903  0.4796\n",
      "     16        0.0869  0.4861\n",
      "     17        0.0830  0.5076\n",
      "     18        0.0838  0.4801\n",
      "     19        0.0809  0.4821\n",
      "     20        0.0774  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0976  0.2515\n",
      "      2        1.7464  0.2580\n",
      "      3        1.3958  0.2565\n",
      "      4        1.1671  0.2490\n",
      "      5        0.9931  0.2515\n",
      "      6        0.8565  0.2510\n",
      "      7        0.7911  0.2411\n",
      "      8        0.7083  0.2401\n",
      "      9        0.6183  0.2495\n",
      "     10        0.5765  0.2520\n",
      "     11        0.5205  0.2460\n",
      "     12        0.5349  0.2540\n",
      "     13        0.5100  0.2435\n",
      "     14        0.4534  0.2505\n",
      "     15        0.4910  0.2550\n",
      "     16        0.4359  0.2515\n",
      "     17        0.3538  0.2545\n",
      "     18        0.3396  0.2615\n",
      "     19        0.3174  0.2540\n",
      "     20        0.4292  0.2416\n",
      "     21        0.3023  0.2425\n",
      "     22        0.3402  0.2515\n",
      "     23        0.3161  0.2346\n",
      "     24        0.2634  0.2435\n",
      "     25        0.2849  0.2515\n",
      "     26        0.3621  0.2510\n",
      "     27        0.2544  0.2316\n",
      "     28        0.2162  0.2450\n",
      "     29        0.2203  0.2565\n",
      "     30        0.2453  0.2595\n",
      "==========Fold:25==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2554  0.5091\n",
      "      2        0.1667  0.4746\n",
      "      3        0.1517  0.4886\n",
      "      4        0.1435  0.4886\n",
      "      5        0.1351  0.4921\n",
      "      6        0.1275  0.4796\n",
      "      7        0.1220  0.4911\n",
      "      8        0.1150  0.4936\n",
      "      9        0.1069  0.4811\n",
      "     10        0.1071  0.4896\n",
      "     11        0.1000  0.4502\n",
      "     12        0.0972  0.4671\n",
      "     13        0.0975  0.4971\n",
      "     14        0.0940  0.4951\n",
      "     15        0.0856  0.4796\n",
      "     16        0.0859  0.4721\n",
      "     17        0.0810  0.4801\n",
      "     18        0.0791  0.4691\n",
      "     19        0.0824  0.4736\n",
      "     20        0.0721  0.4861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0645  0.2445\n",
      "      2        1.6232  0.2465\n",
      "      3        1.2961  0.2490\n",
      "      4        1.0756  0.2445\n",
      "      5        0.8701  0.2535\n",
      "      6        0.8064  0.2600\n",
      "      7        0.6953  0.2450\n",
      "      8        0.6513  0.2505\n",
      "      9        0.5436  0.2555\n",
      "     10        0.5344  0.2575\n",
      "     11        0.4782  0.2580\n",
      "     12        0.5174  0.2545\n",
      "     13        0.4800  0.2371\n",
      "     14        0.4263  0.2460\n",
      "     15        0.3402  0.2545\n",
      "     16        0.3574  0.2460\n",
      "     17        0.3537  0.2440\n",
      "     18        0.3197  0.2396\n",
      "     19        0.3542  0.2580\n",
      "     20        0.3445  0.2336\n",
      "     21        0.3025  0.2386\n",
      "     22        0.3015  0.2500\n",
      "     23        0.3105  0.2575\n",
      "     24        0.2593  0.2416\n",
      "     25        0.2704  0.2525\n",
      "     26        0.3188  0.2620\n",
      "     27        0.2551  0.2565\n",
      "     28        0.2255  0.2655\n",
      "     29        0.2055  0.2655\n",
      "     30        0.1930  0.2585\n",
      "==========Fold:26==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2726  0.4771\n",
      "      2        0.1683  0.4941\n",
      "      3        0.1556  0.4941\n",
      "      4        0.1454  0.4851\n",
      "      5        0.1358  0.4801\n",
      "      6        0.1294  0.4771\n",
      "      7        0.1228  0.4901\n",
      "      8        0.1201  0.4562\n",
      "      9        0.1160  0.4826\n",
      "     10        0.1187  0.4956\n",
      "     11        0.1052  0.4716\n",
      "     12        0.1075  0.4886\n",
      "     13        0.1026  0.4931\n",
      "     14        0.0969  0.4971\n",
      "     15        0.0951  0.4881\n",
      "     16        0.0893  0.4746\n",
      "     17        0.0949  0.4876\n",
      "     18        0.0954  0.4731\n",
      "     19        0.0848  0.4826\n",
      "     20        0.0804  0.4976\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1075  0.2650\n",
      "      2        1.7362  0.2500\n",
      "      3        1.3958  0.2535\n",
      "      4        1.1277  0.2565\n",
      "      5        1.0090  0.2505\n",
      "      6        0.8563  0.2525\n",
      "      7        0.7809  0.2545\n",
      "      8        0.6736  0.2520\n",
      "      9        0.6287  0.2475\n",
      "     10        0.5756  0.2615\n",
      "     11        0.5214  0.2605\n",
      "     12        0.5222  0.2495\n",
      "     13        0.4733  0.2540\n",
      "     14        0.4343  0.2460\n",
      "     15        0.3782  0.2495\n",
      "     16        0.3884  0.2495\n",
      "     17        0.4116  0.2515\n",
      "     18        0.3838  0.2391\n",
      "     19        0.4245  0.2500\n",
      "     20        0.3288  0.2600\n",
      "     21        0.2974  0.2585\n",
      "     22        0.3431  0.2565\n",
      "     23        0.3025  0.2455\n",
      "     24        0.2494  0.2565\n",
      "     25        0.2449  0.2530\n",
      "     26        0.2194  0.2600\n",
      "     27        0.2781  0.2336\n",
      "     28        0.2787  0.2560\n",
      "     29        0.2468  0.2411\n",
      "     30        0.2271  0.2440\n",
      "==========Fold:27==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2067  0.4906\n",
      "      2        0.1635  0.4781\n",
      "      3        0.1509  0.4811\n",
      "      4        0.1402  0.4741\n",
      "      5        0.1352  0.4941\n",
      "      6        0.1256  0.4886\n",
      "      7        0.1232  0.4891\n",
      "      8        0.1138  0.4726\n",
      "      9        0.1134  0.4946\n",
      "     10        0.1035  0.4911\n",
      "     11        0.1026  0.4871\n",
      "     12        0.0939  0.4761\n",
      "     13        0.0918  0.4936\n",
      "     14        0.0914  0.4741\n",
      "     15        0.0835  0.4736\n",
      "     16        0.0870  0.4666\n",
      "     17        0.0802  0.4676\n",
      "     18        0.0814  0.4951\n",
      "     19        0.0740  0.4836\n",
      "     20        0.0763  0.4796\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0784  0.2495\n",
      "      2        1.6963  0.2460\n",
      "      3        1.3706  0.2505\n",
      "      4        1.1097  0.2391\n",
      "      5        0.9418  0.2505\n",
      "      6        0.7990  0.2515\n",
      "      7        0.7089  0.2366\n",
      "      8        0.6758  0.2520\n",
      "      9        0.6282  0.2540\n",
      "     10        0.6086  0.2445\n",
      "     11        0.4896  0.2555\n",
      "     12        0.4150  0.2455\n",
      "     13        0.3806  0.2445\n",
      "     14        0.4376  0.2425\n",
      "     15        0.4884  0.2575\n",
      "     16        0.4022  0.2510\n",
      "     17        0.3666  0.2291\n",
      "     18        0.3233  0.2545\n",
      "     19        0.3364  0.2445\n",
      "     20        0.3435  0.2381\n",
      "     21        0.3157  0.2555\n",
      "     22        0.3096  0.2715\n",
      "     23        0.3094  0.2490\n",
      "     24        0.2585  0.2430\n",
      "     25        0.2280  0.2515\n",
      "     26        0.2662  0.2490\n",
      "     27        0.2725  0.2565\n",
      "     28        0.2306  0.2480\n",
      "     29        0.1772  0.2445\n",
      "     30        0.1814  0.2575\n",
      "==========Fold:28==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2216  0.4726\n",
      "      2        0.1675  0.5375\n",
      "      3        0.1564  0.4901\n",
      "      4        0.1461  0.4851\n",
      "      5        0.1369  0.5061\n",
      "      6        0.1341  0.5091\n",
      "      7        0.1239  0.4951\n",
      "      8        0.1165  0.4806\n",
      "      9        0.1115  0.4926\n",
      "     10        0.1111  0.4871\n",
      "     11        0.1049  0.4741\n",
      "     12        0.1045  0.5220\n",
      "     13        0.1010  0.4901\n",
      "     14        0.0967  0.5250\n",
      "     15        0.0932  0.5240\n",
      "     16        0.0901  0.5011\n",
      "     17        0.0873  0.4776\n",
      "     18        0.0843  0.5330\n",
      "     19        0.0805  0.4886\n",
      "     20        0.0823  0.4641\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0850  0.2545\n",
      "      2        1.7088  0.2485\n",
      "      3        1.3717  0.2605\n",
      "      4        1.1257  0.2495\n",
      "      5        0.9262  0.2525\n",
      "      6        0.8032  0.2490\n",
      "      7        0.7477  0.2411\n",
      "      8        0.6973  0.2460\n",
      "      9        0.6062  0.2361\n",
      "     10        0.6271  0.2416\n",
      "     11        0.5219  0.2396\n",
      "     12        0.5182  0.2540\n",
      "     13        0.4788  0.2326\n",
      "     14        0.5344  0.2560\n",
      "     15        0.4719  0.2425\n",
      "     16        0.3996  0.2515\n",
      "     17        0.3796  0.2435\n",
      "     18        0.3437  0.2460\n",
      "     19        0.3244  0.2790\n",
      "     20        0.4111  0.2565\n",
      "     21        0.3685  0.2575\n",
      "     22        0.4331  0.2500\n",
      "     23        0.3566  0.2540\n",
      "     24        0.2716  0.2555\n",
      "     25        0.2480  0.2515\n",
      "     26        0.2666  0.2515\n",
      "     27        0.2800  0.2530\n",
      "     28        0.3371  0.2480\n",
      "     29        0.2854  0.2600\n",
      "     30        0.2190  0.2480\n",
      "==========Fold:29==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2103  0.4801\n",
      "      2        0.1640  0.4836\n",
      "      3        0.1511  0.4811\n",
      "      4        0.1411  0.4826\n",
      "      5        0.1321  0.4866\n",
      "      6        0.1275  0.4691\n",
      "      7        0.1208  0.4806\n",
      "      8        0.1129  0.4851\n",
      "      9        0.1105  0.4646\n",
      "     10        0.1058  0.4881\n",
      "     11        0.1016  0.4896\n",
      "     12        0.0981  0.4856\n",
      "     13        0.0956  0.4746\n",
      "     14        0.0868  0.4801\n",
      "     15        0.0861  0.4781\n",
      "     16        0.0851  0.4696\n",
      "     17        0.0847  0.4981\n",
      "     18        0.0744  0.4861\n",
      "     19        0.0793  0.4926\n",
      "     20        0.0766  0.4866\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0169  0.2470\n",
      "      2        1.6235  0.2890\n",
      "      3        1.2999  0.2440\n",
      "      4        1.1173  0.2421\n",
      "      5        0.9903  0.2490\n",
      "      6        0.8434  0.2490\n",
      "      7        0.7370  0.2475\n",
      "      8        0.7150  0.2555\n",
      "      9        0.6356  0.2530\n",
      "     10        0.5923  0.2535\n",
      "     11        0.5473  0.2530\n",
      "     12        0.5094  0.2610\n",
      "     13        0.4776  0.2590\n",
      "     14        0.5130  0.2550\n",
      "     15        0.4355  0.2565\n",
      "     16        0.3737  0.2530\n",
      "     17        0.3182  0.2545\n",
      "     18        0.3119  0.2585\n",
      "     19        0.3852  0.2435\n",
      "     20        0.3468  0.2495\n",
      "     21        0.3092  0.2530\n",
      "     22        0.2807  0.2650\n",
      "     23        0.2692  0.2366\n",
      "     24        0.3398  0.2550\n",
      "     25        0.2793  0.2585\n",
      "     26        0.2181  0.2505\n",
      "     27        0.2225  0.2550\n",
      "     28        0.2535  0.2570\n",
      "     29        0.2429  0.2306\n",
      "     30        0.2234  0.2515\n",
      "==========Fold:30==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2424  0.4816\n",
      "      2        0.1651  0.4901\n",
      "      3        0.1515  0.4771\n",
      "      4        0.1401  0.4881\n",
      "      5        0.1344  0.4846\n",
      "      6        0.1261  0.4721\n",
      "      7        0.1193  0.4921\n",
      "      8        0.1168  0.4986\n",
      "      9        0.1109  0.4846\n",
      "     10        0.1044  0.4576\n",
      "     11        0.0975  0.4821\n",
      "     12        0.0954  0.4736\n",
      "     13        0.0932  0.4906\n",
      "     14        0.0917  0.4761\n",
      "     15        0.0879  0.5081\n",
      "     16        0.0862  0.4751\n",
      "     17        0.0813  0.4951\n",
      "     18        0.0773  0.4796\n",
      "     19        0.0805  0.4836\n",
      "     20        0.0773  0.4766\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0982  0.2376\n",
      "      2        1.7190  0.2525\n",
      "      3        1.3574  0.2480\n",
      "      4        1.0809  0.2570\n",
      "      5        0.9283  0.2600\n",
      "      6        0.8083  0.2525\n",
      "      7        0.7443  0.2490\n",
      "      8        0.7018  0.2540\n",
      "      9        0.6564  0.2570\n",
      "     10        0.5638  0.2470\n",
      "     11        0.5058  0.2386\n",
      "     12        0.5285  0.2525\n",
      "     13        0.4853  0.2565\n",
      "     14        0.4532  0.2780\n",
      "     15        0.3991  0.2590\n",
      "     16        0.4001  0.2530\n",
      "     17        0.4140  0.2475\n",
      "     18        0.3785  0.2535\n",
      "     19        0.3052  0.2550\n",
      "     20        0.2895  0.2515\n",
      "     21        0.3799  0.2460\n",
      "     22        0.3234  0.2435\n",
      "     23        0.2834  0.2371\n",
      "     24        0.3220  0.2505\n",
      "     25        0.3321  0.2386\n",
      "     26        0.2919  0.2510\n",
      "     27        0.2473  0.2500\n",
      "     28        0.2438  0.2490\n",
      "     29        0.2265  0.2535\n",
      "     30        0.2233  0.2585\n",
      "==========Fold:31==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2423  0.4761\n",
      "      2        0.1676  0.5495\n",
      "      3        0.1570  0.5365\n",
      "      4        0.1462  0.4921\n",
      "      5        0.1394  0.4926\n",
      "      6        0.1331  0.4961\n",
      "      7        0.1233  0.4936\n",
      "      8        0.1231  0.4951\n",
      "      9        0.1160  0.4866\n",
      "     10        0.1095  0.4896\n",
      "     11        0.1057  0.4736\n",
      "     12        0.1027  0.4981\n",
      "     13        0.0994  0.5096\n",
      "     14        0.0972  0.4931\n",
      "     15        0.0934  0.4891\n",
      "     16        0.0910  0.4991\n",
      "     17        0.0877  0.4836\n",
      "     18        0.0914  0.4851\n",
      "     19        0.0849  0.4886\n",
      "     20        0.0895  0.4816\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0862  0.2455\n",
      "      2        1.8096  0.2490\n",
      "      3        1.4996  0.2411\n",
      "      4        1.2547  0.2416\n",
      "      5        1.0347  0.2550\n",
      "      6        0.8920  0.2450\n",
      "      7        0.8076  0.2595\n",
      "      8        0.7560  0.2371\n",
      "      9        0.7158  0.2425\n",
      "     10        0.6355  0.2670\n",
      "     11        0.5592  0.2590\n",
      "     12        0.5485  0.2580\n",
      "     13        0.4814  0.2490\n",
      "     14        0.4509  0.2580\n",
      "     15        0.4675  0.2565\n",
      "     16        0.4512  0.2600\n",
      "     17        0.4297  0.2515\n",
      "     18        0.4439  0.2530\n",
      "     19        0.4308  0.2420\n",
      "     20        0.3432  0.2271\n",
      "     21        0.3366  0.2475\n",
      "     22        0.3175  0.2605\n",
      "     23        0.3580  0.2600\n",
      "     24        0.3392  0.2570\n",
      "     25        0.3185  0.2515\n",
      "     26        0.3448  0.2401\n",
      "     27        0.3069  0.2411\n",
      "     28        0.2897  0.2790\n",
      "     29        0.2437  0.2545\n",
      "     30        0.2671  0.2475\n",
      "==========Fold:32==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2528  0.4816\n",
      "      2        0.1660  0.4826\n",
      "      3        0.1553  0.4816\n",
      "      4        0.1453  0.4871\n",
      "      5        0.1380  0.4871\n",
      "      6        0.1302  0.4921\n",
      "      7        0.1261  0.5041\n",
      "      8        0.1162  0.4721\n",
      "      9        0.1117  0.4726\n",
      "     10        0.1058  0.4796\n",
      "     11        0.1034  0.4681\n",
      "     12        0.1101  0.4836\n",
      "     13        0.0982  0.4836\n",
      "     14        0.1007  0.4881\n",
      "     15        0.0911  0.4701\n",
      "     16        0.1006  0.5001\n",
      "     17        0.0909  0.4741\n",
      "     18        0.0824  0.5011\n",
      "     19        0.0861  0.4631\n",
      "     20        0.0845  0.4846\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0523  0.2445\n",
      "      2        1.7071  0.2605\n",
      "      3        1.3813  0.2595\n",
      "      4        1.1429  0.2595\n",
      "      5        0.9783  0.2470\n",
      "      6        0.8400  0.2500\n",
      "      7        0.8175  0.2430\n",
      "      8        0.7699  0.2625\n",
      "      9        0.6641  0.2540\n",
      "     10        0.5666  0.2585\n",
      "     11        0.5694  0.2366\n",
      "     12        0.5587  0.2485\n",
      "     13        0.5768  0.2660\n",
      "     14        0.4577  0.2455\n",
      "     15        0.4493  0.2560\n",
      "     16        0.4443  0.2520\n",
      "     17        0.3905  0.2565\n",
      "     18        0.4070  0.2495\n",
      "     19        0.4319  0.2555\n",
      "     20        0.4219  0.2555\n",
      "     21        0.3713  0.2540\n",
      "     22        0.3245  0.2490\n",
      "     23        0.3033  0.3034\n",
      "     24        0.3618  0.2830\n",
      "     25        0.3828  0.2730\n",
      "     26        0.3160  0.2500\n",
      "     27        0.2725  0.2396\n",
      "     28        0.2427  0.2505\n",
      "     29        0.3566  0.2495\n",
      "     30        0.3372  0.2510\n",
      "==========Fold:33==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2302  0.4996\n",
      "      2        0.1690  0.4976\n",
      "      3        0.1595  0.4926\n",
      "      4        0.1479  0.4711\n",
      "      5        0.1415  0.4976\n",
      "      6        0.1356  0.4901\n",
      "      7        0.1265  0.4766\n",
      "      8        0.1232  0.5160\n",
      "      9        0.1167  0.4846\n",
      "     10        0.1158  0.4921\n",
      "     11        0.1083  0.4796\n",
      "     12        0.1068  0.4861\n",
      "     13        0.1019  0.4881\n",
      "     14        0.1018  0.4866\n",
      "     15        0.0988  0.4876\n",
      "     16        0.0942  0.4766\n",
      "     17        0.0947  0.4971\n",
      "     18        0.0891  0.4906\n",
      "     19        0.0864  0.4801\n",
      "     20        0.0835  0.4731\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0679  0.2545\n",
      "      2        1.6154  0.2595\n",
      "      3        1.1997  0.2595\n",
      "      4        1.0005  0.2525\n",
      "      5        0.8523  0.2605\n",
      "      6        0.7353  0.2530\n",
      "      7        0.6314  0.2430\n",
      "      8        0.6618  0.2450\n",
      "      9        0.5631  0.2500\n",
      "     10        0.4706  0.2560\n",
      "     11        0.5080  0.2520\n",
      "     12        0.4539  0.2435\n",
      "     13        0.4647  0.2470\n",
      "     14        0.4164  0.2520\n",
      "     15        0.3420  0.2585\n",
      "     16        0.3596  0.2465\n",
      "     17        0.3757  0.2560\n",
      "     18        0.3722  0.2570\n",
      "     19        0.3860  0.2545\n",
      "     20        0.3059  0.2411\n",
      "     21        0.2510  0.2550\n",
      "     22        0.2425  0.2420\n",
      "     23        0.2684  0.2440\n",
      "     24        0.2421  0.2515\n",
      "     25        0.2437  0.2520\n",
      "     26        0.2727  0.2381\n",
      "     27        0.2669  0.2575\n",
      "     28        0.2273  0.2336\n",
      "     29        0.1928  0.2421\n",
      "     30        0.1636  0.2480\n",
      "==========Fold:34==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2216  0.4706\n",
      "      2        0.1670  0.4896\n",
      "      3        0.1534  0.4756\n",
      "      4        0.1436  0.4666\n",
      "      5        0.1344  0.4811\n",
      "      6        0.1301  0.4686\n",
      "      7        0.1191  0.4881\n",
      "      8        0.1164  0.4946\n",
      "      9        0.1128  0.4901\n",
      "     10        0.1088  0.4736\n",
      "     11        0.1037  0.4691\n",
      "     12        0.0991  0.5021\n",
      "     13        0.0957  0.5235\n",
      "     14        0.0896  0.5215\n",
      "     15        0.0854  0.4926\n",
      "     16        0.0930  0.5076\n",
      "     17        0.0895  0.5040\n",
      "     18        0.0814  0.4631\n",
      "     19        0.0808  0.4736\n",
      "     20        0.0821  0.4841\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0927  0.2515\n",
      "      2        1.7189  0.2565\n",
      "      3        1.3889  0.2520\n",
      "      4        1.1401  0.2520\n",
      "      5        0.9852  0.2570\n",
      "      6        0.8647  0.2535\n",
      "      7        0.7585  0.2555\n",
      "      8        0.6867  0.2450\n",
      "      9        0.6131  0.2490\n",
      "     10        0.5682  0.2500\n",
      "     11        0.5788  0.2520\n",
      "     12        0.5613  0.2460\n",
      "     13        0.4814  0.2565\n",
      "     14        0.4151  0.2421\n",
      "     15        0.4035  0.2600\n",
      "     16        0.3771  0.2790\n",
      "     17        0.4162  0.2720\n",
      "     18        0.3987  0.2615\n",
      "     19        0.3346  0.2505\n",
      "     20        0.3292  0.2515\n",
      "     21        0.3120  0.2525\n",
      "     22        0.4363  0.2560\n",
      "     23        0.4292  0.2630\n",
      "     24        0.2970  0.2525\n",
      "     25        0.2722  0.2710\n",
      "     26        0.3036  0.2670\n",
      "     27        0.3400  0.2470\n",
      "     28        0.3371  0.2530\n",
      "     29        0.2873  0.2505\n",
      "     30        0.2339  0.2495\n",
      "==========Fold:35==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2506  0.4911\n",
      "      2        0.1681  0.5016\n",
      "      3        0.1561  0.5016\n",
      "      4        0.1444  0.5115\n",
      "      5        0.1389  0.4796\n",
      "      6        0.1313  0.5036\n",
      "      7        0.1264  0.4916\n",
      "      8        0.1203  0.4741\n",
      "      9        0.1136  0.4726\n",
      "     10        0.1056  0.4906\n",
      "     11        0.1026  0.4861\n",
      "     12        0.1029  0.4921\n",
      "     13        0.0980  0.4926\n",
      "     14        0.0970  0.4761\n",
      "     15        0.0952  0.4981\n",
      "     16        0.0886  0.4811\n",
      "     17        0.0874  0.4741\n",
      "     18        0.0865  0.4986\n",
      "     19        0.0877  0.4916\n",
      "     20        0.0916  0.4966\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0830  0.2585\n",
      "      2        1.8074  0.2785\n",
      "      3        1.5685  0.2475\n",
      "      4        1.2961  0.2480\n",
      "      5        1.1043  0.2470\n",
      "      6        0.9330  0.2545\n",
      "      7        0.8252  0.2505\n",
      "      8        0.7879  0.2456\n",
      "      9        0.6641  0.2610\n",
      "     10        0.6535  0.2600\n",
      "     11        0.5486  0.2480\n",
      "     12        0.4950  0.2540\n",
      "     13        0.4846  0.2505\n",
      "     14        0.4792  0.2510\n",
      "     15        0.3955  0.2530\n",
      "     16        0.3796  0.2540\n",
      "     17        0.4018  0.2470\n",
      "     18        0.3566  0.2585\n",
      "     19        0.4087  0.2590\n",
      "     20        0.3241  0.2535\n",
      "     21        0.3176  0.2575\n",
      "     22        0.3028  0.2530\n",
      "     23        0.2965  0.2550\n",
      "     24        0.3007  0.2500\n",
      "     25        0.3235  0.2530\n",
      "     26        0.3361  0.2485\n",
      "     27        0.2746  0.2435\n",
      "     28        0.2587  0.2490\n",
      "     29        0.2136  0.2480\n",
      "     30        0.1892  0.2530\n",
      "==========Fold:36==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2346  0.4761\n",
      "      2        0.1652  0.4736\n",
      "      3        0.1520  0.4731\n",
      "      4        0.1431  0.4736\n",
      "      5        0.1330  0.5001\n",
      "      6        0.1304  0.4771\n",
      "      7        0.1224  0.4686\n",
      "      8        0.1198  0.4796\n",
      "      9        0.1065  0.4871\n",
      "     10        0.1067  0.4766\n",
      "     11        0.1054  0.5051\n",
      "     12        0.1008  0.4861\n",
      "     13        0.0993  0.4706\n",
      "     14        0.0963  0.4941\n",
      "     15        0.0931  0.4961\n",
      "     16        0.0874  0.4826\n",
      "     17        0.0881  0.4746\n",
      "     18        0.0868  0.4801\n",
      "     19        0.0890  0.4786\n",
      "     20        0.0851  0.4856\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0887  0.2480\n",
      "      2        1.7484  0.2560\n",
      "      3        1.4041  0.2421\n",
      "      4        1.1789  0.2485\n",
      "      5        0.9686  0.2450\n",
      "      6        0.8741  0.2286\n",
      "      7        0.7903  0.2386\n",
      "      8        0.7358  0.2485\n",
      "      9        0.6459  0.2695\n",
      "     10        0.6320  0.2495\n",
      "     11        0.6515  0.2545\n",
      "     12        0.5061  0.2455\n",
      "     13        0.4635  0.2341\n",
      "     14        0.4644  0.2700\n",
      "     15        0.4253  0.2430\n",
      "     16        0.3756  0.2515\n",
      "     17        0.3756  0.2755\n",
      "     18        0.3906  0.2670\n",
      "     19        0.3414  0.2530\n",
      "     20        0.3348  0.2420\n",
      "     21        0.3906  0.2435\n",
      "     22        0.2976  0.2560\n",
      "     23        0.2823  0.2450\n",
      "     24        0.2663  0.2550\n",
      "     25        0.2591  0.2660\n",
      "     26        0.3068  0.2520\n",
      "     27        0.2701  0.2336\n",
      "     28        0.2225  0.2455\n",
      "     29        0.2358  0.2605\n",
      "     30        0.2334  0.2480\n",
      "==========Fold:37==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.3213  0.4781\n",
      "      2        0.1695  0.4826\n",
      "      3        0.1574  0.4851\n",
      "      4        0.1476  0.4956\n",
      "      5        0.1445  0.5031\n",
      "      6        0.1367  0.4896\n",
      "      7        0.1314  0.5125\n",
      "      8        0.1276  0.4866\n",
      "      9        0.1158  0.4791\n",
      "     10        0.1168  0.4816\n",
      "     11        0.1099  0.4896\n",
      "     12        0.1063  0.4701\n",
      "     13        0.1081  0.4696\n",
      "     14        0.0998  0.4866\n",
      "     15        0.1020  0.4931\n",
      "     16        0.0914  0.4976\n",
      "     17        0.0909  0.4851\n",
      "     18        0.0921  0.4991\n",
      "     19        0.0929  0.4911\n",
      "     20        0.0798  0.4816\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0938  0.2495\n",
      "      2        1.6406  0.2480\n",
      "      3        1.2631  0.2550\n",
      "      4        1.0025  0.2480\n",
      "      5        0.8643  0.2450\n",
      "      6        0.7716  0.2530\n",
      "      7        0.6857  0.2515\n",
      "      8        0.6004  0.2805\n",
      "      9        0.5682  0.2605\n",
      "     10        0.4639  0.2520\n",
      "     11        0.5084  0.2560\n",
      "     12        0.4593  0.2540\n",
      "     13        0.4402  0.2440\n",
      "     14        0.3782  0.2356\n",
      "     15        0.3830  0.2515\n",
      "     16        0.3532  0.2450\n",
      "     17        0.3387  0.2530\n",
      "     18        0.2764  0.2460\n",
      "     19        0.2782  0.2425\n",
      "     20        0.2803  0.2391\n",
      "     21        0.3084  0.2580\n",
      "     22        0.4475  0.2430\n",
      "     23        0.3922  0.2430\n",
      "     24        0.2639  0.2520\n",
      "     25        0.2208  0.2451\n",
      "     26        0.2100  0.2585\n",
      "     27        0.2019  0.2440\n",
      "     28        0.1989  0.2500\n",
      "     29        0.1953  0.2590\n",
      "     30        0.2117  0.2535\n",
      "==========Fold:38==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2067  0.4776\n",
      "      2        0.1678  0.4926\n",
      "      3        0.1574  0.4891\n",
      "      4        0.1481  0.4896\n",
      "      5        0.1392  0.5086\n",
      "      6        0.1358  0.4941\n",
      "      7        0.1252  0.4886\n",
      "      8        0.1198  0.4946\n",
      "      9        0.1139  0.4861\n",
      "     10        0.1096  0.4736\n",
      "     11        0.1035  0.4761\n",
      "     12        0.0986  0.4851\n",
      "     13        0.0997  0.4921\n",
      "     14        0.0926  0.4686\n",
      "     15        0.0971  0.4786\n",
      "     16        0.0868  0.4801\n",
      "     17        0.0870  0.5006\n",
      "     18        0.0830  0.4906\n",
      "     19        0.0776  0.4731\n",
      "     20        0.0792  0.4816\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1466  0.2421\n",
      "      2        1.8679  0.2490\n",
      "      3        1.5071  0.2570\n",
      "      4        1.1968  0.2590\n",
      "      5        0.9797  0.2485\n",
      "      6        0.8508  0.2425\n",
      "      7        0.7878  0.2520\n",
      "      8        0.7046  0.2331\n",
      "      9        0.5881  0.2540\n",
      "     10        0.5556  0.2555\n",
      "     11        0.5743  0.2460\n",
      "     12        0.5388  0.2411\n",
      "     13        0.4325  0.2470\n",
      "     14        0.4581  0.2570\n",
      "     15        0.4448  0.2505\n",
      "     16        0.4055  0.2530\n",
      "     17        0.3485  0.2480\n",
      "     18        0.2990  0.2416\n",
      "     19        0.3033  0.2535\n",
      "     20        0.3115  0.2550\n",
      "     21        0.3201  0.2565\n",
      "     22        0.3554  0.2490\n",
      "     23        0.3264  0.2485\n",
      "     24        0.3100  0.2401\n",
      "     25        0.3275  0.2430\n",
      "     26        0.2802  0.2391\n",
      "     27        0.3252  0.2416\n",
      "     28        0.2151  0.2490\n",
      "     29        0.2381  0.2565\n",
      "     30        0.1932  0.2535\n",
      "==========Fold:39==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2137  0.4816\n",
      "      2        0.1618  0.4826\n",
      "      3        0.1498  0.4606\n",
      "      4        0.1380  0.4716\n",
      "      5        0.1340  0.4881\n",
      "      6        0.1234  0.4876\n",
      "      7        0.1171  0.4736\n",
      "      8        0.1104  0.4956\n",
      "      9        0.1048  0.4831\n",
      "     10        0.1020  0.4816\n",
      "     11        0.1034  0.5096\n",
      "     12        0.0962  0.4741\n",
      "     13        0.0916  0.4961\n",
      "     14        0.0941  0.4726\n",
      "     15        0.0859  0.4821\n",
      "     16        0.0812  0.4856\n",
      "     17        0.0825  0.4791\n",
      "     18        0.0783  0.4816\n",
      "     19        0.0764  0.4911\n",
      "     20        0.0769  0.4836\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0796  0.2445\n",
      "      2        1.7230  0.2525\n",
      "      3        1.3529  0.2366\n",
      "      4        1.0723  0.2401\n",
      "      5        0.9207  0.2386\n",
      "      6        0.8269  0.2545\n",
      "      7        0.7495  0.2460\n",
      "      8        0.6575  0.2535\n",
      "      9        0.5938  0.2615\n",
      "     10        0.5764  0.2550\n",
      "     11        0.5063  0.2500\n",
      "     12        0.4622  0.2595\n",
      "     13        0.5146  0.2485\n",
      "     14        0.5195  0.2530\n",
      "     15        0.4116  0.2475\n",
      "     16        0.3800  0.2455\n",
      "     17        0.3749  0.2455\n",
      "     18        0.3207  0.2535\n",
      "     19        0.3455  0.2560\n",
      "     20        0.4155  0.2590\n",
      "     21        0.4163  0.2480\n",
      "     22        0.3610  0.2430\n",
      "     23        0.3359  0.2470\n",
      "     24        0.3603  0.2520\n",
      "     25        0.4385  0.2560\n",
      "     26        0.3121  0.2485\n",
      "     27        0.2548  0.2520\n",
      "     28        0.2245  0.2615\n",
      "     29        0.2529  0.2550\n",
      "     30        0.2406  0.2326\n",
      "==========Fold:40==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2190  0.4631\n",
      "      2        0.1644  0.4986\n",
      "      3        0.1535  0.4901\n",
      "      4        0.1450  0.5120\n",
      "      5        0.1387  0.4681\n",
      "      6        0.1304  0.4931\n",
      "      7        0.1248  0.5091\n",
      "      8        0.1196  0.5001\n",
      "      9        0.1150  0.5006\n",
      "     10        0.1101  0.4811\n",
      "     11        0.1043  0.4876\n",
      "     12        0.1017  0.4766\n",
      "     13        0.1021  0.4626\n",
      "     14        0.0918  0.5046\n",
      "     15        0.0882  0.4851\n",
      "     16        0.0866  0.4886\n",
      "     17        0.0883  0.4981\n",
      "     18        0.0832  0.5046\n",
      "     19        0.0821  0.5091\n",
      "     20        0.0795  0.4821\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0509  0.2525\n",
      "      2        1.7019  0.2585\n",
      "      3        1.4094  0.2445\n",
      "      4        1.1613  0.2470\n",
      "      5        1.0136  0.2460\n",
      "      6        0.8313  0.2460\n",
      "      7        0.8245  0.2515\n",
      "      8        0.7632  0.2425\n",
      "      9        0.6216  0.2406\n",
      "     10        0.5252  0.2475\n",
      "     11        0.5242  0.2465\n",
      "     12        0.5355  0.2480\n",
      "     13        0.5043  0.2430\n",
      "     14        0.4824  0.2515\n",
      "     15        0.4713  0.2455\n",
      "     16        0.5245  0.2600\n",
      "     17        0.4073  0.2480\n",
      "     18        0.3292  0.2545\n",
      "     19        0.3037  0.2590\n",
      "     20        0.3322  0.2490\n",
      "     21        0.3345  0.2495\n",
      "     22        0.3495  0.2411\n",
      "     23        0.3632  0.2445\n",
      "     24        0.3298  0.2590\n",
      "     25        0.2936  0.2515\n",
      "     26        0.3981  0.2430\n",
      "     27        0.3110  0.2525\n",
      "     28        0.2852  0.2520\n",
      "     29        0.2780  0.2485\n",
      "     30        0.2348  0.2450\n",
      "==========Fold:41==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2289  0.5071\n",
      "      2        0.1666  0.4786\n",
      "      3        0.1523  0.4761\n",
      "      4        0.1453  0.4771\n",
      "      5        0.1383  0.4631\n",
      "      6        0.1285  0.4696\n",
      "      7        0.1238  0.4776\n",
      "      8        0.1202  0.4656\n",
      "      9        0.1099  0.4946\n",
      "     10        0.1055  0.4891\n",
      "     11        0.1073  0.4766\n",
      "     12        0.0996  0.4676\n",
      "     13        0.0966  0.4846\n",
      "     14        0.0936  0.5255\n",
      "     15        0.0969  0.4447\n",
      "     16        0.0861  0.4991\n",
      "     17        0.0848  0.4951\n",
      "     18        0.0876  0.4966\n",
      "     19        0.0840  0.4806\n",
      "     20        0.0772  0.4911\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0680  0.2825\n",
      "      2        1.7047  0.2700\n",
      "      3        1.3280  0.2600\n",
      "      4        1.1032  0.2515\n",
      "      5        0.9615  0.2500\n",
      "      6        0.8191  0.2535\n",
      "      7        0.7290  0.2705\n",
      "      8        0.6496  0.2620\n",
      "      9        0.5715  0.2535\n",
      "     10        0.5218  0.2590\n",
      "     11        0.5654  0.2565\n",
      "     12        0.5164  0.2485\n",
      "     13        0.4664  0.2580\n",
      "     14        0.3841  0.2545\n",
      "     15        0.3795  0.2500\n",
      "     16        0.4196  0.2525\n",
      "     17        0.4338  0.2495\n",
      "     18        0.3209  0.2625\n",
      "     19        0.3132  0.2605\n",
      "     20        0.3363  0.2450\n",
      "     21        0.3237  0.2435\n",
      "     22        0.2843  0.2515\n",
      "     23        0.3189  0.2595\n",
      "     24        0.2930  0.2406\n",
      "     25        0.2677  0.2500\n",
      "     26        0.2895  0.2540\n",
      "     27        0.3267  0.2470\n",
      "     28        0.2391  0.2580\n",
      "     29        0.2192  0.2430\n",
      "     30        0.2800  0.2555\n",
      "==========Fold:42==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2359  0.4806\n",
      "      2        0.1646  0.4876\n",
      "      3        0.1536  0.4931\n",
      "      4        0.1436  0.5061\n",
      "      5        0.1327  0.4801\n",
      "      6        0.1293  0.5056\n",
      "      7        0.1210  0.4771\n",
      "      8        0.1172  0.4826\n",
      "      9        0.1099  0.4846\n",
      "     10        0.1076  0.4651\n",
      "     11        0.0990  0.4886\n",
      "     12        0.0984  0.5051\n",
      "     13        0.0978  0.5230\n",
      "     14        0.0899  0.4946\n",
      "     15        0.0892  0.5011\n",
      "     16        0.0832  0.4881\n",
      "     17        0.0864  0.4931\n",
      "     18        0.0838  0.4826\n",
      "     19        0.0801  0.4901\n",
      "     20        0.0768  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0858  0.2495\n",
      "      2        1.6964  0.2540\n",
      "      3        1.3329  0.2435\n",
      "      4        1.1233  0.2460\n",
      "      5        0.9728  0.2555\n",
      "      6        0.8630  0.2580\n",
      "      7        0.7502  0.2540\n",
      "      8        0.6796  0.2490\n",
      "      9        0.6236  0.2525\n",
      "     10        0.6424  0.2510\n",
      "     11        0.5336  0.2490\n",
      "     12        0.5045  0.2535\n",
      "     13        0.5684  0.2585\n",
      "     14        0.5055  0.2411\n",
      "     15        0.4280  0.2406\n",
      "     16        0.4157  0.2490\n",
      "     17        0.4315  0.2540\n",
      "     18        0.3906  0.2416\n",
      "     19        0.3553  0.2600\n",
      "     20        0.3155  0.2545\n",
      "     21        0.3492  0.2520\n",
      "     22        0.3389  0.2490\n",
      "     23        0.4059  0.2475\n",
      "     24        0.3820  0.2515\n",
      "     25        0.3186  0.2540\n",
      "     26        0.2795  0.2535\n",
      "     27        0.2992  0.2575\n",
      "     28        0.2702  0.2435\n",
      "     29        0.2771  0.2381\n",
      "     30        0.3128  0.2505\n",
      "==========Fold:43==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2891  0.4601\n",
      "      2        0.1692  0.4711\n",
      "      3        0.1561  0.4921\n",
      "      4        0.1461  0.4806\n",
      "      5        0.1383  0.4801\n",
      "      6        0.1304  0.4901\n",
      "      7        0.1250  0.4821\n",
      "      8        0.1204  0.4916\n",
      "      9        0.1158  0.4601\n",
      "     10        0.1120  0.4836\n",
      "     11        0.1094  0.4776\n",
      "     12        0.1040  0.4791\n",
      "     13        0.1001  0.5001\n",
      "     14        0.0939  0.4846\n",
      "     15        0.0915  0.4781\n",
      "     16        0.0869  0.4621\n",
      "     17        0.1028  0.4821\n",
      "     18        0.0831  0.4781\n",
      "     19        0.0840  0.4961\n",
      "     20        0.0827  0.4846\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1088  0.2515\n",
      "      2        1.8554  0.2530\n",
      "      3        1.5502  0.2575\n",
      "      4        1.2390  0.2525\n",
      "      5        0.9881  0.2525\n",
      "      6        0.8500  0.2425\n",
      "      7        0.7041  0.2490\n",
      "      8        0.6429  0.2435\n",
      "      9        0.6565  0.2480\n",
      "     10        0.5579  0.2425\n",
      "     11        0.4956  0.2445\n",
      "     12        0.4315  0.2366\n",
      "     13        0.4587  0.2306\n",
      "     14        0.4027  0.2485\n",
      "     15        0.4802  0.2520\n",
      "     16        0.4775  0.2555\n",
      "     17        0.3314  0.2555\n",
      "     18        0.2792  0.2525\n",
      "     19        0.3024  0.2595\n",
      "     20        0.2987  0.2590\n",
      "     21        0.2827  0.2425\n",
      "     22        0.2535  0.2575\n",
      "     23        0.2685  0.2545\n",
      "     24        0.2727  0.2440\n",
      "     25        0.3106  0.2475\n",
      "     26        0.3451  0.2500\n",
      "     27        0.2358  0.2525\n",
      "     28        0.2236  0.2480\n",
      "     29        0.2122  0.2510\n",
      "     30        0.2329  0.2425\n",
      "==========Fold:44==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2233  0.4891\n",
      "      2        0.1647  0.4736\n",
      "      3        0.1526  0.4941\n",
      "      4        0.1477  0.4861\n",
      "      5        0.1367  0.4771\n",
      "      6        0.1309  0.4841\n",
      "      7        0.1250  0.4871\n",
      "      8        0.1217  0.4851\n",
      "      9        0.1108  0.4636\n",
      "     10        0.1059  0.4781\n",
      "     11        0.1067  0.4951\n",
      "     12        0.1028  0.4951\n",
      "     13        0.0961  0.4996\n",
      "     14        0.0917  0.4861\n",
      "     15        0.0904  0.4911\n",
      "     16        0.0850  0.4606\n",
      "     17        0.0901  0.4896\n",
      "     18        0.0855  0.4961\n",
      "     19        0.0793  0.5036\n",
      "     20        0.0774  0.5001\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0977  0.2525\n",
      "      2        1.7909  0.2515\n",
      "      3        1.4858  0.2515\n",
      "      4        1.2727  0.2500\n",
      "      5        1.0224  0.2455\n",
      "      6        0.9057  0.2346\n",
      "      7        0.8108  0.2470\n",
      "      8        0.7495  0.2560\n",
      "      9        0.6699  0.2401\n",
      "     10        0.6842  0.2560\n",
      "     11        0.6488  0.2500\n",
      "     12        0.6120  0.2465\n",
      "     13        0.5250  0.2411\n",
      "     14        0.4948  0.2450\n",
      "     15        0.4725  0.2585\n",
      "     16        0.4688  0.2595\n",
      "     17        0.6179  0.2530\n",
      "     18        0.4664  0.2595\n",
      "     19        0.3847  0.2495\n",
      "     20        0.3553  0.2580\n",
      "     21        0.3654  0.2495\n",
      "     22        0.3336  0.2406\n",
      "     23        0.3854  0.2540\n",
      "     24        0.3603  0.2610\n",
      "     25        0.3809  0.2690\n",
      "     26        0.3555  0.2690\n",
      "     27        0.3103  0.2575\n",
      "     28        0.3077  0.2520\n",
      "     29        0.3694  0.2905\n",
      "     30        0.2775  0.2805\n",
      "==========Fold:45==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2382  0.4721\n",
      "      2        0.1668  0.4806\n",
      "      3        0.1545  0.4891\n",
      "      4        0.1416  0.4756\n",
      "      5        0.1362  0.4601\n",
      "      6        0.1245  0.4761\n",
      "      7        0.1183  0.4866\n",
      "      8        0.1124  0.4881\n",
      "      9        0.1083  0.5081\n",
      "     10        0.1028  0.4776\n",
      "     11        0.1010  0.4856\n",
      "     12        0.0927  0.4716\n",
      "     13        0.0895  0.4761\n",
      "     14        0.0905  0.4771\n",
      "     15        0.0848  0.4756\n",
      "     16        0.0845  0.4651\n",
      "     17        0.0806  0.4552\n",
      "     18        0.0821  0.4477\n",
      "     19        0.0738  0.5260\n",
      "     20        0.0742  0.4791\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0659  0.2421\n",
      "      2        1.6450  0.2465\n",
      "      3        1.3023  0.2435\n",
      "      4        1.0764  0.2455\n",
      "      5        0.9873  0.2256\n",
      "      6        0.8637  0.2465\n",
      "      7        0.7786  0.2361\n",
      "      8        0.7239  0.2346\n",
      "      9        0.6414  0.2376\n",
      "     10        0.6132  0.2425\n",
      "     11        0.5581  0.2416\n",
      "     12        0.5499  0.2500\n",
      "     13        0.5112  0.2281\n",
      "     14        0.4789  0.2411\n",
      "     15        0.4299  0.2430\n",
      "     16        0.4081  0.2435\n",
      "     17        0.4266  0.2465\n",
      "     18        0.3798  0.2445\n",
      "     19        0.3618  0.2386\n",
      "     20        0.3616  0.2306\n",
      "     21        0.3585  0.2455\n",
      "     22        0.3872  0.2331\n",
      "     23        0.3315  0.2151\n",
      "     24        0.3345  0.2341\n",
      "     25        0.3488  0.2560\n",
      "     26        0.2971  0.2276\n",
      "     27        0.2696  0.2301\n",
      "     28        0.2505  0.2311\n",
      "     29        0.2382  0.2455\n",
      "     30        0.2537  0.2351\n",
      "==========Fold:46==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2395  0.4631\n",
      "      2        0.1673  0.4936\n",
      "      3        0.1544  0.5006\n",
      "      4        0.1448  0.4721\n",
      "      5        0.1362  0.4931\n",
      "      6        0.1305  0.4831\n",
      "      7        0.1218  0.4856\n",
      "      8        0.1201  0.4956\n",
      "      9        0.1247  0.5220\n",
      "     10        0.1148  0.5360\n",
      "     11        0.1064  0.5066\n",
      "     12        0.1029  0.4906\n",
      "     13        0.1023  0.4871\n",
      "     14        0.0956  0.4901\n",
      "     15        0.0890  0.4956\n",
      "     16        0.0961  0.5061\n",
      "     17        0.0893  0.4811\n",
      "     18        0.0839  0.4981\n",
      "     19        0.0852  0.4931\n",
      "     20        0.0832  0.4976\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0625  0.2485\n",
      "      2        1.6778  0.2510\n",
      "      3        1.3318  0.2600\n",
      "      4        1.0475  0.2510\n",
      "      5        0.9067  0.2470\n",
      "      6        0.8096  0.2560\n",
      "      7        0.7173  0.2610\n",
      "      8        0.6761  0.2555\n",
      "      9        0.5685  0.2550\n",
      "     10        0.5419  0.2565\n",
      "     11        0.5843  0.2585\n",
      "     12        0.4452  0.2425\n",
      "     13        0.4028  0.2515\n",
      "     14        0.4305  0.2525\n",
      "     15        0.3757  0.2570\n",
      "     16        0.4333  0.2440\n",
      "     17        0.4939  0.2450\n",
      "     18        0.3823  0.2715\n",
      "     19        0.3158  0.2710\n",
      "     20        0.2498  0.2580\n",
      "     21        0.2442  0.2460\n",
      "     22        0.2395  0.2560\n",
      "     23        0.2376  0.2585\n",
      "     24        0.2593  0.2560\n",
      "     25        0.3078  0.2440\n",
      "     26        0.3012  0.2510\n",
      "     27        0.2485  0.2570\n",
      "     28        0.2216  0.2595\n",
      "     29        0.1791  0.2425\n",
      "     30        0.1633  0.2416\n",
      "==========Fold:47==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2359  0.4886\n",
      "      2        0.1649  0.4631\n",
      "      3        0.1502  0.4816\n",
      "      4        0.1440  0.4921\n",
      "      5        0.1361  0.4756\n",
      "      6        0.1298  0.4841\n",
      "      7        0.1228  0.4796\n",
      "      8        0.1195  0.4841\n",
      "      9        0.1140  0.4971\n",
      "     10        0.1057  0.5011\n",
      "     11        0.1022  0.4841\n",
      "     12        0.1057  0.4821\n",
      "     13        0.0945  0.4866\n",
      "     14        0.0958  0.4806\n",
      "     15        0.0916  0.4836\n",
      "     16        0.0892  0.4826\n",
      "     17        0.0884  0.4846\n",
      "     18        0.0866  0.4871\n",
      "     19        0.0864  0.4866\n",
      "     20        0.0823  0.4851\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0645  0.2401\n",
      "      2        1.5569  0.2550\n",
      "      3        1.2586  0.2505\n",
      "      4        1.0586  0.2555\n",
      "      5        0.9335  0.2595\n",
      "      6        0.8421  0.2560\n",
      "      7        0.7814  0.2500\n",
      "      8        0.7060  0.2510\n",
      "      9        0.6173  0.2525\n",
      "     10        0.5630  0.2381\n",
      "     11        0.5105  0.2675\n",
      "     12        0.5060  0.2580\n",
      "     13        0.4937  0.2440\n",
      "     14        0.4210  0.2520\n",
      "     15        0.4420  0.2535\n",
      "     16        0.4097  0.2505\n",
      "     17        0.4132  0.2371\n",
      "     18        0.4408  0.2595\n",
      "     19        0.3786  0.2555\n",
      "     20        0.3147  0.2475\n",
      "     21        0.3255  0.2560\n",
      "     22        0.3247  0.2465\n",
      "     23        0.3118  0.2565\n",
      "     24        0.2907  0.2475\n",
      "     25        0.3307  0.2455\n",
      "     26        0.4535  0.2510\n",
      "     27        0.3327  0.2440\n",
      "     28        0.3036  0.2770\n",
      "     29        0.2482  0.2765\n",
      "     30        0.2015  0.2630\n",
      "==========Fold:48==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2350  0.5041\n",
      "      2        0.1676  0.4976\n",
      "      3        0.1529  0.4911\n",
      "      4        0.1424  0.4786\n",
      "      5        0.1370  0.4876\n",
      "      6        0.1295  0.4746\n",
      "      7        0.1245  0.4836\n",
      "      8        0.1190  0.4846\n",
      "      9        0.1156  0.4806\n",
      "     10        0.1131  0.4716\n",
      "     11        0.1063  0.4801\n",
      "     12        0.1061  0.4701\n",
      "     13        0.0973  0.5295\n",
      "     14        0.0985  0.5026\n",
      "     15        0.0984  0.4966\n",
      "     16        0.0970  0.4936\n",
      "     17        0.0941  0.4811\n",
      "     18        0.0879  0.5031\n",
      "     19        0.0889  0.4936\n",
      "     20        0.0883  0.4806\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1167  0.2480\n",
      "      2        1.7343  0.2475\n",
      "      3        1.3601  0.2401\n",
      "      4        1.1094  0.2550\n",
      "      5        0.9966  0.2510\n",
      "      6        0.8190  0.2610\n",
      "      7        0.7693  0.2565\n",
      "      8        0.6523  0.2440\n",
      "      9        0.5940  0.2445\n",
      "     10        0.5651  0.2480\n",
      "     11        0.5598  0.2595\n",
      "     12        0.5033  0.2555\n",
      "     13        0.4683  0.2555\n",
      "     14        0.4029  0.2351\n",
      "     15        0.4291  0.2535\n",
      "     16        0.3884  0.2435\n",
      "     17        0.4251  0.2525\n",
      "     18        0.3292  0.2555\n",
      "     19        0.3155  0.2445\n",
      "     20        0.2960  0.2585\n",
      "     21        0.3275  0.2540\n",
      "     22        0.2903  0.2580\n",
      "     23        0.2676  0.2460\n",
      "     24        0.3029  0.2530\n",
      "     25        0.3671  0.2540\n",
      "     26        0.3568  0.2530\n",
      "     27        0.2707  0.2545\n",
      "     28        0.2123  0.2550\n",
      "     29        0.1991  0.2401\n",
      "     30        0.1981  0.2545\n",
      "==========Fold:49==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2540  0.4906\n",
      "      2        0.1691  0.4846\n",
      "      3        0.1559  0.4896\n",
      "      4        0.1461  0.5180\n",
      "      5        0.1391  0.5195\n",
      "      6        0.1318  0.4781\n",
      "      7        0.1270  0.4841\n",
      "      8        0.1177  0.4856\n",
      "      9        0.1178  0.4841\n",
      "     10        0.1117  0.4706\n",
      "     11        0.1069  0.4741\n",
      "     12        0.1064  0.4701\n",
      "     13        0.1035  0.4876\n",
      "     14        0.0962  0.4816\n",
      "     15        0.0946  0.4876\n",
      "     16        0.0917  0.4636\n",
      "     17        0.0856  0.4746\n",
      "     18        0.0840  0.4751\n",
      "     19        0.0917  0.4956\n",
      "     20        0.0821  0.4941\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0700  0.2760\n",
      "      2        1.6934  0.2535\n",
      "      3        1.3444  0.2500\n",
      "      4        1.1424  0.2565\n",
      "      5        0.9064  0.2435\n",
      "      6        0.8293  0.2495\n",
      "      7        0.7189  0.2455\n",
      "      8        0.6948  0.2560\n",
      "      9        0.6356  0.2341\n",
      "     10        0.5819  0.2510\n",
      "     11        0.5416  0.2445\n",
      "     12        0.4865  0.2401\n",
      "     13        0.4506  0.2470\n",
      "     14        0.4226  0.2555\n",
      "     15        0.4293  0.2455\n",
      "     16        0.3969  0.2515\n",
      "     17        0.3710  0.2465\n",
      "     18        0.3115  0.2560\n",
      "     19        0.3608  0.2490\n",
      "     20        0.3291  0.2585\n",
      "     21        0.3549  0.2765\n",
      "     22        0.2838  0.2560\n",
      "     23        0.2413  0.2470\n",
      "     24        0.2495  0.2475\n",
      "     25        0.3111  0.2450\n",
      "     26        0.2742  0.2520\n",
      "     27        0.3070  0.2450\n",
      "     28        0.3233  0.2595\n",
      "     29        0.2639  0.2525\n",
      "     30        0.1989  0.2535\n",
      "==========Fold:50==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2226  0.4846\n",
      "      2        0.1680  0.4811\n",
      "      3        0.1577  0.4701\n",
      "      4        0.1480  0.4771\n",
      "      5        0.1385  0.4935\n",
      "      6        0.1310  0.4916\n",
      "      7        0.1235  0.4776\n",
      "      8        0.1171  0.4796\n",
      "      9        0.1143  0.4841\n",
      "     10        0.1096  0.4696\n",
      "     11        0.0991  0.4991\n",
      "     12        0.0979  0.4706\n",
      "     13        0.0947  0.4986\n",
      "     14        0.0919  0.4886\n",
      "     15        0.0924  0.4876\n",
      "     16        0.0852  0.4851\n",
      "     17        0.0805  0.4891\n",
      "     18        0.0804  0.4786\n",
      "     19        0.0810  0.4971\n",
      "     20        0.0774  0.4846\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1004  0.2371\n",
      "      2        1.6913  0.2470\n",
      "      3        1.3740  0.2435\n",
      "      4        1.1550  0.2450\n",
      "      5        0.9900  0.2465\n",
      "      6        0.8502  0.2460\n",
      "      7        0.7611  0.2371\n",
      "      8        0.7108  0.2495\n",
      "      9        0.6953  0.2475\n",
      "     10        0.6079  0.2575\n",
      "     11        0.5916  0.2545\n",
      "     12        0.5366  0.2565\n",
      "     13        0.5060  0.2720\n",
      "     14        0.5081  0.2650\n",
      "     15        0.4622  0.2600\n",
      "     16        0.4106  0.2535\n",
      "     17        0.3835  0.2490\n",
      "     18        0.3689  0.2570\n",
      "     19        0.3878  0.2495\n",
      "     20        0.3976  0.2580\n",
      "     21        0.3886  0.2550\n",
      "     22        0.4614  0.2565\n",
      "     23        0.3836  0.2485\n",
      "     24        0.2950  0.2440\n",
      "     25        0.2459  0.2555\n",
      "     26        0.2528  0.2460\n",
      "     27        0.3654  0.2565\n",
      "     28        0.2884  0.2371\n",
      "     29        0.2546  0.2585\n",
      "     30        0.2140  0.2540\n",
      "==========Fold:51==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2649  0.5001\n",
      "      2        0.1667  0.4876\n",
      "      3        0.1548  0.4841\n",
      "      4        0.1444  0.4821\n",
      "      5        0.1368  0.4666\n",
      "      6        0.1313  0.4956\n",
      "      7        0.1243  0.4961\n",
      "      8        0.1172  0.4881\n",
      "      9        0.1110  0.4751\n",
      "     10        0.1049  0.4756\n",
      "     11        0.1013  0.4646\n",
      "     12        0.0956  0.4946\n",
      "     13        0.0951  0.4856\n",
      "     14        0.0903  0.4641\n",
      "     15        0.0883  0.4906\n",
      "     16        0.0866  0.4766\n",
      "     17        0.0800  0.4716\n",
      "     18        0.0798  0.4741\n",
      "     19        0.0797  0.4671\n",
      "     20        0.0746  0.4731\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1259  0.2490\n",
      "      2        1.7700  0.2620\n",
      "      3        1.3853  0.2755\n",
      "      4        1.1092  0.2625\n",
      "      5        0.9678  0.2475\n",
      "      6        0.7955  0.2555\n",
      "      7        0.7348  0.2605\n",
      "      8        0.6720  0.2505\n",
      "      9        0.6708  0.2475\n",
      "     10        0.5579  0.2545\n",
      "     11        0.4740  0.2575\n",
      "     12        0.4688  0.2495\n",
      "     13        0.4568  0.2545\n",
      "     14        0.4608  0.2575\n",
      "     15        0.4169  0.2575\n",
      "     16        0.3818  0.2540\n",
      "     17        0.3560  0.2475\n",
      "     18        0.4996  0.2505\n",
      "     19        0.4932  0.2535\n",
      "     20        0.3540  0.2580\n",
      "     21        0.3023  0.2440\n",
      "     22        0.2999  0.2465\n",
      "     23        0.3042  0.2435\n",
      "     24        0.3101  0.2376\n",
      "     25        0.3031  0.2406\n",
      "     26        0.2796  0.2470\n",
      "     27        0.2860  0.2470\n",
      "     28        0.2698  0.2540\n",
      "     29        0.3335  0.2500\n",
      "     30        0.2725  0.2570\n",
      "==========Fold:52==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2724  0.4911\n",
      "      2        0.1718  0.4731\n",
      "      3        0.1569  0.4821\n",
      "      4        0.1465  0.4956\n",
      "      5        0.1416  0.4896\n",
      "      6        0.1308  0.4801\n",
      "      7        0.1262  0.4731\n",
      "      8        0.1242  0.4901\n",
      "      9        0.1207  0.4746\n",
      "     10        0.1152  0.4816\n",
      "     11        0.1143  0.4896\n",
      "     12        0.1051  0.4896\n",
      "     13        0.1016  0.4826\n",
      "     14        0.1075  0.4886\n",
      "     15        0.0976  0.4691\n",
      "     16        0.0940  0.4891\n",
      "     17        0.0946  0.4931\n",
      "     18        0.0995  0.4841\n",
      "     19        0.0926  0.4751\n",
      "     20        0.0861  0.4926\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0898  0.2440\n",
      "      2        1.7486  0.2500\n",
      "      3        1.4741  0.2510\n",
      "      4        1.1561  0.2416\n",
      "      5        1.0044  0.2525\n",
      "      6        0.8470  0.2540\n",
      "      7        0.7543  0.2505\n",
      "      8        0.6520  0.2470\n",
      "      9        0.6007  0.2560\n",
      "     10        0.5757  0.2530\n",
      "     11        0.5051  0.2535\n",
      "     12        0.4565  0.2525\n",
      "     13        0.4797  0.2460\n",
      "     14        0.4299  0.2455\n",
      "     15        0.4131  0.2610\n",
      "     16        0.4919  0.2525\n",
      "     17        0.3703  0.2575\n",
      "     18        0.3049  0.2530\n",
      "     19        0.2839  0.2525\n",
      "     20        0.2806  0.2585\n",
      "     21        0.3007  0.2485\n",
      "     22        0.2929  0.2401\n",
      "     23        0.3676  0.2470\n",
      "     24        0.3828  0.2465\n",
      "     25        0.3228  0.2445\n",
      "     26        0.2528  0.2535\n",
      "     27        0.2178  0.2445\n",
      "     28        0.1976  0.2490\n",
      "     29        0.2264  0.2530\n",
      "     30        0.1857  0.2490\n",
      "==========Fold:53==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2491  0.4986\n",
      "      2        0.1667  0.5061\n",
      "      3        0.1541  0.4796\n",
      "      4        0.1437  0.4841\n",
      "      5        0.1363  0.5125\n",
      "      6        0.1276  0.4831\n",
      "      7        0.1197  0.4881\n",
      "      8        0.1186  0.4771\n",
      "      9        0.1086  0.4796\n",
      "     10        0.1093  0.5006\n",
      "     11        0.1051  0.4686\n",
      "     12        0.0983  0.5011\n",
      "     13        0.0974  0.4881\n",
      "     14        0.0913  0.4906\n",
      "     15        0.0895  0.4831\n",
      "     16        0.0850  0.4701\n",
      "     17        0.0814  0.4816\n",
      "     18        0.0832  0.4986\n",
      "     19        0.0776  0.4731\n",
      "     20        0.0851  0.4781\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1157  0.2580\n",
      "      2        1.7712  0.2445\n",
      "      3        1.4131  0.2475\n",
      "      4        1.1956  0.2605\n",
      "      5        0.9874  0.2560\n",
      "      6        0.8162  0.2430\n",
      "      7        0.7544  0.2560\n",
      "      8        0.7433  0.2560\n",
      "      9        0.6159  0.2386\n",
      "     10        0.5494  0.2600\n",
      "     11        0.5137  0.2490\n",
      "     12        0.4577  0.2550\n",
      "     13        0.4414  0.2585\n",
      "     14        0.4471  0.2495\n",
      "     15        0.4659  0.2585\n",
      "     16        0.3946  0.2500\n",
      "     17        0.3720  0.2625\n",
      "     18        0.3282  0.2455\n",
      "     19        0.3799  0.2366\n",
      "     20        0.3602  0.2535\n",
      "     21        0.3465  0.2490\n",
      "     22        0.2881  0.2520\n",
      "     23        0.2567  0.2585\n",
      "     24        0.2486  0.2720\n",
      "     25        0.2720  0.2450\n",
      "     26        0.3418  0.2600\n",
      "     27        0.3694  0.2545\n",
      "     28        0.2943  0.2485\n",
      "     29        0.2089  0.2515\n",
      "     30        0.1919  0.2525\n",
      "==========Fold:54==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2045  0.4716\n",
      "      2        0.1705  0.4866\n",
      "      3        0.1585  0.4856\n",
      "      4        0.1464  0.4841\n",
      "      5        0.1392  0.4956\n",
      "      6        0.1322  0.5001\n",
      "      7        0.1259  0.4726\n",
      "      8        0.1196  0.4771\n",
      "      9        0.1196  0.4831\n",
      "     10        0.1096  0.4821\n",
      "     11        0.1065  0.4941\n",
      "     12        0.1053  0.4896\n",
      "     13        0.0988  0.4881\n",
      "     14        0.0984  0.4986\n",
      "     15        0.0946  0.4851\n",
      "     16        0.0944  0.4811\n",
      "     17        0.0881  0.4621\n",
      "     18        0.0876  0.4906\n",
      "     19        0.0822  0.4901\n",
      "     20        0.0844  0.4806\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0211  0.2520\n",
      "      2        1.5998  0.2500\n",
      "      3        1.2889  0.2421\n",
      "      4        1.1240  0.2560\n",
      "      5        0.9670  0.2600\n",
      "      6        0.8212  0.2460\n",
      "      7        0.7124  0.2416\n",
      "      8        0.6988  0.2530\n",
      "      9        0.6798  0.2505\n",
      "     10        0.5892  0.2490\n",
      "     11        0.5501  0.2760\n",
      "     12        0.5945  0.2420\n",
      "     13        0.4830  0.2480\n",
      "     14        0.4272  0.2485\n",
      "     15        0.4060  0.2510\n",
      "     16        0.4302  0.2605\n",
      "     17        0.5209  0.2495\n",
      "     18        0.4139  0.2515\n",
      "     19        0.3527  0.2565\n",
      "     20        0.2947  0.2600\n",
      "     21        0.3123  0.2420\n",
      "     22        0.3631  0.2580\n",
      "     23        0.3867  0.2565\n",
      "     24        0.3404  0.2445\n",
      "     25        0.2704  0.2540\n",
      "     26        0.2710  0.2545\n",
      "     27        0.2943  0.2575\n",
      "     28        0.2757  0.2600\n",
      "     29        0.3201  0.2470\n",
      "     30        0.2814  0.2535\n",
      "==========Fold:55==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2326  0.4911\n",
      "      2        0.1645  0.4931\n",
      "      3        0.1506  0.4756\n",
      "      4        0.1422  0.4716\n",
      "      5        0.1326  0.5430\n",
      "      6        0.1243  0.4996\n",
      "      7        0.1203  0.4936\n",
      "      8        0.1168  0.5111\n",
      "      9        0.1112  0.4946\n",
      "     10        0.1062  0.4801\n",
      "     11        0.1033  0.4861\n",
      "     12        0.0963  0.4776\n",
      "     13        0.0953  0.4776\n",
      "     14        0.0914  0.4841\n",
      "     15        0.0890  0.4806\n",
      "     16        0.0839  0.4741\n",
      "     17        0.0825  0.4651\n",
      "     18        0.0803  0.4776\n",
      "     19        0.0835  0.4856\n",
      "     20        0.0776  0.4731\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0644  0.2730\n",
      "      2        1.6567  0.2615\n",
      "      3        1.3400  0.2520\n",
      "      4        1.1160  0.2590\n",
      "      5        0.8991  0.2455\n",
      "      6        0.8582  0.2615\n",
      "      7        0.7916  0.2520\n",
      "      8        0.6424  0.2585\n",
      "      9        0.6270  0.2520\n",
      "     10        0.6027  0.2530\n",
      "     11        0.5014  0.2530\n",
      "     12        0.4772  0.2535\n",
      "     13        0.4910  0.2645\n",
      "     14        0.4309  0.2570\n",
      "     15        0.4531  0.2440\n",
      "     16        0.4515  0.2495\n",
      "     17        0.3763  0.2500\n",
      "     18        0.3905  0.2376\n",
      "     19        0.3195  0.2445\n",
      "     20        0.2799  0.2560\n",
      "     21        0.3253  0.2525\n",
      "     22        0.5028  0.2475\n",
      "     23        0.3569  0.2460\n",
      "     24        0.3013  0.2490\n",
      "     25        0.3387  0.2450\n",
      "     26        0.2539  0.2520\n",
      "     27        0.2269  0.2430\n",
      "     28        0.2150  0.2520\n",
      "     29        0.2442  0.2520\n",
      "     30        0.2279  0.2510\n",
      "==========Fold:56==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2381  0.4966\n",
      "      2        0.1633  0.4671\n",
      "      3        0.1502  0.4906\n",
      "      4        0.1417  0.4896\n",
      "      5        0.1333  0.4861\n",
      "      6        0.1258  0.4796\n",
      "      7        0.1171  0.5026\n",
      "      8        0.1183  0.4861\n",
      "      9        0.1144  0.4886\n",
      "     10        0.1074  0.4971\n",
      "     11        0.0992  0.5130\n",
      "     12        0.0972  0.4861\n",
      "     13        0.0944  0.4711\n",
      "     14        0.0947  0.4836\n",
      "     15        0.0901  0.4716\n",
      "     16        0.0919  0.4706\n",
      "     17        0.0861  0.4676\n",
      "     18        0.0810  0.4821\n",
      "     19        0.0822  0.4926\n",
      "     20        0.0764  0.4801\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0881  0.2425\n",
      "      2        1.7892  0.2570\n",
      "      3        1.4544  0.2555\n",
      "      4        1.2081  0.2480\n",
      "      5        1.0266  0.2435\n",
      "      6        0.8636  0.2515\n",
      "      7        0.7835  0.2551\n",
      "      8        0.7275  0.2520\n",
      "      9        0.6633  0.2386\n",
      "     10        0.7296  0.2515\n",
      "     11        0.5933  0.2430\n",
      "     12        0.5343  0.2440\n",
      "     13        0.4682  0.2445\n",
      "     14        0.4424  0.2550\n",
      "     15        0.4047  0.2505\n",
      "     16        0.3904  0.2520\n",
      "     17        0.4322  0.2470\n",
      "     18        0.4260  0.2530\n",
      "     19        0.3842  0.2495\n",
      "     20        0.3010  0.2550\n",
      "     21        0.3367  0.2550\n",
      "     22        0.2829  0.2416\n",
      "     23        0.2670  0.2550\n",
      "     24        0.2573  0.2470\n",
      "     25        0.2740  0.2490\n",
      "     26        0.3344  0.2575\n",
      "     27        0.2582  0.2460\n",
      "     28        0.2559  0.2416\n",
      "     29        0.2441  0.2366\n",
      "     30        0.3791  0.2710\n",
      "==========Fold:57==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2645  0.4811\n",
      "      2        0.1728  0.4726\n",
      "      3        0.1562  0.4876\n",
      "      4        0.1467  0.4841\n",
      "      5        0.1376  0.4791\n",
      "      6        0.1305  0.4871\n",
      "      7        0.1212  0.4936\n",
      "      8        0.1148  0.4841\n",
      "      9        0.1194  0.4871\n",
      "     10        0.1091  0.4691\n",
      "     11        0.1084  0.5006\n",
      "     12        0.1050  0.4881\n",
      "     13        0.0972  0.4686\n",
      "     14        0.0956  0.4641\n",
      "     15        0.0951  0.4896\n",
      "     16        0.0968  0.4916\n",
      "     17        0.0900  0.4586\n",
      "     18        0.0845  0.4906\n",
      "     19        0.0874  0.4891\n",
      "     20        0.0887  0.4806\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0274  0.2500\n",
      "      2        1.5983  0.2525\n",
      "      3        1.3165  0.2575\n",
      "      4        1.0346  0.2450\n",
      "      5        0.8996  0.2550\n",
      "      6        0.8409  0.2520\n",
      "      7        0.7651  0.2525\n",
      "      8        0.6709  0.2540\n",
      "      9        0.6502  0.2485\n",
      "     10        0.5492  0.2535\n",
      "     11        0.5443  0.2515\n",
      "     12        0.4817  0.2515\n",
      "     13        0.5311  0.2445\n",
      "     14        0.4594  0.2600\n",
      "     15        0.4485  0.2545\n",
      "     16        0.4095  0.2725\n",
      "     17        0.3634  0.2595\n",
      "     18        0.3335  0.2475\n",
      "     19        0.3331  0.2550\n",
      "     20        0.3345  0.2580\n",
      "     21        0.3033  0.2515\n",
      "     22        0.3097  0.2520\n",
      "     23        0.4653  0.2411\n",
      "     24        0.3630  0.2535\n",
      "     25        0.2576  0.2530\n",
      "     26        0.2728  0.2550\n",
      "     27        0.2581  0.2495\n",
      "     28        0.3458  0.2495\n",
      "     29        0.3176  0.2580\n",
      "     30        0.2144  0.2515\n",
      "==========Fold:58==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2385  0.4866\n",
      "      2        0.1655  0.4776\n",
      "      3        0.1528  0.4751\n",
      "      4        0.1413  0.4936\n",
      "      5        0.1319  0.4821\n",
      "      6        0.1351  0.4786\n",
      "      7        0.1241  0.5051\n",
      "      8        0.1160  0.4826\n",
      "      9        0.1119  0.4846\n",
      "     10        0.1074  0.4771\n",
      "     11        0.1061  0.4776\n",
      "     12        0.1038  0.4956\n",
      "     13        0.1016  0.4956\n",
      "     14        0.0992  0.4811\n",
      "     15        0.0896  0.4936\n",
      "     16        0.0912  0.4846\n",
      "     17        0.0911  0.4891\n",
      "     18        0.0868  0.4951\n",
      "     19        0.0866  0.4911\n",
      "     20        0.0862  0.4866\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0808  0.2535\n",
      "      2        1.7117  0.2555\n",
      "      3        1.3508  0.2505\n",
      "      4        1.1284  0.2530\n",
      "      5        1.0460  0.2760\n",
      "      6        0.8711  0.2535\n",
      "      7        0.8060  0.2455\n",
      "      8        0.7970  0.2525\n",
      "      9        0.7448  0.2585\n",
      "     10        0.6745  0.2490\n",
      "     11        0.5655  0.2535\n",
      "     12        0.5576  0.2515\n",
      "     13        0.4964  0.2435\n",
      "     14        0.4818  0.2530\n",
      "     15        0.4455  0.2460\n",
      "     16        0.4131  0.2580\n",
      "     17        0.3890  0.2530\n",
      "     18        0.4144  0.2550\n",
      "     19        0.4536  0.2495\n",
      "     20        0.3811  0.2485\n",
      "     21        0.3063  0.2585\n",
      "     22        0.2904  0.2575\n",
      "     23        0.2556  0.2525\n",
      "     24        0.2521  0.2510\n",
      "     25        0.2761  0.2490\n",
      "     26        0.3018  0.2411\n",
      "     27        0.2830  0.2505\n",
      "     28        0.2581  0.2445\n",
      "     29        0.2603  0.2575\n",
      "     30        0.2238  0.2425\n",
      "==========Fold:59==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2250  0.5046\n",
      "      2        0.1641  0.4991\n",
      "      3        0.1497  0.4881\n",
      "      4        0.1422  0.5335\n",
      "      5        0.1346  0.4673\n",
      "      6        0.1247  0.4871\n",
      "      7        0.1196  0.4816\n",
      "      8        0.1136  0.4966\n",
      "      9        0.1105  0.4811\n",
      "     10        0.1037  0.4926\n",
      "     11        0.0971  0.4696\n",
      "     12        0.0982  0.5066\n",
      "     13        0.0945  0.4986\n",
      "     14        0.0861  0.5260\n",
      "     15        0.0899  0.4986\n",
      "     16        0.0860  0.4931\n",
      "     17        0.0859  0.4766\n",
      "     18        0.0796  0.4961\n",
      "     19        0.0791  0.4821\n",
      "     20        0.0744  0.4801\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0710  0.2525\n",
      "      2        1.6530  0.2485\n",
      "      3        1.2908  0.2545\n",
      "      4        1.0787  0.2571\n",
      "      5        0.9153  0.2570\n",
      "      6        0.8324  0.2475\n",
      "      7        0.7703  0.2425\n",
      "      8        0.7125  0.2610\n",
      "      9        0.6158  0.2585\n",
      "     10        0.5825  0.2535\n",
      "     11        0.5230  0.2515\n",
      "     12        0.5184  0.2475\n",
      "     13        0.5120  0.2530\n",
      "     14        0.5020  0.2555\n",
      "     15        0.3993  0.2425\n",
      "     16        0.3487  0.2495\n",
      "     17        0.3624  0.2565\n",
      "     18        0.3924  0.2445\n",
      "     19        0.3818  0.2560\n",
      "     20        0.3324  0.2575\n",
      "     21        0.3027  0.2530\n",
      "     22        0.3081  0.2475\n",
      "     23        0.2940  0.2550\n",
      "     24        0.2462  0.2475\n",
      "     25        0.2160  0.2535\n",
      "     26        0.2407  0.2510\n",
      "     27        0.2239  0.2595\n",
      "     28        0.2401  0.2580\n",
      "     29        0.3350  0.2470\n",
      "     30        0.2881  0.2590\n",
      "==========Fold:60==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2166  0.5026\n",
      "      2        0.1677  0.5111\n",
      "      3        0.1561  0.5001\n",
      "      4        0.1460  0.4716\n",
      "      5        0.1384  0.5096\n",
      "      6        0.1323  0.4786\n",
      "      7        0.1234  0.4846\n",
      "      8        0.1172  0.4686\n",
      "      9        0.1130  0.4996\n",
      "     10        0.1087  0.5026\n",
      "     11        0.1034  0.4821\n",
      "     12        0.1004  0.4761\n",
      "     13        0.0941  0.4821\n",
      "     14        0.0927  0.4731\n",
      "     15        0.0904  0.4811\n",
      "     16        0.0883  0.4791\n",
      "     17        0.0809  0.4976\n",
      "     18        0.0826  0.5170\n",
      "     19        0.0788  0.4726\n",
      "     20        0.0820  0.5165\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0588  0.2421\n",
      "      2        1.7392  0.2640\n",
      "      3        1.4252  0.2550\n",
      "      4        1.1859  0.2585\n",
      "      5        1.0257  0.2495\n",
      "      6        0.8796  0.2610\n",
      "      7        0.8380  0.2555\n",
      "      8        0.7181  0.2376\n",
      "      9        0.6729  0.2530\n",
      "     10        0.6374  0.2491\n",
      "     11        0.6197  0.2770\n",
      "     12        0.5416  0.2505\n",
      "     13        0.5054  0.2585\n",
      "     14        0.5158  0.2565\n",
      "     15        0.5133  0.2595\n",
      "     16        0.5044  0.2510\n",
      "     17        0.4658  0.2520\n",
      "     18        0.4032  0.2505\n",
      "     19        0.3646  0.2575\n",
      "     20        0.3529  0.2610\n",
      "     21        0.3135  0.2530\n",
      "     22        0.3179  0.2425\n",
      "     23        0.3217  0.2520\n",
      "     24        0.3356  0.2590\n",
      "     25        0.3257  0.2515\n",
      "     26        0.3419  0.2525\n",
      "     27        0.3377  0.2465\n",
      "     28        0.3280  0.2540\n",
      "     29        0.2569  0.2510\n",
      "     30        0.2208  0.2480\n",
      "==========Fold:61==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2436  0.4921\n",
      "      2        0.1682  0.5056\n",
      "      3        0.1551  0.5066\n",
      "      4        0.1440  0.4856\n",
      "      5        0.1331  0.4811\n",
      "      6        0.1300  0.4776\n",
      "      7        0.1226  0.4956\n",
      "      8        0.1178  0.5046\n",
      "      9        0.1103  0.4771\n",
      "     10        0.1107  0.4806\n",
      "     11        0.1045  0.4701\n",
      "     12        0.0987  0.4826\n",
      "     13        0.0961  0.4971\n",
      "     14        0.0912  0.4916\n",
      "     15        0.0886  0.4791\n",
      "     16        0.0852  0.4826\n",
      "     17        0.0838  0.5011\n",
      "     18        0.0857  0.4861\n",
      "     19        0.0836  0.4836\n",
      "     20        0.0782  0.4736\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0883  0.2595\n",
      "      2        1.7277  0.2605\n",
      "      3        1.3689  0.2535\n",
      "      4        1.1177  0.2535\n",
      "      5        1.0184  0.2555\n",
      "      6        0.8878  0.2550\n",
      "      7        0.7985  0.2545\n",
      "      8        0.6952  0.2550\n",
      "      9        0.6139  0.2770\n",
      "     10        0.5947  0.2645\n",
      "     11        0.5310  0.2570\n",
      "     12        0.5131  0.2485\n",
      "     13        0.5336  0.2530\n",
      "     14        0.4404  0.2550\n",
      "     15        0.4006  0.2505\n",
      "     16        0.3641  0.2550\n",
      "     17        0.3895  0.2700\n",
      "     18        0.4071  0.2555\n",
      "     19        0.3786  0.2500\n",
      "     20        0.3189  0.2530\n",
      "     21        0.3320  0.2540\n",
      "     22        0.2906  0.2785\n",
      "     23        0.2637  0.2730\n",
      "     24        0.2457  0.2655\n",
      "     25        0.2411  0.2635\n",
      "     26        0.2310  0.2640\n",
      "     27        0.3753  0.2720\n",
      "     28        0.3342  0.2605\n",
      "     29        0.2525  0.2720\n",
      "     30        0.2042  0.2575\n",
      "==========Fold:62==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2484  0.4831\n",
      "      2        0.1682  0.4841\n",
      "      3        0.1571  0.4756\n",
      "      4        0.1437  0.4696\n",
      "      5        0.1356  0.5051\n",
      "      6        0.1323  0.4851\n",
      "      7        0.1242  0.4746\n",
      "      8        0.1170  0.4956\n",
      "      9        0.1129  0.4861\n",
      "     10        0.1100  0.4796\n",
      "     11        0.1049  0.4626\n",
      "     12        0.1076  0.4796\n",
      "     13        0.0968  0.4641\n",
      "     14        0.0945  0.5031\n",
      "     15        0.0928  0.4891\n",
      "     16        0.0917  0.4971\n",
      "     17        0.0888  0.4981\n",
      "     18        0.0854  0.4946\n",
      "     19        0.0855  0.4811\n",
      "     20        0.0864  0.4911\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1207  0.2460\n",
      "      2        1.7081  0.2540\n",
      "      3        1.2511  0.2545\n",
      "      4        1.0601  0.2575\n",
      "      5        0.8597  0.2505\n",
      "      6        0.7538  0.2560\n",
      "      7        0.7538  0.2530\n",
      "      8        0.6275  0.2535\n",
      "      9        0.5917  0.2411\n",
      "     10        0.5743  0.2545\n",
      "     11        0.5195  0.2440\n",
      "     12        0.4637  0.2570\n",
      "     13        0.4474  0.2655\n",
      "     14        0.4569  0.2535\n",
      "     15        0.4027  0.2470\n",
      "     16        0.4424  0.2600\n",
      "     17        0.4014  0.2610\n",
      "     18        0.4003  0.2570\n",
      "     19        0.3657  0.2590\n",
      "     20        0.3476  0.2530\n",
      "     21        0.2961  0.2515\n",
      "     22        0.2795  0.2590\n",
      "     23        0.2925  0.2535\n",
      "     24        0.3895  0.2570\n",
      "     25        0.3607  0.2585\n",
      "     26        0.3113  0.2560\n",
      "     27        0.2609  0.2485\n",
      "     28        0.2153  0.2515\n",
      "     29        0.2076  0.2580\n",
      "     30        0.1889  0.2610\n",
      "==========Fold:63==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2375  0.4896\n",
      "      2        0.1701  0.5165\n",
      "      3        0.1548  0.5111\n",
      "      4        0.1408  0.4891\n",
      "      5        0.1316  0.4806\n",
      "      6        0.1267  0.4811\n",
      "      7        0.1173  0.4741\n",
      "      8        0.1129  0.4706\n",
      "      9        0.1077  0.4906\n",
      "     10        0.1025  0.4701\n",
      "     11        0.0987  0.4771\n",
      "     12        0.0942  0.4841\n",
      "     13        0.0907  0.4976\n",
      "     14        0.0892  0.4651\n",
      "     15        0.0836  0.4786\n",
      "     16        0.0805  0.4716\n",
      "     17        0.0812  0.4901\n",
      "     18        0.0800  0.4846\n",
      "     19        0.0792  0.4851\n",
      "     20        0.0745  0.5006\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0495  0.2505\n",
      "      2        1.6641  0.2500\n",
      "      3        1.2878  0.2440\n",
      "      4        1.0252  0.2435\n",
      "      5        0.8584  0.2565\n",
      "      6        0.8347  0.2505\n",
      "      7        0.6934  0.2560\n",
      "      8        0.6072  0.2555\n",
      "      9        0.5518  0.2600\n",
      "     10        0.4806  0.2515\n",
      "     11        0.4752  0.2615\n",
      "     12        0.4900  0.2565\n",
      "     13        0.4766  0.2590\n",
      "     14        0.4348  0.2590\n",
      "     15        0.3776  0.2470\n",
      "     16        0.3462  0.2545\n",
      "     17        0.3037  0.2780\n",
      "     18        0.2996  0.2525\n",
      "     19        0.2926  0.2535\n",
      "     20        0.4120  0.2545\n",
      "     21        0.3197  0.2525\n",
      "     22        0.4065  0.2550\n",
      "     23        0.3446  0.2535\n",
      "     24        0.2542  0.2475\n",
      "     25        0.2405  0.2470\n",
      "     26        0.2346  0.2480\n",
      "     27        0.2219  0.2470\n",
      "     28        0.2434  0.2495\n",
      "     29        0.2625  0.2445\n",
      "     30        0.1924  0.2630\n",
      "==========Fold:64==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2564  0.4976\n",
      "      2        0.1635  0.4786\n",
      "      3        0.1490  0.4821\n",
      "      4        0.1403  0.4761\n",
      "      5        0.1333  0.4651\n",
      "      6        0.1248  0.4806\n",
      "      7        0.1195  0.4921\n",
      "      8        0.1131  0.4821\n",
      "      9        0.1133  0.4846\n",
      "     10        0.1080  0.4951\n",
      "     11        0.1012  0.4776\n",
      "     12        0.0973  0.4906\n",
      "     13        0.0998  0.4731\n",
      "     14        0.0897  0.4906\n",
      "     15        0.0911  0.4706\n",
      "     16        0.0878  0.4926\n",
      "     17        0.0895  0.4871\n",
      "     18        0.0801  0.4736\n",
      "     19        0.0821  0.4776\n",
      "     20        0.0886  0.4911\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0885  0.2460\n",
      "      2        1.6460  0.2530\n",
      "      3        1.2040  0.2525\n",
      "      4        1.0432  0.2666\n",
      "      5        0.8463  0.2580\n",
      "      6        0.7610  0.2555\n",
      "      7        0.6789  0.2610\n",
      "      8        0.6133  0.2535\n",
      "      9        0.5674  0.2615\n",
      "     10        0.5290  0.2705\n",
      "     11        0.5033  0.2565\n",
      "     12        0.4551  0.2595\n",
      "     13        0.4403  0.2570\n",
      "     14        0.4107  0.2570\n",
      "     15        0.3956  0.2635\n",
      "     16        0.3613  0.2445\n",
      "     17        0.3749  0.2585\n",
      "     18        0.3295  0.2585\n",
      "     19        0.3057  0.2450\n",
      "     20        0.2866  0.2979\n",
      "     21        0.2716  0.2790\n",
      "     22        0.2584  0.2635\n",
      "     23        0.2542  0.2530\n",
      "     24        0.2712  0.2595\n",
      "     25        0.2424  0.2545\n",
      "     26        0.2189  0.2555\n",
      "     27        0.2526  0.2565\n",
      "     28        0.2278  0.2625\n",
      "     29        0.1974  0.2575\n",
      "     30        0.2043  0.2470\n",
      "==========Fold:65==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2066  0.4991\n",
      "      2        0.1672  0.4796\n",
      "      3        0.1525  0.4821\n",
      "      4        0.1407  0.5021\n",
      "      5        0.1355  0.4856\n",
      "      6        0.1273  0.4676\n",
      "      7        0.1225  0.5195\n",
      "      8        0.1139  0.4926\n",
      "      9        0.1097  0.4791\n",
      "     10        0.1028  0.4891\n",
      "     11        0.1043  0.4891\n",
      "     12        0.0979  0.4936\n",
      "     13        0.0985  0.4896\n",
      "     14        0.0959  0.4776\n",
      "     15        0.0856  0.5036\n",
      "     16        0.0893  0.4881\n",
      "     17        0.0854  0.4771\n",
      "     18        0.0805  0.4896\n",
      "     19        0.0820  0.4736\n",
      "     20        0.0823  0.4716\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0782  0.2635\n",
      "      2        1.7196  0.2610\n",
      "      3        1.3879  0.2500\n",
      "      4        1.1245  0.2545\n",
      "      5        0.9485  0.2575\n",
      "      6        0.8687  0.2575\n",
      "      7        0.7971  0.2600\n",
      "      8        0.6697  0.2470\n",
      "      9        0.6600  0.2575\n",
      "     10        0.6079  0.2565\n",
      "     11        0.5820  0.2515\n",
      "     12        0.4922  0.2520\n",
      "     13        0.4593  0.2495\n",
      "     14        0.4358  0.2470\n",
      "     15        0.4344  0.2525\n",
      "     16        0.4338  0.2555\n",
      "     17        0.4253  0.2575\n",
      "     18        0.3900  0.2570\n",
      "     19        0.3820  0.2800\n",
      "     20        0.3515  0.2570\n",
      "     21        0.3013  0.2575\n",
      "     22        0.2707  0.2550\n",
      "     23        0.2708  0.2575\n",
      "     24        0.3028  0.2565\n",
      "     25        0.2907  0.2605\n",
      "     26        0.3537  0.2505\n",
      "     27        0.2568  0.2535\n",
      "     28        0.2612  0.2555\n",
      "     29        0.2430  0.2540\n",
      "     30        0.2550  0.2500\n",
      "==========Fold:66==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2487  0.4806\n",
      "      2        0.1691  0.4906\n",
      "      3        0.1590  0.4756\n",
      "      4        0.1478  0.4851\n",
      "      5        0.1411  0.4816\n",
      "      6        0.1313  0.4876\n",
      "      7        0.1278  0.4926\n",
      "      8        0.1236  0.4701\n",
      "      9        0.1171  0.4941\n",
      "     10        0.1120  0.4846\n",
      "     11        0.1082  0.4841\n",
      "     12        0.1010  0.4986\n",
      "     13        0.0966  0.4866\n",
      "     14        0.0965  0.4866\n",
      "     15        0.0887  0.4931\n",
      "     16        0.0891  0.4891\n",
      "     17        0.0885  0.4946\n",
      "     18        0.0868  0.4846\n",
      "     19        0.0816  0.4641\n",
      "     20        0.0817  0.4986\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0377  0.2600\n",
      "      2        1.6250  0.2715\n",
      "      3        1.2428  0.2575\n",
      "      4        1.0279  0.2520\n",
      "      5        0.8933  0.2515\n",
      "      6        0.8421  0.2570\n",
      "      7        0.7269  0.2490\n",
      "      8        0.7078  0.2620\n",
      "      9        0.6704  0.2545\n",
      "     10        0.6631  0.2575\n",
      "     11        0.5707  0.2610\n",
      "     12        0.5377  0.2640\n",
      "     13        0.5028  0.2535\n",
      "     14        0.4710  0.2500\n",
      "     15        0.4051  0.2495\n",
      "     16        0.4008  0.2530\n",
      "     17        0.3892  0.2605\n",
      "     18        0.3874  0.2560\n",
      "     19        0.3665  0.2445\n",
      "     20        0.3700  0.2575\n",
      "     21        0.3349  0.2535\n",
      "     22        0.3109  0.2505\n",
      "     23        0.2972  0.2560\n",
      "     24        0.3648  0.2386\n",
      "     25        0.2951  0.2455\n",
      "     26        0.2568  0.2560\n",
      "     27        0.2175  0.2605\n",
      "     28        0.2481  0.2595\n",
      "     29        0.2487  0.2530\n",
      "     30        0.2023  0.2565\n",
      "==========Fold:67==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2464  0.4821\n",
      "      2        0.1643  0.5120\n",
      "      3        0.1534  0.4956\n",
      "      4        0.1427  0.4696\n",
      "      5        0.1367  0.4886\n",
      "      6        0.1291  0.4921\n",
      "      7        0.1260  0.4956\n",
      "      8        0.1186  0.4791\n",
      "      9        0.1114  0.4721\n",
      "     10        0.1075  0.4826\n",
      "     11        0.1048  0.4706\n",
      "     12        0.1018  0.5016\n",
      "     13        0.0944  0.4756\n",
      "     14        0.0982  0.5071\n",
      "     15        0.0967  0.4871\n",
      "     16        0.0879  0.5220\n",
      "     17        0.0870  0.4951\n",
      "     18        0.0831  0.4956\n",
      "     19        0.0775  0.4916\n",
      "     20        0.0827  0.4901\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0814  0.2386\n",
      "      2        1.7444  0.2490\n",
      "      3        1.4006  0.2490\n",
      "      4        1.0964  0.2490\n",
      "      5        0.9779  0.2515\n",
      "      6        0.7967  0.2685\n",
      "      7        0.7178  0.2565\n",
      "      8        0.7116  0.2620\n",
      "      9        0.5902  0.2455\n",
      "     10        0.5386  0.2570\n",
      "     11        0.5167  0.2615\n",
      "     12        0.5116  0.2735\n",
      "     13        0.5080  0.2570\n",
      "     14        0.4582  0.2665\n",
      "     15        0.3851  0.2510\n",
      "     16        0.3699  0.2585\n",
      "     17        0.3614  0.2465\n",
      "     18        0.3793  0.2565\n",
      "     19        0.3469  0.2535\n",
      "     20        0.3704  0.2535\n",
      "     21        0.3030  0.2640\n",
      "     22        0.2596  0.2620\n",
      "     23        0.2830  0.2425\n",
      "     24        0.2848  0.2565\n",
      "     25        0.2667  0.2590\n",
      "     26        0.2427  0.2485\n",
      "     27        0.2352  0.2515\n",
      "     28        0.2735  0.2580\n",
      "     29        0.2629  0.2540\n",
      "     30        0.2471  0.2765\n",
      "==========Fold:68==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2527  0.4841\n",
      "      2        0.1645  0.4861\n",
      "      3        0.1504  0.5036\n",
      "      4        0.1390  0.4871\n",
      "      5        0.1340  0.4661\n",
      "      6        0.1260  0.4811\n",
      "      7        0.1167  0.4696\n",
      "      8        0.1123  0.4696\n",
      "      9        0.1080  0.4921\n",
      "     10        0.1040  0.4746\n",
      "     11        0.0983  0.4981\n",
      "     12        0.0982  0.4781\n",
      "     13        0.0919  0.5046\n",
      "     14        0.0900  0.5116\n",
      "     15        0.0907  0.4851\n",
      "     16        0.0834  0.4801\n",
      "     17        0.0836  0.4796\n",
      "     18        0.0870  0.4851\n",
      "     19        0.0800  0.4826\n",
      "     20        0.0755  0.4841\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0458  0.2530\n",
      "      2        1.6592  0.2465\n",
      "      3        1.2794  0.2845\n",
      "      4        1.1150  0.2695\n",
      "      5        0.9393  0.2625\n",
      "      6        0.7852  0.2615\n",
      "      7        0.7405  0.2490\n",
      "      8        0.6975  0.2565\n",
      "      9        0.6019  0.2585\n",
      "     10        0.5450  0.2665\n",
      "     11        0.5240  0.2740\n",
      "     12        0.4876  0.2590\n",
      "     13        0.4572  0.2530\n",
      "     14        0.4033  0.2550\n",
      "     15        0.4533  0.2550\n",
      "     16        0.4041  0.2550\n",
      "     17        0.4240  0.2535\n",
      "     18        0.3873  0.2450\n",
      "     19        0.3394  0.2510\n",
      "     20        0.3454  0.2495\n",
      "     21        0.2992  0.2580\n",
      "     22        0.3270  0.2515\n",
      "     23        0.3419  0.2515\n",
      "     24        0.3007  0.2620\n",
      "     25        0.2603  0.2525\n",
      "     26        0.2585  0.2760\n",
      "     27        0.2741  0.2555\n",
      "     28        0.2739  0.2570\n",
      "     29        0.2480  0.2455\n",
      "     30        0.2616  0.2550\n",
      "==========Fold:69==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2256  0.4951\n",
      "      2        0.1651  0.4781\n",
      "      3        0.1514  0.5086\n",
      "      4        0.1399  0.4901\n",
      "      5        0.1336  0.4891\n",
      "      6        0.1254  0.4991\n",
      "      7        0.1206  0.5041\n",
      "      8        0.1189  0.4926\n",
      "      9        0.1061  0.4931\n",
      "     10        0.1070  0.4836\n",
      "     11        0.0991  0.4741\n",
      "     12        0.0992  0.4881\n",
      "     13        0.0947  0.4881\n",
      "     14        0.0893  0.5145\n",
      "     15        0.0913  0.4961\n",
      "     16        0.0812  0.4821\n",
      "     17        0.0830  0.4816\n",
      "     18        0.0847  0.4996\n",
      "     19        0.0769  0.4691\n",
      "     20        0.0758  0.4716\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1177  0.2510\n",
      "      2        1.7746  0.2560\n",
      "      3        1.4129  0.2480\n",
      "      4        1.1692  0.2600\n",
      "      5        1.0051  0.2595\n",
      "      6        0.8802  0.2421\n",
      "      7        0.7621  0.2780\n",
      "      8        0.6765  0.2585\n",
      "      9        0.6061  0.2600\n",
      "     10        0.5779  0.2600\n",
      "     11        0.6283  0.2595\n",
      "     12        0.5101  0.2530\n",
      "     13        0.5163  0.2600\n",
      "     14        0.4414  0.2545\n",
      "     15        0.3891  0.2605\n",
      "     16        0.3621  0.2585\n",
      "     17        0.4601  0.2645\n",
      "     18        0.5648  0.2490\n",
      "     19        0.3881  0.2565\n",
      "     20        0.3193  0.2590\n",
      "     21        0.2773  0.2530\n",
      "     22        0.2876  0.2575\n",
      "     23        0.2935  0.2500\n",
      "     24        0.3358  0.2610\n",
      "     25        0.2962  0.2550\n",
      "     26        0.2794  0.2391\n",
      "     27        0.3207  0.2595\n",
      "     28        0.2761  0.2525\n",
      "     29        0.2065  0.2495\n",
      "     30        0.2252  0.2555\n",
      "==========Fold:70==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2000  0.4991\n",
      "      2        0.1690  0.4971\n",
      "      3        0.1547  0.4896\n",
      "      4        0.1460  0.4976\n",
      "      5        0.1373  0.4906\n",
      "      6        0.1339  0.4846\n",
      "      7        0.1254  0.5185\n",
      "      8        0.1165  0.5001\n",
      "      9        0.1187  0.5051\n",
      "     10        0.1080  0.4851\n",
      "     11        0.1050  0.5270\n",
      "     12        0.1019  0.4941\n",
      "     13        0.0945  0.5091\n",
      "     14        0.0965  0.5011\n",
      "     15        0.0918  0.4716\n",
      "     16        0.0882  0.5016\n",
      "     17        0.0878  0.4821\n",
      "     18        0.0847  0.4651\n",
      "     19        0.0820  0.4836\n",
      "     20        0.0837  0.4771\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0335  0.2560\n",
      "      2        1.6100  0.2485\n",
      "      3        1.2142  0.2421\n",
      "      4        1.0401  0.2490\n",
      "      5        0.9434  0.2535\n",
      "      6        0.8167  0.2505\n",
      "      7        0.6996  0.2475\n",
      "      8        0.6584  0.2505\n",
      "      9        0.5970  0.2610\n",
      "     10        0.5837  0.2545\n",
      "     11        0.5295  0.2530\n",
      "     12        0.4276  0.2580\n",
      "     13        0.4379  0.2490\n",
      "     14        0.4391  0.2535\n",
      "     15        0.4553  0.2575\n",
      "     16        0.3769  0.2565\n",
      "     17        0.3664  0.2445\n",
      "     18        0.3336  0.2575\n",
      "     19        0.3103  0.2515\n",
      "     20        0.3473  0.2406\n",
      "     21        0.3444  0.2485\n",
      "     22        0.2636  0.2790\n",
      "     23        0.2913  0.2560\n",
      "     24        0.3183  0.2545\n",
      "     25        0.3253  0.2545\n",
      "     26        0.2396  0.2580\n",
      "     27        0.2093  0.2470\n",
      "     28        0.2829  0.2515\n",
      "     29        0.2319  0.2545\n",
      "     30        0.1785  0.2500\n",
      "==========Fold:71==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2600  0.4831\n",
      "      2        0.1648  0.4926\n",
      "      3        0.1534  0.4801\n",
      "      4        0.1441  0.4871\n",
      "      5        0.1359  0.4886\n",
      "      6        0.1253  0.4921\n",
      "      7        0.1231  0.4796\n",
      "      8        0.1212  0.4931\n",
      "      9        0.1126  0.4931\n",
      "     10        0.1070  0.4606\n",
      "     11        0.1060  0.4886\n",
      "     12        0.0997  0.4826\n",
      "     13        0.0983  0.4831\n",
      "     14        0.0949  0.4916\n",
      "     15        0.0951  0.4981\n",
      "     16        0.0895  0.4816\n",
      "     17        0.0857  0.4746\n",
      "     18        0.0900  0.5061\n",
      "     19        0.0840  0.5006\n",
      "     20        0.0845  0.4831\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0335  0.2720\n",
      "      2        1.6582  0.2645\n",
      "      3        1.3237  0.2570\n",
      "      4        1.0854  0.2505\n",
      "      5        0.8878  0.2625\n",
      "      6        0.7842  0.2580\n",
      "      7        0.7284  0.2505\n",
      "      8        0.6675  0.2605\n",
      "      9        0.5718  0.2570\n",
      "     10        0.5464  0.2595\n",
      "     11        0.5008  0.2575\n",
      "     12        0.4495  0.2525\n",
      "     13        0.4713  0.2550\n",
      "     14        0.4095  0.2605\n",
      "     15        0.3408  0.2625\n",
      "     16        0.3645  0.2575\n",
      "     17        0.3266  0.2485\n",
      "     18        0.3137  0.2500\n",
      "     19        0.2877  0.2550\n",
      "     20        0.3494  0.2505\n",
      "     21        0.3085  0.2495\n",
      "     22        0.2485  0.2530\n",
      "     23        0.3398  0.2366\n",
      "     24        0.2568  0.2575\n",
      "     25        0.2135  0.2530\n",
      "     26        0.2089  0.2625\n",
      "     27        0.1836  0.2605\n",
      "     28        0.2086  0.2535\n",
      "     29        0.1790  0.2430\n",
      "     30        0.1825  0.2520\n",
      "==========Fold:72==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2289  0.5031\n",
      "      2        0.1706  0.4871\n",
      "      3        0.1568  0.4796\n",
      "      4        0.1487  0.4856\n",
      "      5        0.1398  0.4846\n",
      "      6        0.1332  0.4841\n",
      "      7        0.1253  0.4726\n",
      "      8        0.1209  0.4801\n",
      "      9        0.1165  0.4811\n",
      "     10        0.1111  0.4846\n",
      "     11        0.1056  0.4986\n",
      "     12        0.1025  0.4951\n",
      "     13        0.1006  0.4676\n",
      "     14        0.0964  0.4881\n",
      "     15        0.0911  0.4871\n",
      "     16        0.0925  0.4781\n",
      "     17        0.0855  0.4736\n",
      "     18        0.0853  0.4986\n",
      "     19        0.0858  0.4906\n",
      "     20        0.0765  0.4841\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0457  0.2575\n",
      "      2        1.6789  0.2510\n",
      "      3        1.3749  0.2575\n",
      "      4        1.1483  0.2565\n",
      "      5        0.9706  0.2650\n",
      "      6        0.8814  0.2495\n",
      "      7        0.8102  0.2605\n",
      "      8        0.6728  0.2730\n",
      "      9        0.6123  0.2570\n",
      "     10        0.5578  0.2450\n",
      "     11        0.4857  0.2595\n",
      "     12        0.4993  0.2570\n",
      "     13        0.4575  0.2580\n",
      "     14        0.4141  0.2530\n",
      "     15        0.3744  0.2465\n",
      "     16        0.4682  0.2525\n",
      "     17        0.4572  0.2585\n",
      "     18        0.3262  0.2515\n",
      "     19        0.3106  0.2535\n",
      "     20        0.2898  0.2565\n",
      "     21        0.3492  0.2550\n",
      "     22        0.3926  0.2510\n",
      "     23        0.3090  0.2381\n",
      "     24        0.3010  0.2520\n",
      "     25        0.2592  0.2560\n",
      "     26        0.2176  0.2550\n",
      "     27        0.2279  0.2510\n",
      "     28        0.2493  0.2560\n",
      "     29        0.3097  0.2545\n",
      "     30        0.2628  0.2550\n",
      "==========Fold:73==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.3067  0.4911\n",
      "      2        0.1708  0.4831\n",
      "      3        0.1596  0.4746\n",
      "      4        0.1488  0.4971\n",
      "      5        0.1431  0.4796\n",
      "      6        0.1368  0.4786\n",
      "      7        0.1286  0.5006\n",
      "      8        0.1235  0.4941\n",
      "      9        0.1182  0.5061\n",
      "     10        0.1179  0.4841\n",
      "     11        0.1114  0.4801\n",
      "     12        0.1073  0.4801\n",
      "     13        0.1062  0.4696\n",
      "     14        0.1006  0.4941\n",
      "     15        0.1027  0.4886\n",
      "     16        0.0959  0.4791\n",
      "     17        0.0922  0.4906\n",
      "     18        0.0895  0.4831\n",
      "     19        0.0945  0.4816\n",
      "     20        0.0900  0.4706\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0899  0.2550\n",
      "      2        1.7427  0.2500\n",
      "      3        1.4397  0.2490\n",
      "      4        1.1765  0.2565\n",
      "      5        1.0259  0.2575\n",
      "      6        0.9436  0.2565\n",
      "      7        0.8494  0.2515\n",
      "      8        0.7824  0.2565\n",
      "      9        0.6532  0.2565\n",
      "     10        0.6425  0.2510\n",
      "     11        0.5768  0.2585\n",
      "     12        0.5451  0.2535\n",
      "     13        0.4728  0.2570\n",
      "     14        0.5238  0.2510\n",
      "     15        0.4656  0.2495\n",
      "     16        0.4522  0.2655\n",
      "     17        0.4422  0.2815\n",
      "     18        0.4137  0.2640\n",
      "     19        0.3925  0.2495\n",
      "     20        0.3404  0.2580\n",
      "     21        0.3573  0.2525\n",
      "     22        0.4583  0.2530\n",
      "     23        0.3721  0.2505\n",
      "     24        0.2892  0.2545\n",
      "     25        0.3121  0.2401\n",
      "     26        0.2900  0.2535\n",
      "     27        0.2961  0.2450\n",
      "     28        0.2387  0.2460\n",
      "     29        0.2527  0.2545\n",
      "     30        0.2300  0.2560\n",
      "==========Fold:74==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2297  0.4941\n",
      "      2        0.1648  0.4956\n",
      "      3        0.1523  0.5051\n",
      "      4        0.1427  0.4911\n",
      "      5        0.1325  0.4856\n",
      "      6        0.1260  0.4816\n",
      "      7        0.1197  0.4881\n",
      "      8        0.1134  0.4721\n",
      "      9        0.1115  0.4871\n",
      "     10        0.1062  0.4811\n",
      "     11        0.1008  0.4756\n",
      "     12        0.0935  0.4901\n",
      "     13        0.0924  0.4746\n",
      "     14        0.0904  0.4951\n",
      "     15        0.0844  0.5051\n",
      "     16        0.0827  0.5106\n",
      "     17        0.0808  0.4661\n",
      "     18        0.0803  0.4856\n",
      "     19        0.0734  0.4731\n",
      "     20        0.0724  0.5031\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0754  0.2585\n",
      "      2        1.6953  0.2530\n",
      "      3        1.3505  0.2530\n",
      "      4        1.1276  0.2520\n",
      "      5        0.9693  0.2450\n",
      "      6        0.8029  0.2505\n",
      "      7        0.8226  0.2595\n",
      "      8        0.6801  0.2520\n",
      "      9        0.5970  0.2570\n",
      "     10        0.5801  0.2460\n",
      "     11        0.4924  0.2550\n",
      "     12        0.4966  0.2440\n",
      "     13        0.5273  0.2570\n",
      "     14        0.4451  0.2565\n",
      "     15        0.4337  0.2585\n",
      "     16        0.4056  0.2515\n",
      "     17        0.5103  0.2550\n",
      "     18        0.3430  0.2560\n",
      "     19        0.3045  0.2630\n",
      "     20        0.3209  0.2495\n",
      "     21        0.3021  0.2570\n",
      "     22        0.3800  0.2615\n",
      "     23        0.3960  0.2565\n",
      "     24        0.2949  0.2500\n",
      "     25        0.2425  0.2595\n",
      "     26        0.2452  0.2555\n",
      "     27        0.2142  0.2600\n",
      "     28        0.1967  0.2815\n",
      "     29        0.2402  0.2585\n",
      "     30        0.2209  0.2585\n",
      "==========Fold:75==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2390  0.5006\n",
      "      2        0.1660  0.4941\n",
      "      3        0.1544  0.4856\n",
      "      4        0.1457  0.4761\n",
      "      5        0.1387  0.4961\n",
      "      6        0.1311  0.4976\n",
      "      7        0.1252  0.4826\n",
      "      8        0.1172  0.5140\n",
      "      9        0.1130  0.4826\n",
      "     10        0.1116  0.5056\n",
      "     11        0.1048  0.5006\n",
      "     12        0.1000  0.5300\n",
      "     13        0.0959  0.4991\n",
      "     14        0.0948  0.5011\n",
      "     15        0.0892  0.5046\n",
      "     16        0.0882  0.5215\n",
      "     17        0.0920  0.5096\n",
      "     18        0.0794  0.5006\n",
      "     19        0.0828  0.4876\n",
      "     20        0.0766  0.4856\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0632  0.2575\n",
      "      2        1.7551  0.2520\n",
      "      3        1.4837  0.2620\n",
      "      4        1.2516  0.2505\n",
      "      5        1.0903  0.2730\n",
      "      6        0.9451  0.2615\n",
      "      7        0.8381  0.2625\n",
      "      8        0.7384  0.2560\n",
      "      9        0.7113  0.2575\n",
      "     10        0.6690  0.2520\n",
      "     11        0.5815  0.2600\n",
      "     12        0.5487  0.2565\n",
      "     13        0.5104  0.2510\n",
      "     14        0.4542  0.2565\n",
      "     15        0.4223  0.2615\n",
      "     16        0.4367  0.2580\n",
      "     17        0.4027  0.2485\n",
      "     18        0.3734  0.2525\n",
      "     19        0.4061  0.2600\n",
      "     20        0.3775  0.2555\n",
      "     21        0.3402  0.2605\n",
      "     22        0.3638  0.2625\n",
      "     23        0.3345  0.2685\n",
      "     24        0.3153  0.2700\n",
      "     25        0.2941  0.2640\n",
      "     26        0.2494  0.2550\n",
      "     27        0.2499  0.2510\n",
      "     28        0.2934  0.2675\n",
      "     29        0.3679  0.2555\n",
      "     30        0.3050  0.2470\n",
      "==========Fold:76==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2466  0.4961\n",
      "      2        0.1699  0.5031\n",
      "      3        0.1582  0.4816\n",
      "      4        0.1471  0.5115\n",
      "      5        0.1416  0.4986\n",
      "      6        0.1331  0.4936\n",
      "      7        0.1300  0.4836\n",
      "      8        0.1235  0.4926\n",
      "      9        0.1178  0.4886\n",
      "     10        0.1130  0.4746\n",
      "     11        0.1062  0.4926\n",
      "     12        0.1032  0.4801\n",
      "     13        0.1042  0.4876\n",
      "     14        0.0973  0.4921\n",
      "     15        0.0956  0.4886\n",
      "     16        0.0972  0.4766\n",
      "     17        0.0931  0.4906\n",
      "     18        0.0909  0.4941\n",
      "     19        0.0870  0.4656\n",
      "     20        0.0819  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0720  0.2371\n",
      "      2        1.7141  0.2580\n",
      "      3        1.3291  0.2535\n",
      "      4        1.1263  0.2455\n",
      "      5        0.9697  0.2480\n",
      "      6        0.8522  0.2555\n",
      "      7        0.7860  0.2600\n",
      "      8        0.6975  0.2625\n",
      "      9        0.6669  0.2540\n",
      "     10        0.6522  0.2445\n",
      "     11        0.5953  0.2810\n",
      "     12        0.5354  0.2615\n",
      "     13        0.5211  0.2625\n",
      "     14        0.5106  0.2590\n",
      "     15        0.4888  0.2495\n",
      "     16        0.4088  0.2490\n",
      "     17        0.4085  0.2510\n",
      "     18        0.3946  0.2475\n",
      "     19        0.3652  0.2570\n",
      "     20        0.3691  0.2525\n",
      "     21        0.3794  0.2555\n",
      "     22        0.3395  0.2605\n",
      "     23        0.3332  0.2535\n",
      "     24        0.3143  0.2475\n",
      "     25        0.3187  0.2570\n",
      "     26        0.3820  0.2555\n",
      "     27        0.2951  0.2575\n",
      "     28        0.3244  0.2505\n",
      "     29        0.3053  0.2580\n",
      "     30        0.2327  0.2570\n",
      "==========Fold:77==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2520  0.4821\n",
      "      2        0.1699  0.4811\n",
      "      3        0.1545  0.4876\n",
      "      4        0.1432  0.4886\n",
      "      5        0.1331  0.4966\n",
      "      6        0.1276  0.4676\n",
      "      7        0.1288  0.4746\n",
      "      8        0.1215  0.4801\n",
      "      9        0.1163  0.4931\n",
      "     10        0.1118  0.5031\n",
      "     11        0.1056  0.4856\n",
      "     12        0.0984  0.5016\n",
      "     13        0.1026  0.4851\n",
      "     14        0.0985  0.4921\n",
      "     15        0.0970  0.4796\n",
      "     16        0.0898  0.4731\n",
      "     17        0.1009  0.4881\n",
      "     18        0.0932  0.4941\n",
      "     19        0.0918  0.4801\n",
      "     20        0.0917  0.4861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0897  0.2570\n",
      "      2        1.7150  0.2500\n",
      "      3        1.4082  0.2565\n",
      "      4        1.1374  0.2550\n",
      "      5        0.9732  0.2570\n",
      "      6        0.8453  0.2470\n",
      "      7        0.7275  0.2500\n",
      "      8        0.6354  0.2570\n",
      "      9        0.6027  0.2535\n",
      "     10        0.5673  0.2590\n",
      "     11        0.5520  0.2535\n",
      "     12        0.4558  0.2615\n",
      "     13        0.4410  0.2510\n",
      "     14        0.3860  0.2535\n",
      "     15        0.3700  0.2515\n",
      "     16        0.4546  0.2585\n",
      "     17        0.3641  0.2430\n",
      "     18        0.3332  0.2525\n",
      "     19        0.3710  0.2575\n",
      "     20        0.2799  0.2495\n",
      "     21        0.2918  0.2785\n",
      "     22        0.2808  0.2650\n",
      "     23        0.2762  0.2580\n",
      "     24        0.2555  0.2625\n",
      "     25        0.2902  0.2545\n",
      "     26        0.2587  0.2520\n",
      "     27        0.2125  0.2610\n",
      "     28        0.1993  0.2485\n",
      "     29        0.2234  0.2520\n",
      "     30        0.2352  0.2565\n",
      "==========Fold:78==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2311  0.4911\n",
      "      2        0.1654  0.4841\n",
      "      3        0.1533  0.4846\n",
      "      4        0.1451  0.5016\n",
      "      5        0.1383  0.4916\n",
      "      6        0.1300  0.4886\n",
      "      7        0.1198  0.4776\n",
      "      8        0.1173  0.4926\n",
      "      9        0.1147  0.4971\n",
      "     10        0.1056  0.4961\n",
      "     11        0.1039  0.4821\n",
      "     12        0.1093  0.4996\n",
      "     13        0.1001  0.4971\n",
      "     14        0.0915  0.4926\n",
      "     15        0.0905  0.4966\n",
      "     16        0.0881  0.4881\n",
      "     17        0.0859  0.4756\n",
      "     18        0.0824  0.4886\n",
      "     19        0.0871  0.4891\n",
      "     20        0.0758  0.4791\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0549  0.2780\n",
      "      2        1.6803  0.2565\n",
      "      3        1.3734  0.2635\n",
      "      4        1.1248  0.2605\n",
      "      5        0.9366  0.2540\n",
      "      6        0.7877  0.2590\n",
      "      7        0.7422  0.2440\n",
      "      8        0.6595  0.2485\n",
      "      9        0.5678  0.2605\n",
      "     10        0.6081  0.2505\n",
      "     11        0.5126  0.2465\n",
      "     12        0.4516  0.2470\n",
      "     13        0.4999  0.2540\n",
      "     14        0.4602  0.2530\n",
      "     15        0.3669  0.2615\n",
      "     16        0.3597  0.2510\n",
      "     17        0.3559  0.2665\n",
      "     18        0.3328  0.2570\n",
      "     19        0.2653  0.2595\n",
      "     20        0.3121  0.2600\n",
      "     21        0.3104  0.2560\n",
      "     22        0.2981  0.2525\n",
      "     23        0.2873  0.2575\n",
      "     24        0.2546  0.2495\n",
      "     25        0.2710  0.2555\n",
      "     26        0.2298  0.2550\n",
      "     27        0.2196  0.2530\n",
      "     28        0.2120  0.2545\n",
      "     29        0.2863  0.2515\n",
      "     30        0.2612  0.2630\n",
      "==========Fold:79==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2106  0.4851\n",
      "      2        0.1650  0.5265\n",
      "      3        0.1531  0.4706\n",
      "      4        0.1440  0.4661\n",
      "      5        0.1347  0.4831\n",
      "      6        0.1252  0.4896\n",
      "      7        0.1251  0.4761\n",
      "      8        0.1147  0.4796\n",
      "      9        0.1122  0.4856\n",
      "     10        0.1059  0.4776\n",
      "     11        0.1032  0.4831\n",
      "     12        0.0967  0.4946\n",
      "     13        0.0959  0.4926\n",
      "     14        0.0918  0.4846\n",
      "     15        0.0892  0.5026\n",
      "     16        0.0857  0.4831\n",
      "     17        0.0885  0.4916\n",
      "     18        0.0867  0.4866\n",
      "     19        0.0823  0.4726\n",
      "     20        0.0798  0.4916\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0899  0.2525\n",
      "      2        1.7255  0.2600\n",
      "      3        1.3946  0.2535\n",
      "      4        1.0935  0.2555\n",
      "      5        1.0029  0.2475\n",
      "      6        0.8714  0.2570\n",
      "      7        0.8021  0.2595\n",
      "      8        0.7443  0.2810\n",
      "      9        0.6891  0.2600\n",
      "     10        0.6149  0.2475\n",
      "     11        0.5653  0.2425\n",
      "     12        0.5277  0.2525\n",
      "     13        0.4845  0.2495\n",
      "     14        0.4913  0.2545\n",
      "     15        0.4741  0.2525\n",
      "     16        0.4384  0.2550\n",
      "     17        0.3934  0.2545\n",
      "     18        0.3621  0.2605\n",
      "     19        0.4377  0.2535\n",
      "     20        0.3724  0.2565\n",
      "     21        0.3159  0.2605\n",
      "     22        0.3289  0.2495\n",
      "     23        0.3409  0.2515\n",
      "     24        0.3195  0.2555\n",
      "     25        0.2856  0.2540\n",
      "     26        0.2557  0.2465\n",
      "     27        0.4137  0.2485\n",
      "     28        0.4181  0.2520\n",
      "     29        0.2608  0.2530\n",
      "     30        0.2128  0.2505\n",
      "==========Fold:80==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2384  0.4961\n",
      "      2        0.1640  0.4831\n",
      "      3        0.1541  0.4856\n",
      "      4        0.1421  0.4891\n",
      "      5        0.1338  0.5375\n",
      "      6        0.1253  0.5150\n",
      "      7        0.1201  0.5056\n",
      "      8        0.1158  0.4931\n",
      "      9        0.1092  0.4721\n",
      "     10        0.1050  0.4826\n",
      "     11        0.1005  0.5011\n",
      "     12        0.0988  0.4871\n",
      "     13        0.0984  0.4706\n",
      "     14        0.0872  0.4801\n",
      "     15        0.0899  0.5086\n",
      "     16        0.0834  0.4631\n",
      "     17        0.0795  0.4766\n",
      "     18        0.0784  0.5011\n",
      "     19        0.0814  0.4801\n",
      "     20        0.0776  0.4666\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0704  0.2520\n",
      "      2        1.7065  0.2655\n",
      "      3        1.3688  0.2465\n",
      "      4        1.0966  0.2505\n",
      "      5        0.9795  0.2535\n",
      "      6        0.8242  0.2525\n",
      "      7        0.7529  0.2460\n",
      "      8        0.6828  0.2845\n",
      "      9        0.7348  0.2575\n",
      "     10        0.6633  0.2560\n",
      "     11        0.5652  0.2565\n",
      "     12        0.4934  0.2605\n",
      "     13        0.4540  0.2540\n",
      "     14        0.4165  0.2440\n",
      "     15        0.4282  0.2575\n",
      "     16        0.4052  0.2535\n",
      "     17        0.4310  0.2635\n",
      "     18        0.3706  0.2555\n",
      "     19        0.3546  0.2520\n",
      "     20        0.3323  0.2560\n",
      "     21        0.4286  0.2575\n",
      "     22        0.3710  0.2480\n",
      "     23        0.3209  0.2505\n",
      "     24        0.2619  0.2610\n",
      "     25        0.2601  0.2505\n",
      "     26        0.2794  0.2515\n",
      "     27        0.2575  0.2560\n",
      "     28        0.2113  0.2470\n",
      "     29        0.2134  0.2495\n",
      "     30        0.1867  0.2495\n",
      "==========Fold:81==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2415  0.4866\n",
      "      2        0.1654  0.4956\n",
      "      3        0.1533  0.4846\n",
      "      4        0.1444  0.4951\n",
      "      5        0.1366  0.4746\n",
      "      6        0.1283  0.5215\n",
      "      7        0.1229  0.4966\n",
      "      8        0.1194  0.4811\n",
      "      9        0.1094  0.4956\n",
      "     10        0.1110  0.4861\n",
      "     11        0.1038  0.5051\n",
      "     12        0.0981  0.4871\n",
      "     13        0.1002  0.4966\n",
      "     14        0.1021  0.5041\n",
      "     15        0.0928  0.5051\n",
      "     16        0.0899  0.5066\n",
      "     17        0.0914  0.4881\n",
      "     18        0.0867  0.4726\n",
      "     19        0.0813  0.4951\n",
      "     20        0.0876  0.4986\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0722  0.2565\n",
      "      2        1.7073  0.2660\n",
      "      3        1.4059  0.2984\n",
      "      4        1.1309  0.2670\n",
      "      5        1.0035  0.2535\n",
      "      6        0.8551  0.2550\n",
      "      7        0.7477  0.2605\n",
      "      8        0.7136  0.2615\n",
      "      9        0.6339  0.2585\n",
      "     10        0.5970  0.2570\n",
      "     11        0.5619  0.2545\n",
      "     12        0.5217  0.2550\n",
      "     13        0.5199  0.2780\n",
      "     14        0.5100  0.2560\n",
      "     15        0.4217  0.2615\n",
      "     16        0.3892  0.2595\n",
      "     17        0.3657  0.2465\n",
      "     18        0.3608  0.2640\n",
      "     19        0.4036  0.2475\n",
      "     20        0.3190  0.2500\n",
      "     21        0.3471  0.2550\n",
      "     22        0.3974  0.2525\n",
      "     23        0.3999  0.2595\n",
      "     24        0.3037  0.2605\n",
      "     25        0.2431  0.2525\n",
      "     26        0.2431  0.2520\n",
      "     27        0.2680  0.2495\n",
      "     28        0.2891  0.2470\n",
      "     29        0.2795  0.2610\n",
      "     30        0.2565  0.2765\n",
      "==========Fold:82==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2600  0.5056\n",
      "      2        0.1702  0.4911\n",
      "      3        0.1577  0.4926\n",
      "      4        0.1462  0.5295\n",
      "      5        0.1389  0.5076\n",
      "      6        0.1290  0.4901\n",
      "      7        0.1280  0.4896\n",
      "      8        0.1210  0.4831\n",
      "      9        0.1185  0.4916\n",
      "     10        0.1142  0.4901\n",
      "     11        0.1113  0.4931\n",
      "     12        0.1029  0.4791\n",
      "     13        0.1013  0.4926\n",
      "     14        0.0967  0.4751\n",
      "     15        0.0964  0.4961\n",
      "     16        0.0952  0.5245\n",
      "     17        0.0894  0.4871\n",
      "     18        0.0904  0.4956\n",
      "     19        0.0880  0.5265\n",
      "     20        0.0832  0.4826\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1064  0.2580\n",
      "      2        1.7713  0.2620\n",
      "      3        1.4382  0.2610\n",
      "      4        1.1965  0.2630\n",
      "      5        1.0251  0.2600\n",
      "      6        0.8828  0.2585\n",
      "      7        0.8126  0.2490\n",
      "      8        0.7368  0.2655\n",
      "      9        0.6958  0.2570\n",
      "     10        0.5917  0.2625\n",
      "     11        0.6067  0.2635\n",
      "     12        0.6617  0.2555\n",
      "     13        0.4834  0.2520\n",
      "     14        0.4970  0.2575\n",
      "     15        0.4365  0.2425\n",
      "     16        0.4408  0.2625\n",
      "     17        0.4187  0.2555\n",
      "     18        0.3951  0.2560\n",
      "     19        0.3692  0.2470\n",
      "     20        0.3436  0.2635\n",
      "     21        0.3535  0.2540\n",
      "     22        0.3332  0.2805\n",
      "     23        0.3252  0.2710\n",
      "     24        0.3154  0.2530\n",
      "     25        0.2646  0.2585\n",
      "     26        0.2567  0.2610\n",
      "     27        0.3129  0.2600\n",
      "     28        0.3033  0.2570\n",
      "     29        0.2406  0.2620\n",
      "     30        0.2275  0.2595\n",
      "==========Fold:83==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2279  0.4696\n",
      "      2        0.1641  0.4921\n",
      "      3        0.1520  0.4946\n",
      "      4        0.1425  0.4926\n",
      "      5        0.1338  0.4936\n",
      "      6        0.1282  0.4686\n",
      "      7        0.1194  0.4936\n",
      "      8        0.1178  0.4841\n",
      "      9        0.1100  0.4881\n",
      "     10        0.1089  0.4891\n",
      "     11        0.1047  0.4876\n",
      "     12        0.0967  0.4851\n",
      "     13        0.0949  0.5026\n",
      "     14        0.0917  0.4816\n",
      "     15        0.0914  0.4691\n",
      "     16        0.0884  0.5310\n",
      "     17        0.0861  0.4881\n",
      "     18        0.0838  0.5036\n",
      "     19        0.0803  0.4866\n",
      "     20        0.0760  0.4971\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0817  0.2535\n",
      "      2        1.7475  0.2610\n",
      "      3        1.4581  0.2540\n",
      "      4        1.1431  0.2560\n",
      "      5        0.9987  0.2580\n",
      "      6        0.8741  0.2485\n",
      "      7        0.7854  0.2545\n",
      "      8        0.6204  0.2530\n",
      "      9        0.5711  0.2615\n",
      "     10        0.6354  0.2500\n",
      "     11        0.5872  0.2570\n",
      "     12        0.4731  0.2485\n",
      "     13        0.4208  0.2500\n",
      "     14        0.3919  0.2440\n",
      "     15        0.4234  0.2615\n",
      "     16        0.3819  0.2510\n",
      "     17        0.3250  0.2625\n",
      "     18        0.3001  0.2585\n",
      "     19        0.3225  0.2630\n",
      "     20        0.3642  0.2595\n",
      "     21        0.2957  0.2560\n",
      "     22        0.2540  0.2510\n",
      "     23        0.2568  0.2785\n",
      "     24        0.2367  0.2630\n",
      "     25        0.2250  0.2545\n",
      "     26        0.1932  0.2515\n",
      "     27        0.2698  0.2535\n",
      "     28        0.2335  0.2610\n",
      "     29        0.2441  0.2580\n",
      "     30        0.2015  0.2470\n",
      "==========Fold:84==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2547  0.4911\n",
      "      2        0.1676  0.5026\n",
      "      3        0.1556  0.4896\n",
      "      4        0.1441  0.4666\n",
      "      5        0.1369  0.4931\n",
      "      6        0.1315  0.4781\n",
      "      7        0.1250  0.4946\n",
      "      8        0.1227  0.4831\n",
      "      9        0.1136  0.4946\n",
      "     10        0.1093  0.4911\n",
      "     11        0.1038  0.4946\n",
      "     12        0.1027  0.4926\n",
      "     13        0.0954  0.4886\n",
      "     14        0.1004  0.4881\n",
      "     15        0.0916  0.4756\n",
      "     16        0.0876  0.4936\n",
      "     17        0.0891  0.5051\n",
      "     18        0.0824  0.4896\n",
      "     19        0.0881  0.4856\n",
      "     20        0.0832  0.4821\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0789  0.2550\n",
      "      2        1.6695  0.2820\n",
      "      3        1.3322  0.2605\n",
      "      4        1.1301  0.2490\n",
      "      5        0.9786  0.2595\n",
      "      6        0.8268  0.2630\n",
      "      7        0.7534  0.2500\n",
      "      8        0.7527  0.2535\n",
      "      9        0.6556  0.2500\n",
      "     10        0.5816  0.2555\n",
      "     11        0.5703  0.2555\n",
      "     12        0.5075  0.2535\n",
      "     13        0.4694  0.2445\n",
      "     14        0.4455  0.2600\n",
      "     15        0.4052  0.2585\n",
      "     16        0.4355  0.2530\n",
      "     17        0.3877  0.2510\n",
      "     18        0.3237  0.2550\n",
      "     19        0.3154  0.2550\n",
      "     20        0.3990  0.2570\n",
      "     21        0.2994  0.2650\n",
      "     22        0.2862  0.2625\n",
      "     23        0.2601  0.2540\n",
      "     24        0.2739  0.2610\n",
      "     25        0.3184  0.2585\n",
      "     26        0.3555  0.2530\n",
      "     27        0.2714  0.2550\n",
      "     28        0.2080  0.2500\n",
      "     29        0.1917  0.2560\n",
      "     30        0.2171  0.2750\n",
      "==========Fold:85==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2292  0.4981\n",
      "      2        0.1658  0.4901\n",
      "      3        0.1530  0.4976\n",
      "      4        0.1415  0.4951\n",
      "      5        0.1344  0.5056\n",
      "      6        0.1269  0.4721\n",
      "      7        0.1204  0.4876\n",
      "      8        0.1157  0.4986\n",
      "      9        0.1104  0.4781\n",
      "     10        0.1087  0.4771\n",
      "     11        0.1024  0.4971\n",
      "     12        0.0999  0.4966\n",
      "     13        0.0968  0.4916\n",
      "     14        0.0907  0.4891\n",
      "     15        0.0856  0.4781\n",
      "     16        0.0817  0.4976\n",
      "     17        0.0813  0.5071\n",
      "     18        0.0825  0.4871\n",
      "     19        0.0795  0.4931\n",
      "     20        0.0776  0.4651\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0749  0.2565\n",
      "      2        1.7460  0.2565\n",
      "      3        1.4838  0.2585\n",
      "      4        1.2373  0.2805\n",
      "      5        0.9820  0.2600\n",
      "      6        0.8477  0.2535\n",
      "      7        0.8947  0.2600\n",
      "      8        0.7696  0.2625\n",
      "      9        0.6651  0.2580\n",
      "     10        0.5659  0.2575\n",
      "     11        0.5280  0.2545\n",
      "     12        0.5046  0.2555\n",
      "     13        0.4613  0.2540\n",
      "     14        0.4269  0.2580\n",
      "     15        0.4861  0.2520\n",
      "     16        0.5395  0.2595\n",
      "     17        0.4794  0.2560\n",
      "     18        0.3715  0.2445\n",
      "     19        0.3758  0.2580\n",
      "     20        0.3906  0.2600\n",
      "     21        0.3856  0.2500\n",
      "     22        0.3038  0.2620\n",
      "     23        0.2830  0.2520\n",
      "     24        0.3693  0.2490\n",
      "     25        0.3305  0.2610\n",
      "     26        0.2768  0.2540\n",
      "     27        0.2667  0.2600\n",
      "     28        0.2431  0.2585\n",
      "     29        0.2079  0.2535\n",
      "     30        0.1977  0.2580\n",
      "==========Fold:86==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.1993  0.4986\n",
      "      2        0.1653  0.5140\n",
      "      3        0.1533  0.4866\n",
      "      4        0.1434  0.4726\n",
      "      5        0.1334  0.4971\n",
      "      6        0.1273  0.4956\n",
      "      7        0.1217  0.4846\n",
      "      8        0.1152  0.5006\n",
      "      9        0.1156  0.4916\n",
      "     10        0.1031  0.4951\n",
      "     11        0.1070  0.4966\n",
      "     12        0.0989  0.4866\n",
      "     13        0.0982  0.5021\n",
      "     14        0.0958  0.4616\n",
      "     15        0.0945  0.4996\n",
      "     16        0.0906  0.4891\n",
      "     17        0.0898  0.4721\n",
      "     18        0.0852  0.4871\n",
      "     19        0.0873  0.4786\n",
      "     20        0.0857  0.4756\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1013  0.2560\n",
      "      2        1.7762  0.2595\n",
      "      3        1.4486  0.2570\n",
      "      4        1.1837  0.2590\n",
      "      5        1.0538  0.2570\n",
      "      6        0.8845  0.2650\n",
      "      7        0.8179  0.2800\n",
      "      8        0.7284  0.2630\n",
      "      9        0.7085  0.2545\n",
      "     10        0.6459  0.2525\n",
      "     11        0.5746  0.2475\n",
      "     12        0.5578  0.2525\n",
      "     13        0.5450  0.2585\n",
      "     14        0.5002  0.2515\n",
      "     15        0.4870  0.2520\n",
      "     16        0.4293  0.2570\n",
      "     17        0.4560  0.2610\n",
      "     18        0.4766  0.2570\n",
      "     19        0.3573  0.2470\n",
      "     20        0.3348  0.2500\n",
      "     21        0.3189  0.2590\n",
      "     22        0.3326  0.2580\n",
      "     23        0.3448  0.2515\n",
      "     24        0.3835  0.2570\n",
      "     25        0.3596  0.2555\n",
      "     26        0.3369  0.2605\n",
      "     27        0.2755  0.2560\n",
      "     28        0.2614  0.2515\n",
      "     29        0.2679  0.2565\n",
      "     30        0.2537  0.2575\n",
      "==========Fold:87==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2307  0.5051\n",
      "      2        0.1681  0.4966\n",
      "      3        0.1554  0.4891\n",
      "      4        0.1446  0.5006\n",
      "      5        0.1358  0.5140\n",
      "      6        0.1323  0.4896\n",
      "      7        0.1248  0.4876\n",
      "      8        0.1202  0.4761\n",
      "      9        0.1163  0.4941\n",
      "     10        0.1149  0.4781\n",
      "     11        0.1055  0.4886\n",
      "     12        0.1032  0.4861\n",
      "     13        0.0998  0.4836\n",
      "     14        0.1001  0.4826\n",
      "     15        0.0956  0.4911\n",
      "     16        0.0987  0.4766\n",
      "     17        0.0977  0.4991\n",
      "     18        0.0905  0.5056\n",
      "     19        0.0892  0.4876\n",
      "     20        0.0924  0.4971\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0592  0.2540\n",
      "      2        1.6998  0.2575\n",
      "      3        1.4100  0.2555\n",
      "      4        1.1419  0.2505\n",
      "      5        0.9888  0.2580\n",
      "      6        0.9244  0.2570\n",
      "      7        0.8335  0.2615\n",
      "      8        0.7621  0.2545\n",
      "      9        0.7243  0.2510\n",
      "     10        0.6727  0.2570\n",
      "     11        0.6151  0.2780\n",
      "     12        0.5347  0.2630\n",
      "     13        0.5511  0.2505\n",
      "     14        0.5289  0.2535\n",
      "     15        0.5574  0.2590\n",
      "     16        0.4729  0.2495\n",
      "     17        0.4183  0.2585\n",
      "     18        0.4499  0.2600\n",
      "     19        0.4473  0.2590\n",
      "     20        0.4863  0.2585\n",
      "     21        0.3652  0.2535\n",
      "     22        0.3777  0.2610\n",
      "     23        0.3988  0.2535\n",
      "     24        0.3765  0.2610\n",
      "     25        0.3148  0.2535\n",
      "     26        0.2761  0.2585\n",
      "     27        0.2676  0.2595\n",
      "     28        0.2948  0.2610\n",
      "     29        0.2783  0.2540\n",
      "     30        0.2941  0.2555\n",
      "==========Fold:88==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2301  0.4936\n",
      "      2        0.1639  0.4981\n",
      "      3        0.1529  0.4821\n",
      "      4        0.1391  0.4921\n",
      "      5        0.1338  0.4861\n",
      "      6        0.1254  0.4946\n",
      "      7        0.1195  0.4851\n",
      "      8        0.1173  0.4856\n",
      "      9        0.1106  0.5250\n",
      "     10        0.1048  0.4981\n",
      "     11        0.0995  0.4881\n",
      "     12        0.1019  0.4791\n",
      "     13        0.0942  0.4906\n",
      "     14        0.0907  0.5101\n",
      "     15        0.0902  0.4986\n",
      "     16        0.0857  0.5026\n",
      "     17        0.0823  0.4836\n",
      "     18        0.0852  0.4831\n",
      "     19        0.0766  0.4886\n",
      "     20        0.0794  0.4926\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1028  0.2570\n",
      "      2        1.7950  0.2619\n",
      "      3        1.4448  0.2630\n",
      "      4        1.2013  0.2545\n",
      "      5        1.0450  0.2590\n",
      "      6        0.8812  0.2625\n",
      "      7        0.7825  0.2615\n",
      "      8        0.7155  0.2865\n",
      "      9        0.6976  0.2750\n",
      "     10        0.5913  0.2630\n",
      "     11        0.5142  0.2670\n",
      "     12        0.4898  0.2575\n",
      "     13        0.5111  0.2475\n",
      "     14        0.5086  0.2660\n",
      "     15        0.4404  0.2750\n",
      "     16        0.4337  0.2890\n",
      "     17        0.4536  0.2635\n",
      "     18        0.4114  0.2695\n",
      "     19        0.4170  0.2545\n",
      "     20        0.3368  0.2595\n",
      "     21        0.3590  0.2620\n",
      "     22        0.4247  0.2555\n",
      "     23        0.3504  0.2525\n",
      "     24        0.2704  0.2470\n",
      "     25        0.3332  0.2665\n",
      "     26        0.3398  0.2625\n",
      "     27        0.3105  0.2585\n",
      "     28        0.3244  0.2530\n",
      "     29        0.2558  0.2500\n",
      "     30        0.2263  0.2635\n",
      "==========Fold:89==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2925  0.5270\n",
      "      2        0.1676  0.5076\n",
      "      3        0.1543  0.4986\n",
      "      4        0.1465  0.4966\n",
      "      5        0.1375  0.4691\n",
      "      6        0.1321  0.5106\n",
      "      7        0.1247  0.4926\n",
      "      8        0.1195  0.5011\n",
      "      9        0.1198  0.5021\n",
      "     10        0.1122  0.4846\n",
      "     11        0.1091  0.4766\n",
      "     12        0.1067  0.4791\n",
      "     13        0.1025  0.4861\n",
      "     14        0.0989  0.5086\n",
      "     15        0.0994  0.4911\n",
      "     16        0.0934  0.5061\n",
      "     17        0.0966  0.5056\n",
      "     18        0.0887  0.4741\n",
      "     19        0.0921  0.4886\n",
      "     20        0.0925  0.4976\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1413  0.2560\n",
      "      2        1.8782  0.2585\n",
      "      3        1.4709  0.2555\n",
      "      4        1.2265  0.2565\n",
      "      5        1.0730  0.2575\n",
      "      6        0.9392  0.2505\n",
      "      7        0.8016  0.2590\n",
      "      8        0.7975  0.2530\n",
      "      9        0.6785  0.2570\n",
      "     10        0.6238  0.2540\n",
      "     11        0.5654  0.2610\n",
      "     12        0.5778  0.2460\n",
      "     13        0.4874  0.2580\n",
      "     14        0.4437  0.2570\n",
      "     15        0.4178  0.2500\n",
      "     16        0.4436  0.2570\n",
      "     17        0.4856  0.2535\n",
      "     18        0.3857  0.2590\n",
      "     19        0.3517  0.2575\n",
      "     20        0.3849  0.2620\n",
      "     21        0.3418  0.2830\n",
      "     22        0.3160  0.2635\n",
      "     23        0.3103  0.2640\n",
      "     24        0.2732  0.2570\n",
      "     25        0.2949  0.2540\n",
      "     26        0.3183  0.2620\n",
      "     27        0.3069  0.2545\n",
      "     28        0.2786  0.2580\n",
      "     29        0.3151  0.2590\n",
      "     30        0.2813  0.2570\n",
      "==========Fold:90==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2608  0.4896\n",
      "      2        0.1694  0.4861\n",
      "      3        0.1555  0.4801\n",
      "      4        0.1466  0.5026\n",
      "      5        0.1377  0.4721\n",
      "      6        0.1305  0.4896\n",
      "      7        0.1260  0.4916\n",
      "      8        0.1231  0.4931\n",
      "      9        0.1192  0.4951\n",
      "     10        0.1167  0.4806\n",
      "     11        0.1174  0.4901\n",
      "     12        0.1132  0.5076\n",
      "     13        0.1003  0.4736\n",
      "     14        0.0987  0.4876\n",
      "     15        0.0984  0.4971\n",
      "     16        0.1002  0.4911\n",
      "     17        0.0918  0.4991\n",
      "     18        0.0950  0.5195\n",
      "     19        0.0941  0.5056\n",
      "     20        0.0934  0.4861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0715  0.2575\n",
      "      2        1.7192  0.2560\n",
      "      3        1.3724  0.2510\n",
      "      4        1.1346  0.2515\n",
      "      5        0.9573  0.2545\n",
      "      6        0.8397  0.2535\n",
      "      7        0.7642  0.2610\n",
      "      8        0.6812  0.2605\n",
      "      9        0.5961  0.2575\n",
      "     10        0.6417  0.2560\n",
      "     11        0.6239  0.2555\n",
      "     12        0.5044  0.2575\n",
      "     13        0.4437  0.2545\n",
      "     14        0.3882  0.2580\n",
      "     15        0.4138  0.2555\n",
      "     16        0.4497  0.2485\n",
      "     17        0.3684  0.2555\n",
      "     18        0.3644  0.2500\n",
      "     19        0.3593  0.2635\n",
      "     20        0.3263  0.2620\n",
      "     21        0.2848  0.2570\n",
      "     22        0.2652  0.2625\n",
      "     23        0.2652  0.2565\n",
      "     24        0.2292  0.2760\n",
      "     25        0.2557  0.2630\n",
      "     26        0.3648  0.2445\n",
      "     27        0.3662  0.2530\n",
      "     28        0.2420  0.2550\n",
      "     29        0.2389  0.2525\n",
      "     30        0.3212  0.2585\n",
      "==========Fold:91==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2562  0.4966\n",
      "      2        0.1690  0.4856\n",
      "      3        0.1564  0.4951\n",
      "      4        0.1456  0.4991\n",
      "      5        0.1337  0.4866\n",
      "      6        0.1330  0.4856\n",
      "      7        0.1245  0.4896\n",
      "      8        0.1199  0.4781\n",
      "      9        0.1128  0.5021\n",
      "     10        0.1095  0.4776\n",
      "     11        0.1048  0.4816\n",
      "     12        0.1057  0.4881\n",
      "     13        0.1011  0.4756\n",
      "     14        0.0984  0.4991\n",
      "     15        0.0987  0.4851\n",
      "     16        0.0915  0.4956\n",
      "     17        0.0918  0.5135\n",
      "     18        0.0893  0.4896\n",
      "     19        0.0865  0.4836\n",
      "     20        0.0882  0.4946\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0621  0.2460\n",
      "      2        1.7067  0.2825\n",
      "      3        1.3674  0.2655\n",
      "      4        1.0696  0.2600\n",
      "      5        0.9032  0.2620\n",
      "      6        0.8225  0.2545\n",
      "      7        0.7754  0.2640\n",
      "      8        0.6245  0.2585\n",
      "      9        0.6030  0.2570\n",
      "     10        0.6024  0.2645\n",
      "     11        0.5585  0.2530\n",
      "     12        0.4868  0.2600\n",
      "     13        0.4384  0.2455\n",
      "     14        0.4392  0.2545\n",
      "     15        0.4401  0.2565\n",
      "     16        0.4334  0.2590\n",
      "     17        0.3918  0.2630\n",
      "     18        0.3924  0.2550\n",
      "     19        0.2988  0.2510\n",
      "     20        0.3134  0.2680\n",
      "     21        0.2814  0.2570\n",
      "     22        0.3088  0.2595\n",
      "     23        0.2560  0.2700\n",
      "     24        0.2646  0.2660\n",
      "     25        0.2731  0.2505\n",
      "     26        0.4256  0.2575\n",
      "     27        0.3387  0.2650\n",
      "     28        0.2457  0.2585\n",
      "     29        0.2242  0.2500\n",
      "     30        0.2034  0.2840\n",
      "==========Fold:92==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2258  0.5006\n",
      "      2        0.1695  0.4956\n",
      "      3        0.1533  0.4776\n",
      "      4        0.1451  0.4776\n",
      "      5        0.1352  0.5021\n",
      "      6        0.1283  0.4921\n",
      "      7        0.1183  0.5021\n",
      "      8        0.1177  0.5260\n",
      "      9        0.1077  0.5165\n",
      "     10        0.1030  0.4686\n",
      "     11        0.1006  0.4686\n",
      "     12        0.0966  0.4811\n",
      "     13        0.0903  0.4956\n",
      "     14        0.0925  0.4771\n",
      "     15        0.0892  0.4821\n",
      "     16        0.0853  0.4826\n",
      "     17        0.0823  0.5315\n",
      "     18        0.0791  0.5195\n",
      "     19        0.0764  0.4901\n",
      "     20        0.0759  0.4961\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0463  0.2565\n",
      "      2        1.6243  0.2625\n",
      "      3        1.2464  0.2565\n",
      "      4        0.9947  0.2515\n",
      "      5        0.8426  0.2820\n",
      "      6        0.7860  0.2635\n",
      "      7        0.6922  0.2525\n",
      "      8        0.6545  0.2605\n",
      "      9        0.5668  0.2505\n",
      "     10        0.5559  0.2605\n",
      "     11        0.4562  0.2540\n",
      "     12        0.5030  0.2445\n",
      "     13        0.4253  0.2595\n",
      "     14        0.3854  0.2550\n",
      "     15        0.3589  0.2565\n",
      "     16        0.3835  0.2660\n",
      "     17        0.3215  0.2785\n",
      "     18        0.3083  0.2655\n",
      "     19        0.2840  0.2570\n",
      "     20        0.3424  0.2440\n",
      "     21        0.2666  0.2470\n",
      "     22        0.2477  0.2615\n",
      "     23        0.3687  0.2530\n",
      "     24        0.3511  0.2525\n",
      "     25        0.2451  0.2520\n",
      "     26        0.1997  0.2575\n",
      "     27        0.2024  0.2525\n",
      "     28        0.1977  0.2575\n",
      "     29        0.1685  0.2585\n",
      "     30        0.1668  0.2590\n",
      "==========Fold:93==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2176  0.4956\n",
      "      2        0.1647  0.4896\n",
      "      3        0.1509  0.4831\n",
      "      4        0.1398  0.4806\n",
      "      5        0.1319  0.5016\n",
      "      6        0.1229  0.4931\n",
      "      7        0.1176  0.4876\n",
      "      8        0.1066  0.4881\n",
      "      9        0.1072  0.5021\n",
      "     10        0.1036  0.4956\n",
      "     11        0.0988  0.5205\n",
      "     12        0.0929  0.4956\n",
      "     13        0.0917  0.4911\n",
      "     14        0.0849  0.5001\n",
      "     15        0.0832  0.4851\n",
      "     16        0.0837  0.4861\n",
      "     17        0.0793  0.4876\n",
      "     18        0.0750  0.5061\n",
      "     19        0.0756  0.5016\n",
      "     20        0.0753  0.4981\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0827  0.2540\n",
      "      2        1.7070  0.2620\n",
      "      3        1.3131  0.2540\n",
      "      4        1.1101  0.2510\n",
      "      5        0.9305  0.2545\n",
      "      6        0.8300  0.2475\n",
      "      7        0.7636  0.2545\n",
      "      8        0.7021  0.2605\n",
      "      9        0.6383  0.2535\n",
      "     10        0.5686  0.2610\n",
      "     11        0.5303  0.2540\n",
      "     12        0.6055  0.2570\n",
      "     13        0.4627  0.2535\n",
      "     14        0.4527  0.2575\n",
      "     15        0.3878  0.2775\n",
      "     16        0.4065  0.2620\n",
      "     17        0.3711  0.2595\n",
      "     18        0.3597  0.2620\n",
      "     19        0.3650  0.2555\n",
      "     20        0.4234  0.2580\n",
      "     21        0.4175  0.2535\n",
      "     22        0.3138  0.2575\n",
      "     23        0.3147  0.2495\n",
      "     24        0.2929  0.2545\n",
      "     25        0.2493  0.2590\n",
      "     26        0.2358  0.2595\n",
      "     27        0.3668  0.2640\n",
      "     28        0.3497  0.2610\n",
      "     29        0.3075  0.2595\n",
      "     30        0.2526  0.2600\n",
      "==========Fold:94==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2265  0.5021\n",
      "      2        0.1631  0.4976\n",
      "      3        0.1510  0.4876\n",
      "      4        0.1424  0.4741\n",
      "      5        0.1330  0.4926\n",
      "      6        0.1262  0.5016\n",
      "      7        0.1229  0.4811\n",
      "      8        0.1175  0.4996\n",
      "      9        0.1110  0.4876\n",
      "     10        0.1108  0.5061\n",
      "     11        0.1036  0.5106\n",
      "     12        0.1058  0.4956\n",
      "     13        0.1044  0.4726\n",
      "     14        0.0947  0.4911\n",
      "     15        0.0986  0.4851\n",
      "     16        0.0964  0.5001\n",
      "     17        0.0920  0.4901\n",
      "     18        0.0917  0.5011\n",
      "     19        0.0888  0.4921\n",
      "     20        0.0917  0.4821\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0820  0.2590\n",
      "      2        1.7147  0.2625\n",
      "      3        1.3457  0.2550\n",
      "      4        1.0602  0.2565\n",
      "      5        0.9046  0.2580\n",
      "      6        0.7693  0.2590\n",
      "      7        0.6796  0.2555\n",
      "      8        0.6429  0.2515\n",
      "      9        0.5580  0.2580\n",
      "     10        0.5232  0.2590\n",
      "     11        0.5784  0.2555\n",
      "     12        0.5227  0.2600\n",
      "     13        0.4020  0.2635\n",
      "     14        0.4125  0.2840\n",
      "     15        0.3939  0.2635\n",
      "     16        0.3956  0.2550\n",
      "     17        0.4055  0.2580\n",
      "     18        0.3416  0.2620\n",
      "     19        0.2712  0.2645\n",
      "     20        0.3053  0.2545\n",
      "     21        0.3128  0.2535\n",
      "     22        0.3272  0.2595\n",
      "     23        0.3041  0.2625\n",
      "     24        0.2627  0.2575\n",
      "     25        0.2689  0.2625\n",
      "     26        0.2654  0.2525\n",
      "     27        0.2212  0.2615\n",
      "     28        0.2357  0.2590\n",
      "     29        0.3084  0.2615\n",
      "     30        0.2661  0.2525\n",
      "==========Fold:95==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2022  0.4851\n",
      "      2        0.1662  0.5026\n",
      "      3        0.1551  0.5086\n",
      "      4        0.1430  0.4856\n",
      "      5        0.1405  0.4866\n",
      "      6        0.1300  0.4941\n",
      "      7        0.1236  0.4816\n",
      "      8        0.1133  0.4891\n",
      "      9        0.1117  0.4611\n",
      "     10        0.1016  0.5210\n",
      "     11        0.1060  0.5310\n",
      "     12        0.1012  0.5345\n",
      "     13        0.1007  0.5051\n",
      "     14        0.0881  0.5001\n",
      "     15        0.0910  0.5125\n",
      "     16        0.0881  0.5061\n",
      "     17        0.0906  0.5130\n",
      "     18        0.0857  0.4946\n",
      "     19        0.0875  0.5120\n",
      "     20        0.0773  0.5235\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0758  0.2500\n",
      "      2        1.6855  0.2615\n",
      "      3        1.2764  0.2580\n",
      "      4        1.0848  0.2600\n",
      "      5        0.8690  0.2540\n",
      "      6        0.8020  0.2625\n",
      "      7        0.6970  0.2610\n",
      "      8        0.5831  0.2620\n",
      "      9        0.5614  0.2595\n",
      "     10        0.5780  0.2575\n",
      "     11        0.4265  0.2555\n",
      "     12        0.4012  0.2470\n",
      "     13        0.4031  0.2855\n",
      "     14        0.3412  0.2610\n",
      "     15        0.4014  0.2645\n",
      "     16        0.3367  0.2490\n",
      "     17        0.3226  0.2615\n",
      "     18        0.2836  0.2575\n",
      "     19        0.2647  0.2525\n",
      "     20        0.2760  0.2585\n",
      "     21        0.2563  0.2555\n",
      "     22        0.3124  0.2635\n",
      "     23        0.2989  0.2525\n",
      "     24        0.2494  0.2605\n",
      "     25        0.2379  0.2615\n",
      "     26        0.1806  0.2610\n",
      "     27        0.1781  0.2530\n",
      "     28        0.1587  0.2530\n",
      "     29        0.1746  0.2640\n",
      "     30        0.1778  0.2615\n",
      "==========Fold:96==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2312  0.5026\n",
      "      2        0.1641  0.4881\n",
      "      3        0.1504  0.4971\n",
      "      4        0.1398  0.4986\n",
      "      5        0.1314  0.5006\n",
      "      6        0.1265  0.5145\n",
      "      7        0.1251  0.5155\n",
      "      8        0.1129  0.4911\n",
      "      9        0.1137  0.4756\n",
      "     10        0.1056  0.4941\n",
      "     11        0.1038  0.5031\n",
      "     12        0.1028  0.4986\n",
      "     13        0.0928  0.4881\n",
      "     14        0.0977  0.4816\n",
      "     15        0.0969  0.4926\n",
      "     16        0.0911  0.4971\n",
      "     17        0.0879  0.4916\n",
      "     18        0.0865  0.4771\n",
      "     19        0.0852  0.4771\n",
      "     20        0.0819  0.4756\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0978  0.2505\n",
      "      2        1.7426  0.2535\n",
      "      3        1.3620  0.2580\n",
      "      4        1.1289  0.2620\n",
      "      5        0.9682  0.2575\n",
      "      6        0.8136  0.2510\n",
      "      7        0.7140  0.2720\n",
      "      8        0.6743  0.2670\n",
      "      9        0.6131  0.2595\n",
      "     10        0.5486  0.2600\n",
      "     11        0.4972  0.2560\n",
      "     12        0.5046  0.2590\n",
      "     13        0.4819  0.2585\n",
      "     14        0.4874  0.2625\n",
      "     15        0.4163  0.2605\n",
      "     16        0.3778  0.2605\n",
      "     17        0.3381  0.2640\n",
      "     18        0.4737  0.2570\n",
      "     19        0.4300  0.2535\n",
      "     20        0.3322  0.2530\n",
      "     21        0.3308  0.2555\n",
      "     22        0.2505  0.2605\n",
      "     23        0.2626  0.2530\n",
      "     24        0.2798  0.2620\n",
      "     25        0.2456  0.2555\n",
      "     26        0.2135  0.2585\n",
      "     27        0.2582  0.2625\n",
      "     28        0.3183  0.2575\n",
      "     29        0.2456  0.2590\n",
      "     30        0.2748  0.2610\n",
      "==========Fold:97==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2548  0.5066\n",
      "      2        0.1717  0.5130\n",
      "      3        0.1585  0.4871\n",
      "      4        0.1499  0.4991\n",
      "      5        0.1412  0.4941\n",
      "      6        0.1349  0.4851\n",
      "      7        0.1351  0.5021\n",
      "      8        0.1253  0.4771\n",
      "      9        0.1187  0.4816\n",
      "     10        0.1163  0.4916\n",
      "     11        0.1090  0.4791\n",
      "     12        0.1068  0.4741\n",
      "     13        0.1062  0.4951\n",
      "     14        0.1013  0.5101\n",
      "     15        0.0968  0.4811\n",
      "     16        0.0982  0.4891\n",
      "     17        0.0924  0.4891\n",
      "     18        0.0966  0.4846\n",
      "     19        0.0891  0.4796\n",
      "     20        0.0913  0.4906\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1081  0.2580\n",
      "      2        1.7361  0.2635\n",
      "      3        1.3335  0.2780\n",
      "      4        1.1254  0.2645\n",
      "      5        0.9506  0.2620\n",
      "      6        0.8468  0.2540\n",
      "      7        0.7966  0.2610\n",
      "      8        0.6598  0.2575\n",
      "      9        0.6761  0.2615\n",
      "     10        0.7422  0.2535\n",
      "     11        0.5303  0.2535\n",
      "     12        0.4599  0.2615\n",
      "     13        0.4488  0.2500\n",
      "     14        0.3930  0.2695\n",
      "     15        0.3869  0.2610\n",
      "     16        0.4107  0.2560\n",
      "     17        0.3415  0.2530\n",
      "     18        0.3758  0.2620\n",
      "     19        0.4412  0.2570\n",
      "     20        0.3431  0.2570\n",
      "     21        0.3025  0.2620\n",
      "     22        0.3062  0.2550\n",
      "     23        0.2540  0.2635\n",
      "     24        0.2234  0.2535\n",
      "     25        0.2267  0.2595\n",
      "     26        0.2522  0.2790\n",
      "     27        0.4480  0.2590\n",
      "     28        0.3907  0.2605\n",
      "     29        0.2642  0.2560\n",
      "     30        0.2072  0.2560\n",
      "==========Fold:98==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2398  0.4996\n",
      "      2        0.1701  0.4986\n",
      "      3        0.1570  0.5036\n",
      "      4        0.1445  0.4816\n",
      "      5        0.1365  0.4916\n",
      "      6        0.1289  0.5051\n",
      "      7        0.1225  0.5515\n",
      "      8        0.1161  0.5530\n",
      "      9        0.1138  0.4896\n",
      "     10        0.1066  0.5295\n",
      "     11        0.1034  0.4846\n",
      "     12        0.0995  0.4966\n",
      "     13        0.0953  0.5086\n",
      "     14        0.0946  0.5170\n",
      "     15        0.0869  0.4921\n",
      "     16        0.0879  0.5165\n",
      "     17        0.0953  0.4896\n",
      "     18        0.0904  0.5265\n",
      "     19        0.0809  0.5101\n",
      "     20        0.0800  0.5165\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0787  0.2560\n",
      "      2        1.7821  0.2545\n",
      "      3        1.4246  0.2605\n",
      "      4        1.1200  0.2560\n",
      "      5        0.9666  0.2570\n",
      "      6        0.9110  0.2525\n",
      "      7        0.8355  0.2605\n",
      "      8        0.6866  0.2600\n",
      "      9        0.7001  0.2630\n",
      "     10        0.6457  0.2585\n",
      "     11        0.5757  0.2520\n",
      "     12        0.5226  0.2655\n",
      "     13        0.5144  0.2530\n",
      "     14        0.4874  0.2605\n",
      "     15        0.4071  0.2595\n",
      "     16        0.3939  0.2565\n",
      "     17        0.3556  0.2795\n",
      "     18        0.3696  0.2635\n",
      "     19        0.4870  0.2565\n",
      "     20        0.3597  0.2580\n",
      "     21        0.3442  0.2600\n",
      "     22        0.3171  0.2540\n",
      "     23        0.2813  0.2495\n",
      "     24        0.3573  0.2510\n",
      "     25        0.4157  0.2635\n",
      "     26        0.3108  0.2594\n",
      "     27        0.2391  0.2545\n",
      "     28        0.2367  0.2510\n",
      "     29        0.2147  0.2610\n",
      "     30        0.2078  0.2570\n",
      "==========Fold:99==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2268  0.4966\n",
      "      2        0.1634  0.4861\n",
      "      3        0.1502  0.4851\n",
      "      4        0.1385  0.4956\n",
      "      5        0.1310  0.4941\n",
      "      6        0.1216  0.5081\n",
      "      7        0.1204  0.4916\n",
      "      8        0.1098  0.5081\n",
      "      9        0.1094  0.5115\n",
      "     10        0.1081  0.4916\n",
      "     11        0.0954  0.4976\n",
      "     12        0.0957  0.4891\n",
      "     13        0.0919  0.4901\n",
      "     14        0.0886  0.4841\n",
      "     15        0.0865  0.4966\n",
      "     16        0.0812  0.4891\n",
      "     17        0.0775  0.4836\n",
      "     18        0.0771  0.4851\n",
      "     19        0.0775  0.5046\n",
      "     20        0.0745  0.4871\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0932  0.2540\n",
      "      2        1.6978  0.2615\n",
      "      3        1.3065  0.2515\n",
      "      4        1.1204  0.2635\n",
      "      5        0.9177  0.2565\n",
      "      6        0.8071  0.2575\n",
      "      7        0.7683  0.2630\n",
      "      8        0.7159  0.2830\n",
      "      9        0.6070  0.2675\n",
      "     10        0.5577  0.2480\n",
      "     11        0.4847  0.2625\n",
      "     12        0.4631  0.2620\n",
      "     13        0.4444  0.2575\n",
      "     14        0.4681  0.2655\n",
      "     15        0.4193  0.2590\n",
      "     16        0.4084  0.2645\n",
      "     17        0.3939  0.2705\n",
      "     18        0.3280  0.2555\n",
      "     19        0.3357  0.2590\n",
      "     20        0.3703  0.2880\n",
      "     21        0.3164  0.2820\n",
      "     22        0.3499  0.2755\n",
      "     23        0.2857  0.2675\n",
      "     24        0.2628  0.2575\n",
      "     25        0.2719  0.2660\n",
      "     26        0.3043  0.2860\n",
      "     27        0.2976  0.2650\n",
      "     28        0.2989  0.2485\n",
      "     29        0.2816  0.2545\n",
      "     30        0.2840  0.2580\n",
      "==========Fold:100==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2386  0.4996\n",
      "      2        0.1680  0.5190\n",
      "      3        0.1563  0.5150\n",
      "      4        0.1450  0.5066\n",
      "      5        0.1387  0.5041\n",
      "      6        0.1330  0.4901\n",
      "      7        0.1295  0.4986\n",
      "      8        0.1200  0.4906\n",
      "      9        0.1184  0.4931\n",
      "     10        0.1148  0.4876\n",
      "     11        0.1112  0.4756\n",
      "     12        0.1069  0.4861\n",
      "     13        0.1053  0.4936\n",
      "     14        0.1001  0.4966\n",
      "     15        0.0978  0.4911\n",
      "     16        0.0922  0.4981\n",
      "     17        0.0968  0.4786\n",
      "     18        0.0929  0.5006\n",
      "     19        0.0903  0.4916\n",
      "     20        0.0901  0.4956\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0649  0.2630\n",
      "      2        1.6713  0.2825\n",
      "      3        1.3159  0.2585\n",
      "      4        1.0995  0.2655\n",
      "      5        0.9445  0.2615\n",
      "      6        0.9087  0.2585\n",
      "      7        0.7370  0.2605\n",
      "      8        0.7205  0.2560\n",
      "      9        0.7129  0.2580\n",
      "     10        0.6058  0.2525\n",
      "     11        0.5681  0.2610\n",
      "     12        0.6108  0.2515\n",
      "     13        0.5465  0.2615\n",
      "     14        0.4554  0.2565\n",
      "     15        0.4012  0.2495\n",
      "     16        0.4476  0.2590\n",
      "     17        0.3881  0.2590\n",
      "     18        0.4020  0.2595\n",
      "     19        0.3974  0.2580\n",
      "     20        0.3168  0.2575\n",
      "     21        0.3117  0.2635\n",
      "     22        0.3085  0.2580\n",
      "     23        0.3739  0.2795\n",
      "     24        0.2922  0.2595\n",
      "     25        0.2346  0.2665\n",
      "     26        0.2902  0.2515\n",
      "     27        0.3001  0.2520\n",
      "     28        0.2906  0.2515\n",
      "     29        0.2423  0.2500\n",
      "     30        0.2045  0.2560\n",
      "==========Fold:101==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2346  0.5041\n",
      "      2        0.1685  0.5135\n",
      "      3        0.1566  0.4921\n",
      "      4        0.1471  0.4931\n",
      "      5        0.1390  0.4886\n",
      "      6        0.1339  0.4876\n",
      "      7        0.1281  0.4946\n",
      "      8        0.1218  0.4966\n",
      "      9        0.1167  0.4911\n",
      "     10        0.1146  0.4796\n",
      "     11        0.1088  0.4836\n",
      "     12        0.1019  0.4821\n",
      "     13        0.0992  0.4876\n",
      "     14        0.1042  0.4966\n",
      "     15        0.0964  0.4831\n",
      "     16        0.0949  0.4841\n",
      "     17        0.0886  0.4941\n",
      "     18        0.0854  0.5051\n",
      "     19        0.0826  0.4926\n",
      "     20        0.0855  0.5046\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0631  0.2520\n",
      "      2        1.7138  0.2595\n",
      "      3        1.3770  0.2580\n",
      "      4        1.0625  0.2645\n",
      "      5        0.9396  0.2610\n",
      "      6        0.7950  0.2675\n",
      "      7        0.7202  0.2610\n",
      "      8        0.7146  0.2605\n",
      "      9        0.6057  0.2570\n",
      "     10        0.5412  0.2540\n",
      "     11        0.5221  0.2525\n",
      "     12        0.5150  0.2605\n",
      "     13        0.4706  0.2580\n",
      "     14        0.4170  0.2580\n",
      "     15        0.4067  0.2600\n",
      "     16        0.4484  0.2825\n",
      "     17        0.5085  0.2630\n",
      "     18        0.3756  0.2610\n",
      "     19        0.3447  0.2555\n",
      "     20        0.3249  0.2495\n",
      "     21        0.3350  0.2605\n",
      "     22        0.2863  0.2610\n",
      "     23        0.3582  0.2625\n",
      "     24        0.3566  0.2770\n",
      "     25        0.3319  0.2745\n",
      "     26        0.2585  0.2610\n",
      "     27        0.2504  0.2675\n",
      "     28        0.3029  0.2635\n",
      "     29        0.2327  0.2600\n",
      "     30        0.3052  0.2570\n",
      "==========Fold:102==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2668  0.5235\n",
      "      2        0.1665  0.4891\n",
      "      3        0.1545  0.4891\n",
      "      4        0.1464  0.4981\n",
      "      5        0.1372  0.4891\n",
      "      6        0.1295  0.4991\n",
      "      7        0.1258  0.4941\n",
      "      8        0.1201  0.4936\n",
      "      9        0.1151  0.5360\n",
      "     10        0.1163  0.5295\n",
      "     11        0.1040  0.4831\n",
      "     12        0.1032  0.5016\n",
      "     13        0.0987  0.4816\n",
      "     14        0.1050  0.4926\n",
      "     15        0.0891  0.4896\n",
      "     16        0.0957  0.4781\n",
      "     17        0.0880  0.5016\n",
      "     18        0.0894  0.4996\n",
      "     19        0.0892  0.4656\n",
      "     20        0.0862  0.5235\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0412  0.2600\n",
      "      2        1.6624  0.2575\n",
      "      3        1.3326  0.2575\n",
      "      4        1.0948  0.2575\n",
      "      5        0.9239  0.2620\n",
      "      6        0.8373  0.2505\n",
      "      7        0.7561  0.2600\n",
      "      8        0.6806  0.2820\n",
      "      9        0.6750  0.2655\n",
      "     10        0.5289  0.2525\n",
      "     11        0.5436  0.2530\n",
      "     12        0.5372  0.2620\n",
      "     13        0.4582  0.2570\n",
      "     14        0.4281  0.2585\n",
      "     15        0.3841  0.2625\n",
      "     16        0.3878  0.2605\n",
      "     17        0.3823  0.2560\n",
      "     18        0.3901  0.2590\n",
      "     19        0.4059  0.2590\n",
      "     20        0.2938  0.2530\n",
      "     21        0.2868  0.2570\n",
      "     22        0.2964  0.2580\n",
      "     23        0.2743  0.2625\n",
      "     24        0.2571  0.2585\n",
      "     25        0.2330  0.2620\n",
      "     26        0.2715  0.2640\n",
      "     27        0.3498  0.2625\n",
      "     28        0.2858  0.2535\n",
      "     29        0.1998  0.2590\n",
      "     30        0.2025  0.2550\n",
      "==========Fold:103==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2580  0.5021\n",
      "      2        0.1716  0.5225\n",
      "      3        0.1561  0.4971\n",
      "      4        0.1474  0.4816\n",
      "      5        0.1386  0.5006\n",
      "      6        0.1309  0.4756\n",
      "      7        0.1272  0.5041\n",
      "      8        0.1271  0.4861\n",
      "      9        0.1224  0.4981\n",
      "     10        0.1160  0.4966\n",
      "     11        0.1099  0.5101\n",
      "     12        0.1095  0.5031\n",
      "     13        0.0994  0.5066\n",
      "     14        0.1048  0.5096\n",
      "     15        0.1012  0.4996\n",
      "     16        0.1031  0.4811\n",
      "     17        0.0929  0.5036\n",
      "     18        0.0981  0.4856\n",
      "     19        0.0917  0.4941\n",
      "     20        0.0887  0.5006\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0751  0.2630\n",
      "      2        1.6735  0.2755\n",
      "      3        1.3246  0.2555\n",
      "      4        1.1471  0.2650\n",
      "      5        0.9281  0.2590\n",
      "      6        0.7926  0.2555\n",
      "      7        0.7602  0.2605\n",
      "      8        0.6623  0.2650\n",
      "      9        0.5790  0.2580\n",
      "     10        0.5423  0.2565\n",
      "     11        0.5402  0.2590\n",
      "     12        0.4843  0.2580\n",
      "     13        0.4622  0.2600\n",
      "     14        0.4443  0.2625\n",
      "     15        0.4381  0.2550\n",
      "     16        0.3686  0.2580\n",
      "     17        0.4095  0.2580\n",
      "     18        0.4117  0.2635\n",
      "     19        0.3288  0.2615\n",
      "     20        0.2888  0.2580\n",
      "     21        0.2731  0.2615\n",
      "     22        0.3588  0.2790\n",
      "     23        0.3427  0.2620\n",
      "     24        0.2609  0.2605\n",
      "     25        0.2714  0.2570\n",
      "     26        0.2670  0.2570\n",
      "     27        0.2293  0.2545\n",
      "     28        0.2451  0.2575\n",
      "     29        0.2114  0.2545\n",
      "     30        0.3002  0.2560\n",
      "==========Fold:104==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2414  0.5021\n",
      "      2        0.1660  0.4866\n",
      "      3        0.1543  0.4691\n",
      "      4        0.1455  0.4931\n",
      "      5        0.1358  0.4876\n",
      "      6        0.1291  0.5056\n",
      "      7        0.1251  0.5031\n",
      "      8        0.1164  0.4826\n",
      "      9        0.1108  0.4781\n",
      "     10        0.1062  0.4931\n",
      "     11        0.1036  0.4876\n",
      "     12        0.1001  0.4901\n",
      "     13        0.0906  0.4736\n",
      "     14        0.0916  0.4886\n",
      "     15        0.0863  0.4881\n",
      "     16        0.0867  0.4931\n",
      "     17        0.0807  0.5006\n",
      "     18        0.0809  0.5165\n",
      "     19        0.0778  0.4976\n",
      "     20        0.0740  0.4951\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0264  0.2540\n",
      "      2        1.6514  0.2600\n",
      "      3        1.3476  0.2610\n",
      "      4        1.0978  0.2605\n",
      "      5        0.9921  0.2575\n",
      "      6        0.8447  0.2590\n",
      "      7        0.8427  0.2560\n",
      "      8        0.7383  0.2630\n",
      "      9        0.7086  0.2615\n",
      "     10        0.6038  0.2585\n",
      "     11        0.5449  0.2540\n",
      "     12        0.5627  0.2610\n",
      "     13        0.5293  0.2620\n",
      "     14        0.5306  0.2615\n",
      "     15        0.4408  0.2555\n",
      "     16        0.3938  0.2585\n",
      "     17        0.4011  0.2635\n",
      "     18        0.4503  0.2620\n",
      "     19        0.4358  0.2610\n",
      "     20        0.4379  0.2565\n",
      "     21        0.3769  0.2570\n",
      "     22        0.3026  0.2590\n",
      "     23        0.2905  0.2590\n",
      "     24        0.2940  0.2785\n",
      "     25        0.2905  0.2630\n",
      "     26        0.4032  0.2630\n",
      "     27        0.3087  0.2610\n",
      "     28        0.2805  0.2575\n",
      "     29        0.2620  0.2625\n",
      "     30        0.2568  0.2615\n",
      "==========Fold:105==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2167  0.5056\n",
      "      2        0.1655  0.4966\n",
      "      3        0.1524  0.4956\n",
      "      4        0.1455  0.4856\n",
      "      5        0.1379  0.4866\n",
      "      6        0.1318  0.4996\n",
      "      7        0.1230  0.5046\n",
      "      8        0.1188  0.4906\n",
      "      9        0.1159  0.4956\n",
      "     10        0.1107  0.4991\n",
      "     11        0.1047  0.5001\n",
      "     12        0.1009  0.4781\n",
      "     13        0.0984  0.4731\n",
      "     14        0.0955  0.4916\n",
      "     15        0.0904  0.4921\n",
      "     16        0.0924  0.5026\n",
      "     17        0.0844  0.4946\n",
      "     18        0.0898  0.5205\n",
      "     19        0.0800  0.5125\n",
      "     20        0.0762  0.5061\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0148  0.2630\n",
      "      2        1.5695  0.2575\n",
      "      3        1.2608  0.2660\n",
      "      4        1.0415  0.2575\n",
      "      5        0.9220  0.2590\n",
      "      6        0.8104  0.2585\n",
      "      7        0.7172  0.2565\n",
      "      8        0.6236  0.2570\n",
      "      9        0.5774  0.2585\n",
      "     10        0.5494  0.2635\n",
      "     11        0.6069  0.2655\n",
      "     12        0.4633  0.2565\n",
      "     13        0.3978  0.2590\n",
      "     14        0.3902  0.2590\n",
      "     15        0.4253  0.2565\n",
      "     16        0.3713  0.2590\n",
      "     17        0.3134  0.2600\n",
      "     18        0.3420  0.2815\n",
      "     19        0.2831  0.2610\n",
      "     20        0.2808  0.2660\n",
      "     21        0.3622  0.2610\n",
      "     22        0.3635  0.2655\n",
      "     23        0.3227  0.2610\n",
      "     24        0.2950  0.2475\n",
      "     25        0.2584  0.2615\n",
      "     26        0.2047  0.2550\n",
      "     27        0.2189  0.2515\n",
      "     28        0.1971  0.2560\n",
      "     29        0.1875  0.2610\n",
      "     30        0.2361  0.2615\n",
      "==========Fold:106==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2391  0.5111\n",
      "      2        0.1698  0.4996\n",
      "      3        0.1577  0.4976\n",
      "      4        0.1492  0.4901\n",
      "      5        0.1385  0.4866\n",
      "      6        0.1332  0.4941\n",
      "      7        0.1270  0.4756\n",
      "      8        0.1189  0.4906\n",
      "      9        0.1130  0.4931\n",
      "     10        0.1187  0.5026\n",
      "     11        0.1056  0.5096\n",
      "     12        0.1057  0.4901\n",
      "     13        0.0976  0.4766\n",
      "     14        0.1064  0.5051\n",
      "     15        0.0968  0.5180\n",
      "     16        0.0947  0.5076\n",
      "     17        0.0943  0.4906\n",
      "     18        0.0932  0.4891\n",
      "     19        0.0881  0.5106\n",
      "     20        0.0836  0.4971\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1051  0.2610\n",
      "      2        1.7489  0.2670\n",
      "      3        1.3553  0.2585\n",
      "      4        1.0671  0.2580\n",
      "      5        0.9140  0.2595\n",
      "      6        0.7767  0.2615\n",
      "      7        0.7212  0.2605\n",
      "      8        0.7514  0.2595\n",
      "      9        0.6289  0.2595\n",
      "     10        0.5359  0.2620\n",
      "     11        0.4813  0.2610\n",
      "     12        0.4816  0.2555\n",
      "     13        0.4908  0.2600\n",
      "     14        0.4423  0.2605\n",
      "     15        0.3633  0.2780\n",
      "     16        0.3621  0.2655\n",
      "     17        0.3445  0.2575\n",
      "     18        0.3281  0.2525\n",
      "     19        0.3133  0.2530\n",
      "     20        0.2993  0.2625\n",
      "     21        0.3031  0.2545\n",
      "     22        0.3232  0.2580\n",
      "     23        0.2989  0.2620\n",
      "     24        0.2435  0.2630\n",
      "     25        0.2564  0.2625\n",
      "     26        0.2616  0.2545\n",
      "     27        0.2787  0.2600\n",
      "     28        0.2770  0.2570\n",
      "     29        0.2899  0.2625\n",
      "     30        0.2692  0.2630\n",
      "==========Fold:107==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2771  0.5111\n",
      "      2        0.1702  0.4766\n",
      "      3        0.1584  0.4931\n",
      "      4        0.1479  0.4906\n",
      "      5        0.1407  0.5096\n",
      "      6        0.1325  0.5006\n",
      "      7        0.1261  0.4951\n",
      "      8        0.1260  0.4996\n",
      "      9        0.1172  0.5076\n",
      "     10        0.1110  0.4731\n",
      "     11        0.1089  0.4946\n",
      "     12        0.1063  0.4901\n",
      "     13        0.1070  0.5056\n",
      "     14        0.1027  0.4921\n",
      "     15        0.0997  0.4791\n",
      "     16        0.0947  0.4701\n",
      "     17        0.0950  0.4981\n",
      "     18        0.0843  0.5041\n",
      "     19        0.0833  0.4771\n",
      "     20        0.0891  0.4836\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0378  0.2525\n",
      "      2        1.5662  0.2685\n",
      "      3        1.2719  0.2785\n",
      "      4        1.0873  0.2570\n",
      "      5        0.8867  0.2670\n",
      "      6        0.8252  0.2565\n",
      "      7        0.7303  0.2570\n",
      "      8        0.6849  0.2625\n",
      "      9        0.6384  0.2605\n",
      "     10        0.6111  0.2595\n",
      "     11        0.4987  0.2630\n",
      "     12        0.4543  0.2625\n",
      "     13        0.4548  0.2655\n",
      "     14        0.4169  0.2585\n",
      "     15        0.3823  0.2570\n",
      "     16        0.3470  0.2615\n",
      "     17        0.4127  0.2630\n",
      "     18        0.3749  0.2580\n",
      "     19        0.2993  0.2640\n",
      "     20        0.2969  0.2595\n",
      "     21        0.3272  0.2600\n",
      "     22        0.3305  0.2625\n",
      "     23        0.2819  0.2560\n",
      "     24        0.2952  0.2560\n",
      "     25        0.2454  0.2550\n",
      "     26        0.2672  0.2595\n",
      "     27        0.2834  0.2545\n",
      "     28        0.2681  0.2825\n",
      "     29        0.2102  0.2635\n",
      "     30        0.1814  0.2505\n",
      "==========Fold:108==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2244  0.5006\n",
      "      2        0.1679  0.4825\n",
      "      3        0.1569  0.5011\n",
      "      4        0.1493  0.4851\n",
      "      5        0.1419  0.4941\n",
      "      6        0.1366  0.5051\n",
      "      7        0.1288  0.4976\n",
      "      8        0.1247  0.4971\n",
      "      9        0.1200  0.5001\n",
      "     10        0.1159  0.4911\n",
      "     11        0.1116  0.4956\n",
      "     12        0.1061  0.4971\n",
      "     13        0.0987  0.4951\n",
      "     14        0.1034  0.4916\n",
      "     15        0.0957  0.4996\n",
      "     16        0.0929  0.4871\n",
      "     17        0.0932  0.4986\n",
      "     18        0.0863  0.4771\n",
      "     19        0.0848  0.5714\n",
      "     20        0.0835  0.5096\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0137  0.2690\n",
      "      2        1.6186  0.2550\n",
      "      3        1.3409  0.2635\n",
      "      4        1.1721  0.2705\n",
      "      5        0.9404  0.2765\n",
      "      6        0.8722  0.2670\n",
      "      7        0.7923  0.2620\n",
      "      8        0.6863  0.2590\n",
      "      9        0.6220  0.2605\n",
      "     10        0.6754  0.2615\n",
      "     11        0.6221  0.2585\n",
      "     12        0.5281  0.2650\n",
      "     13        0.4961  0.2610\n",
      "     14        0.4587  0.2555\n",
      "     15        0.4028  0.2610\n",
      "     16        0.3706  0.2750\n",
      "     17        0.4780  0.2635\n",
      "     18        0.4144  0.2635\n",
      "     19        0.3457  0.2560\n",
      "     20        0.3203  0.2610\n",
      "     21        0.3341  0.2505\n",
      "     22        0.2970  0.2585\n",
      "     23        0.3140  0.2550\n",
      "     24        0.3693  0.2640\n",
      "     25        0.4267  0.2595\n",
      "     26        0.2838  0.2610\n",
      "     27        0.2708  0.2610\n",
      "     28        0.2677  0.2615\n",
      "     29        0.2454  0.2625\n",
      "     30        0.2318  0.2630\n",
      "==========Fold:109==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2714  0.4976\n",
      "      2        0.1681  0.5011\n",
      "      3        0.1544  0.4836\n",
      "      4        0.1454  0.5086\n",
      "      5        0.1379  0.4966\n",
      "      6        0.1303  0.4901\n",
      "      7        0.1241  0.4906\n",
      "      8        0.1176  0.4906\n",
      "      9        0.1135  0.5061\n",
      "     10        0.1089  0.4961\n",
      "     11        0.1065  0.4821\n",
      "     12        0.1001  0.4836\n",
      "     13        0.0977  0.5016\n",
      "     14        0.0931  0.4881\n",
      "     15        0.0915  0.4851\n",
      "     16        0.0894  0.5011\n",
      "     17        0.0862  0.5125\n",
      "     18        0.0859  0.4916\n",
      "     19        0.0787  0.4926\n",
      "     20        0.0785  0.4946\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0857  0.2565\n",
      "      2        1.7473  0.2610\n",
      "      3        1.3591  0.2560\n",
      "      4        1.1579  0.2545\n",
      "      5        0.9586  0.2765\n",
      "      6        0.8270  0.2585\n",
      "      7        0.7322  0.2655\n",
      "      8        0.6448  0.2655\n",
      "      9        0.6836  0.2574\n",
      "     10        0.5991  0.2590\n",
      "     11        0.5029  0.2595\n",
      "     12        0.5332  0.2565\n",
      "     13        0.4676  0.2610\n",
      "     14        0.4324  0.2630\n",
      "     15        0.3812  0.2640\n",
      "     16        0.4088  0.2550\n",
      "     17        0.3948  0.2570\n",
      "     18        0.3682  0.2665\n",
      "     19        0.3367  0.2615\n",
      "     20        0.3439  0.2575\n",
      "     21        0.3251  0.2610\n",
      "     22        0.2668  0.2570\n",
      "     23        0.2779  0.2565\n",
      "     24        0.2700  0.2595\n",
      "     25        0.3009  0.2870\n",
      "     26        0.3677  0.2585\n",
      "     27        0.2671  0.2630\n",
      "     28        0.2324  0.2665\n",
      "     29        0.2489  0.2580\n",
      "     30        0.2676  0.2640\n",
      "==========Fold:110==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2137  0.5041\n",
      "      2        0.1688  0.4896\n",
      "      3        0.1560  0.4771\n",
      "      4        0.1463  0.5175\n",
      "      5        0.1377  0.5001\n",
      "      6        0.1305  0.4806\n",
      "      7        0.1216  0.4971\n",
      "      8        0.1189  0.5046\n",
      "      9        0.1129  0.4800\n",
      "     10        0.1060  0.5016\n",
      "     11        0.1069  0.4876\n",
      "     12        0.1009  0.5056\n",
      "     13        0.0944  0.4886\n",
      "     14        0.0915  0.5016\n",
      "     15        0.0920  0.4886\n",
      "     16        0.0875  0.5041\n",
      "     17        0.0865  0.5011\n",
      "     18        0.0787  0.4991\n",
      "     19        0.0781  0.5260\n",
      "     20        0.0785  0.4916\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        1.9993  0.2535\n",
      "      2        1.6024  0.2660\n",
      "      3        1.2715  0.2615\n",
      "      4        1.0845  0.2620\n",
      "      5        0.9189  0.2625\n",
      "      6        0.8124  0.2615\n",
      "      7        0.7266  0.2560\n",
      "      8        0.6939  0.2640\n",
      "      9        0.6248  0.2565\n",
      "     10        0.5440  0.2600\n",
      "     11        0.5469  0.2620\n",
      "     12        0.5531  0.2615\n",
      "     13        0.4502  0.2600\n",
      "     14        0.4395  0.2625\n",
      "     15        0.4078  0.2640\n",
      "     16        0.4393  0.2815\n",
      "     17        0.4570  0.2600\n",
      "     18        0.3547  0.2600\n",
      "     19        0.3299  0.2610\n",
      "     20        0.3560  0.2690\n",
      "     21        0.3431  0.2625\n",
      "     22        0.3246  0.2580\n",
      "     23        0.3205  0.2540\n",
      "     24        0.2908  0.2595\n",
      "     25        0.2588  0.2575\n",
      "     26        0.3055  0.2590\n",
      "     27        0.3163  0.2640\n",
      "     28        0.2732  0.2610\n",
      "     29        0.2246  0.2585\n",
      "     30        0.2188  0.2645\n",
      "==========Fold:111==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2338  0.4881\n",
      "      2        0.1651  0.4984\n",
      "      3        0.1539  0.5061\n",
      "      4        0.1452  0.4976\n",
      "      5        0.1347  0.5016\n",
      "      6        0.1370  0.4946\n",
      "      7        0.1236  0.4926\n",
      "      8        0.1255  0.4816\n",
      "      9        0.1106  0.4881\n",
      "     10        0.1107  0.5041\n",
      "     11        0.1037  0.5021\n",
      "     12        0.1047  0.4941\n",
      "     13        0.0988  0.5056\n",
      "     14        0.0939  0.4986\n",
      "     15        0.0939  0.5290\n",
      "     16        0.0907  0.5021\n",
      "     17        0.0916  0.5011\n",
      "     18        0.0840  0.4876\n",
      "     19        0.0814  0.4996\n",
      "     20        0.0798  0.4941\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0809  0.2590\n",
      "      2        1.7362  0.2590\n",
      "      3        1.3686  0.2655\n",
      "      4        1.0918  0.2490\n",
      "      5        0.9382  0.2605\n",
      "      6        0.8105  0.2620\n",
      "      7        0.7029  0.2550\n",
      "      8        0.6900  0.2745\n",
      "      9        0.5788  0.2630\n",
      "     10        0.5558  0.2625\n",
      "     11        0.4791  0.2585\n",
      "     12        0.4436  0.2585\n",
      "     13        0.4153  0.2640\n",
      "     14        0.4945  0.2600\n",
      "     15        0.4591  0.2625\n",
      "     16        0.3830  0.2580\n",
      "     17        0.3679  0.2615\n",
      "     18        0.3770  0.2620\n",
      "     19        0.3180  0.2535\n",
      "     20        0.3285  0.2590\n",
      "     21        0.3227  0.2610\n",
      "     22        0.4138  0.2605\n",
      "     23        0.2698  0.2580\n",
      "     24        0.2970  0.2550\n",
      "     25        0.2675  0.2570\n",
      "     26        0.2446  0.2630\n",
      "     27        0.2632  0.2625\n",
      "     28        0.2235  0.2700\n",
      "     29        0.2368  0.2815\n",
      "     30        0.2178  0.2635\n",
      "==========Fold:112==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2330  0.5220\n",
      "      2        0.1679  0.5011\n",
      "      3        0.1532  0.4951\n",
      "      4        0.1432  0.5071\n",
      "      5        0.1367  0.4976\n",
      "      6        0.1284  0.5011\n",
      "      7        0.1238  0.5066\n",
      "      8        0.1157  0.5135\n",
      "      9        0.1142  0.4901\n",
      "     10        0.1103  0.4956\n",
      "     11        0.1058  0.5380\n",
      "     12        0.0996  0.4771\n",
      "     13        0.1010  0.5046\n",
      "     14        0.0974  0.5021\n",
      "     15        0.0897  0.4916\n",
      "     16        0.0975  0.5250\n",
      "     17        0.0887  0.5215\n",
      "     18        0.0891  0.5016\n",
      "     19        0.0818  0.4886\n",
      "     20        0.0959  0.4771\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1319  0.2595\n",
      "      2        1.8260  0.2615\n",
      "      3        1.5236  0.2815\n",
      "      4        1.3093  0.2610\n",
      "      5        1.1188  0.2635\n",
      "      6        1.0272  0.2690\n",
      "      7        0.8626  0.2635\n",
      "      8        0.8205  0.2625\n",
      "      9        0.6722  0.2650\n",
      "     10        0.6420  0.2610\n",
      "     11        0.6347  0.2790\n",
      "     12        0.6268  0.2685\n",
      "     13        0.6079  0.2645\n",
      "     14        0.6119  0.2640\n",
      "     15        0.5132  0.2565\n",
      "     16        0.4156  0.2630\n",
      "     17        0.4175  0.2615\n",
      "     18        0.4448  0.2595\n",
      "     19        0.3579  0.2640\n",
      "     20        0.3488  0.2595\n",
      "     21        0.3366  0.2600\n",
      "     22        0.3272  0.2615\n",
      "     23        0.3713  0.2585\n",
      "     24        0.4362  0.2635\n",
      "     25        0.3144  0.2550\n",
      "     26        0.2488  0.2600\n",
      "     27        0.2489  0.2560\n",
      "     28        0.2533  0.2560\n",
      "     29        0.3024  0.2595\n",
      "     30        0.3054  0.2590\n",
      "==========Fold:113==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2751  0.5056\n",
      "      2        0.1738  0.5270\n",
      "      3        0.1588  0.5101\n",
      "      4        0.1467  0.5111\n",
      "      5        0.1390  0.4791\n",
      "      6        0.1314  0.4926\n",
      "      7        0.1287  0.4936\n",
      "      8        0.1182  0.4926\n",
      "      9        0.1144  0.4816\n",
      "     10        0.1096  0.4846\n",
      "     11        0.1049  0.4926\n",
      "     12        0.1031  0.4936\n",
      "     13        0.0979  0.4896\n",
      "     14        0.0979  0.4881\n",
      "     15        0.0893  0.4836\n",
      "     16        0.0931  0.4966\n",
      "     17        0.0928  0.4966\n",
      "     18        0.0871  0.4921\n",
      "     19        0.0805  0.4846\n",
      "     20        0.0802  0.4936\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1045  0.2815\n",
      "      2        1.7549  0.2590\n",
      "      3        1.4114  0.2670\n",
      "      4        1.1290  0.2610\n",
      "      5        0.9484  0.2620\n",
      "      6        0.8701  0.2600\n",
      "      7        0.8119  0.2725\n",
      "      8        0.6995  0.2625\n",
      "      9        0.6502  0.2610\n",
      "     10        0.6242  0.2615\n",
      "     11        0.5417  0.2640\n",
      "     12        0.5051  0.2635\n",
      "     13        0.5174  0.2640\n",
      "     14        0.5577  0.2575\n",
      "     15        0.5577  0.2590\n",
      "     16        0.4691  0.2615\n",
      "     17        0.3663  0.2760\n",
      "     18        0.3406  0.2655\n",
      "     19        0.3232  0.2690\n",
      "     20        0.3977  0.2630\n",
      "     21        0.3440  0.2610\n",
      "     22        0.3476  0.2575\n",
      "     23        0.3426  0.2535\n",
      "     24        0.3523  0.2565\n",
      "     25        0.3872  0.2490\n",
      "     26        0.3177  0.2600\n",
      "     27        0.2722  0.2655\n",
      "     28        0.3035  0.2670\n",
      "     29        0.3436  0.2620\n",
      "     30        0.2834  0.2640\n",
      "==========Fold:114==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2208  0.4986\n",
      "      2        0.1727  0.4921\n",
      "      3        0.1597  0.4806\n",
      "      4        0.1477  0.4941\n",
      "      5        0.1406  0.5066\n",
      "      6        0.1331  0.4811\n",
      "      7        0.1264  0.4631\n",
      "      8        0.1229  0.4811\n",
      "      9        0.1145  0.5071\n",
      "     10        0.1094  0.4916\n",
      "     11        0.1104  0.5295\n",
      "     12        0.0954  0.4996\n",
      "     13        0.0995  0.4876\n",
      "     14        0.0917  0.4981\n",
      "     15        0.0951  0.4996\n",
      "     16        0.0910  0.4816\n",
      "     17        0.0876  0.4876\n",
      "     18        0.0841  0.4931\n",
      "     19        0.0771  0.5145\n",
      "     20        0.0798  0.4996\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1210  0.2565\n",
      "      2        1.7256  0.2620\n",
      "      3        1.3236  0.2625\n",
      "      4        1.0986  0.2605\n",
      "      5        0.9406  0.2615\n",
      "      6        0.8582  0.2605\n",
      "      7        0.7858  0.3034\n",
      "      8        0.6699  0.2645\n",
      "      9        0.6074  0.2610\n",
      "     10        0.6293  0.2800\n",
      "     11        0.5651  0.2700\n",
      "     12        0.5074  0.2670\n",
      "     13        0.4615  0.2605\n",
      "     14        0.5650  0.2615\n",
      "     15        0.4949  0.2565\n",
      "     16        0.4026  0.2615\n",
      "     17        0.3934  0.2695\n",
      "     18        0.3704  0.2685\n",
      "     19        0.3385  0.2630\n",
      "     20        0.3671  0.2660\n",
      "     21        0.3802  0.2615\n",
      "     22        0.3164  0.2575\n",
      "     23        0.3112  0.2595\n",
      "     24        0.2676  0.2605\n",
      "     25        0.2605  0.2555\n",
      "     26        0.2683  0.2615\n",
      "     27        0.2633  0.2620\n",
      "     28        0.2808  0.2625\n",
      "     29        0.3037  0.2645\n",
      "     30        0.2675  0.2600\n",
      "==========Fold:115==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.3496  0.5250\n",
      "      2        0.1730  0.4841\n",
      "      3        0.1579  0.5205\n",
      "      4        0.1487  0.5185\n",
      "      5        0.1410  0.5061\n",
      "      6        0.1338  0.4761\n",
      "      7        0.1281  0.4966\n",
      "      8        0.1187  0.5280\n",
      "      9        0.1212  0.5056\n",
      "     10        0.1135  0.4886\n",
      "     11        0.1113  0.4971\n",
      "     12        0.1050  0.5001\n",
      "     13        0.1021  0.4921\n",
      "     14        0.1015  0.4966\n",
      "     15        0.0956  0.5061\n",
      "     16        0.0946  0.4831\n",
      "     17        0.0950  0.4831\n",
      "     18        0.0949  0.4946\n",
      "     19        0.0907  0.5046\n",
      "     20        0.0848  0.5036\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0545  0.2795\n",
      "      2        1.6712  0.2640\n",
      "      3        1.2893  0.2630\n",
      "      4        1.1043  0.2585\n",
      "      5        0.9592  0.2665\n",
      "      6        0.8362  0.2615\n",
      "      7        0.7215  0.2660\n",
      "      8        0.6914  0.2650\n",
      "      9        0.6028  0.2640\n",
      "     10        0.5856  0.2660\n",
      "     11        0.5985  0.2630\n",
      "     12        0.5002  0.2600\n",
      "     13        0.4277  0.2590\n",
      "     14        0.4053  0.2625\n",
      "     15        0.4193  0.2675\n",
      "     16        0.4202  0.2620\n",
      "     17        0.4546  0.2645\n",
      "     18        0.3887  0.2625\n",
      "     19        0.3118  0.2755\n",
      "     20        0.3993  0.2655\n",
      "     21        0.3787  0.2550\n",
      "     22        0.3120  0.2610\n",
      "     23        0.2741  0.2615\n",
      "     24        0.2722  0.2650\n",
      "     25        0.2962  0.2710\n",
      "     26        0.2670  0.2660\n",
      "     27        0.2934  0.2655\n",
      "     28        0.2462  0.2590\n",
      "     29        0.2844  0.2645\n",
      "     30        0.2804  0.2595\n",
      "==========Fold:116==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2369  0.4991\n",
      "      2        0.1716  0.5051\n",
      "      3        0.1580  0.4971\n",
      "      4        0.1502  0.4946\n",
      "      5        0.1391  0.5006\n",
      "      6        0.1331  0.5056\n",
      "      7        0.1250  0.4861\n",
      "      8        0.1220  0.4931\n",
      "      9        0.1157  0.4941\n",
      "     10        0.1142  0.5026\n",
      "     11        0.1073  0.5280\n",
      "     12        0.1064  0.4946\n",
      "     13        0.0995  0.4936\n",
      "     14        0.0966  0.4896\n",
      "     15        0.0937  0.4731\n",
      "     16        0.0910  0.4931\n",
      "     17        0.0896  0.4946\n",
      "     18        0.0907  0.4911\n",
      "     19        0.0815  0.5001\n",
      "     20        0.0808  0.4926\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0577  0.2595\n",
      "      2        1.6770  0.2590\n",
      "      3        1.2993  0.2625\n",
      "      4        1.0839  0.2645\n",
      "      5        0.9577  0.2585\n",
      "      6        0.8494  0.2610\n",
      "      7        0.7370  0.2575\n",
      "      8        0.6714  0.2775\n",
      "      9        0.6478  0.2625\n",
      "     10        0.6089  0.2605\n",
      "     11        0.5823  0.2605\n",
      "     12        0.5099  0.2580\n",
      "     13        0.4632  0.2560\n",
      "     14        0.4934  0.2535\n",
      "     15        0.4980  0.2635\n",
      "     16        0.4238  0.2600\n",
      "     17        0.4074  0.2635\n",
      "     18        0.4009  0.2585\n",
      "     19        0.3544  0.2560\n",
      "     20        0.3425  0.2625\n",
      "     21        0.3173  0.2620\n",
      "     22        0.3688  0.2585\n",
      "     23        0.3474  0.2560\n",
      "     24        0.3031  0.2620\n",
      "     25        0.2960  0.2615\n",
      "     26        0.2735  0.2615\n",
      "     27        0.3094  0.2805\n",
      "     28        0.2850  0.2570\n",
      "     29        0.2722  0.2615\n",
      "     30        0.2931  0.2610\n",
      "==========Fold:117==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2071  0.5026\n",
      "      2        0.1656  0.4921\n",
      "      3        0.1536  0.4911\n",
      "      4        0.1457  0.4851\n",
      "      5        0.1402  0.5021\n",
      "      6        0.1271  0.4846\n",
      "      7        0.1233  0.4971\n",
      "      8        0.1173  0.4766\n",
      "      9        0.1136  0.5130\n",
      "     10        0.1110  0.4981\n",
      "     11        0.1063  0.4846\n",
      "     12        0.1024  0.5026\n",
      "     13        0.0951  0.4876\n",
      "     14        0.0946  0.5250\n",
      "     15        0.0895  0.4791\n",
      "     16        0.0873  0.4836\n",
      "     17        0.0846  0.5006\n",
      "     18        0.0834  0.4931\n",
      "     19        0.0821  0.4851\n",
      "     20        0.0821  0.4846\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0752  0.2575\n",
      "      2        1.7184  0.2655\n",
      "      3        1.3459  0.2560\n",
      "      4        1.1417  0.2570\n",
      "      5        0.9563  0.2620\n",
      "      6        0.8531  0.2565\n",
      "      7        0.6826  0.2555\n",
      "      8        0.6686  0.2570\n",
      "      9        0.6302  0.2585\n",
      "     10        0.5440  0.2820\n",
      "     11        0.4680  0.2670\n",
      "     12        0.4588  0.2535\n",
      "     13        0.4997  0.2625\n",
      "     14        0.4726  0.2640\n",
      "     15        0.3763  0.2630\n",
      "     16        0.3968  0.2555\n",
      "     17        0.4371  0.2600\n",
      "     18        0.3750  0.2615\n",
      "     19        0.3178  0.2635\n",
      "     20        0.2822  0.2580\n",
      "     21        0.2785  0.2555\n",
      "     22        0.2595  0.2635\n",
      "     23        0.3262  0.2640\n",
      "     24        0.3005  0.2620\n",
      "     25        0.2447  0.2580\n",
      "     26        0.2127  0.2560\n",
      "     27        0.2540  0.2625\n",
      "     28        0.2688  0.2620\n",
      "     29        0.3034  0.2640\n",
      "     30        0.2377  0.2605\n",
      "==========Fold:118==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.3458  0.5071\n",
      "      2        0.1765  0.5175\n",
      "      3        0.1611  0.4976\n",
      "      4        0.1534  0.5026\n",
      "      5        0.1449  0.4986\n",
      "      6        0.1383  0.5056\n",
      "      7        0.1331  0.5101\n",
      "      8        0.1267  0.4946\n",
      "      9        0.1246  0.4976\n",
      "     10        0.1181  0.4961\n",
      "     11        0.1131  0.4931\n",
      "     12        0.1064  0.4936\n",
      "     13        0.1079  0.4751\n",
      "     14        0.1050  0.4921\n",
      "     15        0.1031  0.4856\n",
      "     16        0.1062  0.5036\n",
      "     17        0.0945  0.4996\n",
      "     18        0.0897  0.4906\n",
      "     19        0.0937  0.5210\n",
      "     20        0.0865  0.5041\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1092  0.2560\n",
      "      2        1.7418  0.2675\n",
      "      3        1.3364  0.2650\n",
      "      4        1.0612  0.2635\n",
      "      5        0.9714  0.2635\n",
      "      6        0.7985  0.2620\n",
      "      7        0.7300  0.2605\n",
      "      8        0.6588  0.2620\n",
      "      9        0.6276  0.2595\n",
      "     10        0.5459  0.2545\n",
      "     11        0.5754  0.2545\n",
      "     12        0.5083  0.2615\n",
      "     13        0.4650  0.2635\n",
      "     14        0.4235  0.2620\n",
      "     15        0.4418  0.2535\n",
      "     16        0.3975  0.2570\n",
      "     17        0.3437  0.2785\n",
      "     18        0.4218  0.2655\n",
      "     19        0.4784  0.2615\n",
      "     20        0.3469  0.2615\n",
      "     21        0.2701  0.2620\n",
      "     22        0.2452  0.2620\n",
      "     23        0.2452  0.2585\n",
      "     24        0.2438  0.2620\n",
      "     25        0.2262  0.2570\n",
      "     26        0.2719  0.2610\n",
      "     27        0.3845  0.2605\n",
      "     28        0.2798  0.2615\n",
      "     29        0.2769  0.2550\n",
      "     30        0.2153  0.2605\n",
      "==========Fold:119==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2316  0.4896\n",
      "      2        0.1702  0.5026\n",
      "      3        0.1553  0.5017\n",
      "      4        0.1474  0.4961\n",
      "      5        0.1356  0.4871\n",
      "      6        0.1317  0.4891\n",
      "      7        0.1239  0.5016\n",
      "      8        0.1165  0.5061\n",
      "      9        0.1159  0.4761\n",
      "     10        0.1151  0.4956\n",
      "     11        0.1067  0.5145\n",
      "     12        0.1028  0.4866\n",
      "     13        0.1002  0.4871\n",
      "     14        0.0990  0.4891\n",
      "     15        0.0970  0.4981\n",
      "     16        0.0926  0.4981\n",
      "     17        0.0901  0.4761\n",
      "     18        0.0986  0.4936\n",
      "     19        0.0868  0.5056\n",
      "     20        0.0862  0.4901\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0835  0.2615\n",
      "      2        1.6755  0.2600\n",
      "      3        1.2865  0.2695\n",
      "      4        1.0385  0.2620\n",
      "      5        0.8617  0.2675\n",
      "      6        0.7664  0.2600\n",
      "      7        0.7658  0.2755\n",
      "      8        0.6472  0.2660\n",
      "      9        0.5863  0.2640\n",
      "     10        0.5584  0.2585\n",
      "     11        0.5332  0.2655\n",
      "     12        0.4762  0.2645\n",
      "     13        0.4654  0.2620\n",
      "     14        0.4530  0.2585\n",
      "     15        0.4201  0.2650\n",
      "     16        0.3694  0.2620\n",
      "     17        0.3727  0.2560\n",
      "     18        0.3374  0.2625\n",
      "     19        0.3474  0.2640\n",
      "     20        0.3333  0.2650\n",
      "     21        0.2922  0.2597\n",
      "     22        0.2483  0.2655\n",
      "     23        0.3317  0.2590\n",
      "     24        0.3152  0.2580\n",
      "     25        0.2794  0.2575\n",
      "     26        0.3285  0.2645\n",
      "     27        0.2903  0.2615\n",
      "     28        0.2655  0.2805\n",
      "     29        0.2338  0.2630\n",
      "     30        0.2165  0.2630\n",
      "==========Fold:120==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2249  0.4951\n",
      "      2        0.1661  0.4941\n",
      "      3        0.1543  0.4781\n",
      "      4        0.1451  0.5036\n",
      "      5        0.1365  0.5006\n",
      "      6        0.1304  0.4821\n",
      "      7        0.1229  0.4976\n",
      "      8        0.1152  0.4821\n",
      "      9        0.1114  0.5061\n",
      "     10        0.1068  0.4816\n",
      "     11        0.1046  0.5061\n",
      "     12        0.0958  0.4861\n",
      "     13        0.0994  0.4901\n",
      "     14        0.0936  0.5170\n",
      "     15        0.0902  0.4981\n",
      "     16        0.0858  0.4961\n",
      "     17        0.0835  0.4876\n",
      "     18        0.0862  0.5021\n",
      "     19        0.0774  0.4931\n",
      "     20        0.0753  0.4711\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0716  0.2595\n",
      "      2        1.7627  0.2675\n",
      "      3        1.4930  0.2590\n",
      "      4        1.2116  0.2620\n",
      "      5        1.0215  0.2640\n",
      "      6        0.8697  0.2595\n",
      "      7        0.7775  0.2625\n",
      "      8        0.7101  0.2625\n",
      "      9        0.5896  0.2620\n",
      "     10        0.5738  0.2606\n",
      "     11        0.5335  0.2815\n",
      "     12        0.5073  0.2670\n",
      "     13        0.5020  0.2630\n",
      "     14        0.4558  0.2660\n",
      "     15        0.4184  0.2630\n",
      "     16        0.3787  0.2680\n",
      "     17        0.3761  0.2655\n",
      "     18        0.3150  0.2660\n",
      "     19        0.3327  0.2610\n",
      "     20        0.2886  0.2620\n",
      "     21        0.3434  0.2655\n",
      "     22        0.3806  0.2570\n",
      "     23        0.3569  0.2725\n",
      "     24        0.2971  0.2685\n",
      "     25        0.2410  0.2660\n",
      "     26        0.2391  0.2565\n",
      "     27        0.2559  0.2640\n",
      "     28        0.2441  0.2680\n",
      "     29        0.2689  0.2630\n",
      "     30        0.2810  0.2590\n",
      "==========Fold:121==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2598  0.5245\n",
      "      2        0.1649  0.4931\n",
      "      3        0.1537  0.4866\n",
      "      4        0.1458  0.4856\n",
      "      5        0.1357  0.5001\n",
      "      6        0.1323  0.4896\n",
      "      7        0.1246  0.5011\n",
      "      8        0.1244  0.4881\n",
      "      9        0.1143  0.4996\n",
      "     10        0.1126  0.4806\n",
      "     11        0.1094  0.4901\n",
      "     12        0.1067  0.4851\n",
      "     13        0.1086  0.4996\n",
      "     14        0.0940  0.5026\n",
      "     15        0.0980  0.4931\n",
      "     16        0.0992  0.4996\n",
      "     17        0.0959  0.4891\n",
      "     18        0.0948  0.5086\n",
      "     19        0.0909  0.4926\n",
      "     20        0.0891  0.5016\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0656  0.2635\n",
      "      2        1.6642  0.2640\n",
      "      3        1.2713  0.2625\n",
      "      4        1.0428  0.2600\n",
      "      5        0.9247  0.2595\n",
      "      6        0.7890  0.2625\n",
      "      7        0.7384  0.2675\n",
      "      8        0.6654  0.2835\n",
      "      9        0.6081  0.2770\n",
      "     10        0.5171  0.2615\n",
      "     11        0.4953  0.2675\n",
      "     12        0.5256  0.2600\n",
      "     13        0.5789  0.2580\n",
      "     14        0.4633  0.2775\n",
      "     15        0.4007  0.2760\n",
      "     16        0.4138  0.2635\n",
      "     17        0.4192  0.2805\n",
      "     18        0.3735  0.2620\n",
      "     19        0.3794  0.2620\n",
      "     20        0.3145  0.2590\n",
      "     21        0.2847  0.2585\n",
      "     22        0.3381  0.2620\n",
      "     23        0.2996  0.2640\n",
      "     24        0.2515  0.2635\n",
      "     25        0.2535  0.2585\n",
      "     26        0.3330  0.2590\n",
      "     27        0.2881  0.2640\n",
      "     28        0.2123  0.2610\n",
      "     29        0.2172  0.2600\n",
      "     30        0.2705  0.2595\n",
      "==========Fold:122==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2234  0.5160\n",
      "      2        0.1671  0.4936\n",
      "      3        0.1538  0.4911\n",
      "      4        0.1454  0.4876\n",
      "      5        0.1372  0.4971\n",
      "      6        0.1293  0.4886\n",
      "      7        0.1240  0.5021\n",
      "      8        0.1215  0.4821\n",
      "      9        0.1133  0.5006\n",
      "     10        0.1098  0.5330\n",
      "     11        0.1065  0.4961\n",
      "     12        0.1052  0.4756\n",
      "     13        0.0991  0.4786\n",
      "     14        0.0967  0.5011\n",
      "     15        0.0931  0.4971\n",
      "     16        0.0912  0.4926\n",
      "     17        0.0827  0.4881\n",
      "     18        0.0879  0.4931\n",
      "     19        0.0835  0.4986\n",
      "     20        0.0808  0.4876\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.0947  0.2605\n",
      "      2        1.8321  0.2620\n",
      "      3        1.4831  0.2605\n",
      "      4        1.1927  0.2600\n",
      "      5        1.0003  0.2605\n",
      "      6        0.8970  0.2595\n",
      "      7        0.8236  0.2810\n",
      "      8        0.7505  0.2640\n",
      "      9        0.6736  0.2650\n",
      "     10        0.6256  0.2625\n",
      "     11        0.5766  0.2670\n",
      "     12        0.5435  0.2555\n",
      "     13        0.5038  0.2624\n",
      "     14        0.4825  0.2565\n",
      "     15        0.4116  0.2610\n",
      "     16        0.4550  0.2640\n",
      "     17        0.4045  0.2660\n",
      "     18        0.4555  0.2570\n",
      "     19        0.3833  0.2615\n",
      "     20        0.3499  0.2545\n",
      "     21        0.3072  0.2615\n",
      "     22        0.3261  0.2625\n",
      "     23        0.4086  0.2515\n",
      "     24        0.3806  0.2585\n",
      "     25        0.3442  0.2565\n",
      "     26        0.3273  0.2635\n",
      "     27        0.2672  0.2780\n",
      "     28        0.2495  0.2670\n",
      "     29        0.2824  0.2590\n",
      "     30        0.2659  0.2625\n",
      "==========Fold:123==========\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        0.2398  0.4871\n",
      "      2        0.1656  0.5036\n",
      "      3        0.1534  0.4931\n",
      "      4        0.1409  0.4961\n",
      "      5        0.1357  0.4981\n",
      "      6        0.1253  0.4916\n",
      "      7        0.1237  0.4916\n",
      "      8        0.1170  0.5011\n",
      "      9        0.1151  0.4921\n",
      "     10        0.1112  0.4971\n",
      "     11        0.0994  0.5026\n",
      "     12        0.1018  0.4931\n",
      "     13        0.0991  0.5086\n",
      "     14        0.0993  0.5195\n",
      "     15        0.0928  0.5061\n",
      "     16        0.0949  0.4896\n",
      "     17        0.0872  0.5011\n",
      "     18        0.0892  0.4851\n",
      "     19        0.0909  0.4846\n",
      "     20        0.0816  0.4861\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        2.1146  0.2600\n",
      "      2        1.7559  0.2625\n",
      "      3        1.3881  0.2635\n",
      "      4        1.1665  0.2615\n",
      "      5        1.0527  0.2570\n",
      "      6        0.8488  0.2625\n",
      "      7        0.7718  0.2605\n",
      "      8        0.6954  0.2565\n",
      "      9        0.6747  0.2620\n",
      "     10        0.6732  0.2605\n",
      "     11        0.6063  0.2625\n",
      "     12        0.5434  0.2760\n",
      "     13        0.5087  0.2675\n",
      "     14        0.4942  0.2655\n",
      "     15        0.4749  0.2605\n",
      "     16        0.4872  0.2670\n",
      "     17        0.4565  0.2605\n",
      "     18        0.3890  0.2650\n",
      "     19        0.3788  0.2625\n",
      "     20        0.3432  0.2625\n",
      "     21        0.3445  0.2600\n",
      "     22        0.3001  0.2615\n",
      "     23        0.3518  0.2575\n",
      "     24        0.2879  0.2585\n",
      "     25        0.2879  0.2630\n",
      "     26        0.2798  0.2625\n",
      "     27        0.2545  0.2600\n",
      "     28        0.3249  0.2605\n",
      "     29        0.3190  0.2645\n",
      "     30        0.2669  0.2610\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train the model\n",
    "\"\"\"\n",
    "from sklearn.datasets import make_classification\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# if data_type == 'major':\n",
    "#     X_raw = X_major\n",
    "#     y = y_major\n",
    "#     class_weight = weight_major\n",
    "#     num_output = 2\n",
    "# else:\n",
    "#     X_raw = X_minor\n",
    "#     y = y_minor    \n",
    "#     class_weight = weight_minor\n",
    "#     num_output = 9\n",
    "\n",
    "# adjustable parameters\n",
    "num_folds = 123\n",
    "batch_size = 100\n",
    "major_weights=[20]\n",
    "\n",
    "# fixed parameters\n",
    "today_date = '03082019'\n",
    "criterion = nn.CrossEntropyLoss\n",
    "\n",
    "#k-fold\n",
    "skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "# cross-validation\n",
    "y_probas = np.zeros(shape=(X_raw.shape[0],10))\n",
    "fold_i = 0\n",
    "\n",
    "for major_weight in major_weights:\n",
    "    for train_index, test_index in skf.split(X_raw, y):\n",
    "        # fold\n",
    "        fold_i = fold_i + 1\n",
    "        print('==========Fold:%s==========' %fold_i)\n",
    "\n",
    "        # data split\n",
    "        X_train, X_test = X_raw[train_index], X_raw[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # check label values distribution\n",
    "    #         unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "    #         print(unique_elements)\n",
    "    #         print(counts_elements)\n",
    "    #         dasda\n",
    "\n",
    "        # split the data into major (2-class) and minor (9-class) set\n",
    "        X_minor, y_minor, X_major, y_major = get_dataset(X_train, y_train)\n",
    "\n",
    "        # preprocess\n",
    "        X_major, y_major = xy_preprocess(X_major,y_major)\n",
    "        X_minor, y_minor = xy_preprocess(X_minor,y_minor)\n",
    "        X_test, y_test = xy_preprocess(X_test, y_test)\n",
    "\n",
    "        # ======================================== major classifier\n",
    "        # define the weight of each class\n",
    "        # https://discuss.pytorch.org/t/passing-the-weights-to-crossentropyloss-correctly/14731\n",
    "        class_weight = get_weight_balance(y_major)\n",
    "        class_weight[1] = major_weight\n",
    "        class_weight = torch.FloatTensor(class_weight)\n",
    "        # define the network\n",
    "        num_output = 2\n",
    "        max_epochs = 20\n",
    "        net_major = NeuralNetClassifier(\n",
    "                MyModule,\n",
    "                max_epochs=max_epochs,\n",
    "                lr=0.01,\n",
    "                criterion=criterion,\n",
    "                train_split=None,\n",
    "                optimizer=optim.Adam,\n",
    "                batch_size=batch_size,\n",
    "                warm_start=False,\n",
    "                iterator_train__shuffle=True,  \n",
    "                criterion__weight=class_weight,\n",
    "                module__num_output=num_output,\n",
    "        )\n",
    "        net_major.fit(X_major, y_major)\n",
    "\n",
    "\n",
    "        # ======================================== minor classifier\n",
    "        # define the weight of each class\n",
    "        # https://discuss.pytorch.org/t/passing-the-weights-to-crossentropyloss-correctly/14731\n",
    "        class_weight = get_weight_balance(y_minor)\n",
    "        class_weight = torch.FloatTensor(class_weight)\n",
    "\n",
    "        # define the network\n",
    "        num_output = 9\n",
    "        max_epochs = 30\n",
    "        net_minor = NeuralNetClassifier(\n",
    "                MyModule,\n",
    "                max_epochs=max_epochs,\n",
    "                lr=0.01,\n",
    "                criterion=criterion,\n",
    "                train_split=None,\n",
    "                optimizer=optim.Adam,\n",
    "                batch_size=batch_size,\n",
    "                warm_start=False,\n",
    "                iterator_train__shuffle=True,  \n",
    "                criterion__weight=class_weight,\n",
    "                module__num_output=num_output,\n",
    "        )\n",
    "        net_minor.fit(X_minor, y_minor)\n",
    "        \n",
    "        # ======================================== prediction\n",
    "\n",
    "        # predict\n",
    "        y_proba_major = convert_to_proba(net_major.predict_proba(X_test))\n",
    "        y_proba_minor = convert_to_proba(net_minor.predict_proba(X_test))\n",
    "\n",
    "\n",
    "        # combine\n",
    "        # combine prob\n",
    "        for i in range(0, y_proba_minor.shape[0]):\n",
    "            y_proba_minor[i,:] = y_proba_minor[i,:] * y_proba_major[i,1]\n",
    "        y_proba_major = y_proba_major[:,0]\n",
    "        y_proba_major = np.expand_dims(y_proba_major, axis=1)\n",
    "        y_proba =  np.concatenate((y_proba_major, y_proba_minor), 1)\n",
    "\n",
    "        # assign probs to certain row\n",
    "        y_probas[test_index,:] = y_proba\n",
    "    ############################################################################\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1     22556\n",
      "8      1539\n",
      "5       820\n",
      "10      791\n",
      "9       733\n",
      "4       571\n",
      "6       297\n",
      "7       210\n",
      "3        92\n",
      "2        66\n",
      "Name: 0, dtype: int64\n",
      "confusion matrix!\n",
      "[[12219   195   229  1486  1798   544   283  2193  1881  1728]\n",
      " [    5    46     0     3     4     2     0     3     0     3]\n",
      " [    4     0    76     0     2     0     7     3     0     0]\n",
      " [   16     5     4   432    17     4     2    31    40    20]\n",
      " [   26    15     2    38   577    22     9    61    33    37]\n",
      " [    9     2     1     0    11   245     0    11    10     8]\n",
      " [    2     0    15     1     8     2   176     5     1     0]\n",
      " [   28     6    11    85    99    35    27  1141    43    64]\n",
      " [   28     1     0    41    21    15     3    23   567    34]\n",
      " [   27     4     2    15    21    11     2    52    52   605]]\n"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    Evaluate the performance\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "\n",
    "    # get the y_pred\n",
    "    y_preds = np.argmax(y_probas,axis=1)\n",
    "\n",
    "    # calculation the confusion matrix\n",
    "    # row: true label\n",
    "    # col: predicted value\n",
    "    cm = np.array(confusion_matrix(y,y_preds))\n",
    "    print(df.iloc[:,0].value_counts())\n",
    "    freq = np.array([22556,66, 92, 571,820,297,210,1539,733,791])\n",
    "    #cm = np.transpose(cm)\n",
    "    print('confusion matrix!')\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # export to csv`\n",
    "    df_cm = pd.DataFrame(cm/freq[:,None])\n",
    "    sn.set(font_scale=1)\n",
    "    accuracies = sn.heatmap(df_cm, annot=True, annot_kws={\"size\":9})\n",
    "    figure = accuracies.get_figure()\n",
    "    figure.savefig('Subject1_cnn.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
